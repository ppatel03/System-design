Links :

https://bytebytego.com/courses/system-design-interview/back-of-the-envelope-estimation

https://matthewdbill.medium.com/back-of-envelope-calculations-cheat-sheet-d6758d276b05

Tips :
- Ask functional requirements first (example, ability to post tweet, view tweet)
- Ask DAU. If MAU is provided, atleast ask % of daily users. 
- Ask user activity : example read vs write ratio, or number of writes (example, 2 tweets per day)
- Ask non-functional requirements 
  These are very important and can convert entire inteview in your favor. Example if latency is 1s, then you do not need complex caching architecture.
    -- Latency 
           this would help decide whether you really need low latency optimizations like cache, or not. 
           Remember today mosts have 1 Gbps network which has 10 us to transfer 1 KB
    -- Availability 
          this would help decide the redundancy and reliability - example mostly 99.99% a day means you can compromise 10s (10 ^ 5 * 10 ^ -4)
          of unavailability or being unrelible. With this you can even decide that, your system could be out of sync from third part servers for 10s in a
          day 
    

QPS estimate
  DO NOT divided by no. of seconds to get QPS if its explictly mentioned about concurrent users say 10% of DAU. In such cases, QPS = 10% DAU
- 1 day ~ 10^5 s
- 1 month ~ 3 * 10 ^ 6s and further ~ 10 ^ 6s
- 1 week ~ 7 * 10 ^ 5s and further ~ 10 ^ 6s
----- For time constraints, do not hesitate to round off large amounts

Read : Write ratio


Distributed Systems estimate

Estimates in KB

Send 1 KB sequential from 1 Gbps network = 10 us  
Send 1 KB sequential from memory  = 0.25 us   (4 GB/s = 0.25 *  10 ^ -6 s) 
Send 1 KB sequential from disk    = 30 us     (30 MB/s = 0.03 * 10 ^ -3 s ) 
Send 1 KB sequential from cache   = 1 us    (1 GB/s = 1 * 10 ^ -6 s ) 

So network is 3 times faster than disk
memory is 40 times faster than network

Benchmarks on common systems

Cache
Azure Redis Cache : 
  53 GB, with 99.9% availability (https://github.com/Huachao/azure-content/blob/master/articles/redis-cache/cache-faq.md)
  230 us (https://redis.io/docs/management/optimization/latency/)




================================================================
================================================================
================================================================

References

Dump : https://coursehunters.online/t/educative-io-design-gurus-grokking-the-system-design-interview-part-5/584
Quick read : https://github.com/Jeevan-kumar-Raj/Grokking-System-Design


================================================================

- Key characteristics
    Resources
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/key-characteristics.md 
    Scalability, Availability, Reliability, Efficiency and Servicability/Manageability
    -> Remember an Avaiability system does not means Reliable but Reliable system is always available. 
    
   
================================================================================================================================
- Key learnings from system design


    
    Proximity Service 
    https://bytebytego.com/courses/system-design-interview/proximity-service
    
        Different types of address look up
        two dimensional search query, Geohash, Quadtree

        High level storage based on read : write ratio
          If Read ratio is high than writes, remember to apply master/slave architecture for storage  replica for reads and master/primary for write 

        Sharding and Replicating 
          These are two different concepts and usually you need both. Sharding is very common though.
          However, if your DB requirements are not huge (say 10 GB), you can simply create replicas to distribute read load than sharding it

        Operation complexity with in-memory data
          - If your in-memory data is large (in GBs), then it takes time to start up and serve traffic. Needs gradual roll out of new releases or 
          blue green deployment technique (https://martinfowler.com/bliki/BlueGreenDeployment.html)
          - Locking mechanism is needed for updates in a multithread environment
      
      
    NearBy Friends
    https://bytebytego.com/courses/system-design-interview/nearby-friends
    
        Back of envelope
        Req is 10% of DAU are concurrent. In that case, QPS is 10% of DAU and do not need additional calculations

        High level design vs API design + data model 
          Questions like this needs high level design first
        
        Update and Notify system like this 
          Always prefer Websocket connection which also works well with load balancer. Attach the user to web socket servers.
          Leverage impressive lightweight cache systems like Redis pub/sub which allows millions of channels https://redis.io/docs/manual/pubsub/
          
        How does API design for websocket works
          For any request, server is not obligated to send the response.
          Example, 1. for periodic location update -> initiated by client but no response.
                    2. for receiving update from friends -> initiated by server without client request

        WEBsocket servers operational complexity
          Since websocket servers are stateful, hard to auto-scale up / down needs extra care with load balancer help.

        Scaling up/down with stateful servers 
        With stateful clusters, scaling up or down has some operational overhead and risks, so it should be done with careful planning. The cluster is normally over-provisioned to make sure it can handle daily peak traffic with some comfortable headroom to avoid unnecessary resizing of the cluster.

                    
        Cache considerations
          only store most recent user location update. 
          also if cache goes down, no need to pre-fill since it will be having new updates via periodic mechanism


        Suggesting Cassandra for heavy-write horizontal scaled db ????

        There are many service discovery packages available, with etcd [4] and Zookeeper [5] among the most popular ones


        
        TODO : 
        Study more about Websocket connection working
        Study about relational database sharding
        Study about Cassandra (why columnar db is preferrable for write heavy)
        Study about redis well
        Data structure about pub/sub model : queue and hashmap for tracking purpose
        Alternative to Redis pub/sub is "Earlang" - is a general programming language and runtime environment built for highly distributed and concurrent applications.

         
    Distributed Message Queue
    https://bytebytego.com/courses/system-design-interview/distributed-message-queue

      Strictly speaking, Apache Kafka and Pulsar are not message queues as they are event streaming platforms. However, there is a convergence of features that starts to blur the distinction between message queues (RocketMQ, ActiveMQ, RabbitMQ, ZeroMQ, etc.) and event streaming platforms (Kafka, Pulsar). 

      Messaging models
        The most popular messaging models are point-to-point and publish-subscribe. Our distributed message queue supports both models. The publish-subscribe model is implemented by topics, and the point-to-point model can be simulated by the concept of the consumer group

        Point-to-point
          This model is commonly found in traditional message queues. In a point-to-point model, a message is sent to a queue and consumed by one and only one consumer.

        Publish-subscribe
          First, let’s introduce a new concept, the topic. Topics are the categories used to organize messages. Each topic has a name that is unique across the entire message queue service. Messages are sent to and read from a specific topic.

          In the publish-subscribe model, a message is sent to a topic and received by the consumers subscribing to this topic.

        Data storage
        Option 1: Database
          The first option is to use a database.
          Relational database: create a topic table and write messages to the table as rows.
          NoSQL database: create a collection as a topic and write messages as documents.

        Option 2: Write-ahead log (WAL)
          The second option is write-ahead log (WAL). WAL is just a plain file where new entries are appended to an append-only log. WAL is used in many systems, such as the redo log in MySQL and the WAL in ZooKeeper.

        A note on disk performance

          To meet the high data retention requirement, our design relies heavily on disk drives to hold a large amount of data. There is a common misconception that rotational disks are slow, but this is really only the case for random access. For our workload, as long as we design our on-disk data structure to take advantage of the sequential access pattern, the modern disk drives in a RAID configuration (i.e., with disks striped together for higher performance) could comfortably achieve several hundred MB/sec of read and write speed. This is more than enough for our needs, and the cost structure is favorable.

        Batching
          Batching is pervasive in this design. 
          There is a tradeoff between throughput and latency. If the system is deployed as a traditional message queue where latency might be more important, the system could be tuned to use a smaller batch size.

        Consumer flow
          most message queues choose the pull model instead of push  

        TODO : 
        Study more about Apache Zookeeper
        Working of Apacke Kafka  


      

    
    
================================================================================================================================

    
- Load balancing
    Resources
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/load-balancing.md
      https://www.cloudflare.com/learning/performance/types-of-load-balancing-algorithms/
      https://igotanoffer.com/blogs/tech/load-balancing-system-design-interview
    Static load balancing - round robin, weight round robin, IP hash
    Dynamic load balanacing - least connection, least resource, least response time, least bandwidht
    When to use Static ?
      If the server pool and the requests are both homogeneous, and there is only one balancer
    When to use Dynamic ?
      if the requests are heterogenous, Least Load is helpful to prevent servers from being intermittently overloaded.
      
    What are Hardware load balancers ?
      They have high performant hardware resources like L4 and L7. 
      Read about L4 and L7 here https://levelup.gitconnected.com/l4-vs-l7-load-balancing-d2012e271f56
      
    What are Software load balancers ?
      They run on standard servers and are less hardware optimized, but cheaper to set up and run. Example, Nginx or HAProxy. For Nginx 100s video, 
      refer https://www.youtube.com/watch?v=JKxlsvZXG7c. Nginx can serve routing to different servers, rate limiting, handle spikes,
      reverse proxy, security, cache, etc. 
      
    When to use Software or Hardware ?
      Tip : Since software load balancing can run on ordinary hardware and supports, always prefer software load balancer. Example Nginx
       
      
    What is proxy server, reverse proxy and API Gateway ? How is different from load balancer
      proxy vs reverse proxy vs load balancer : https://www.youtube.com/watch?v=MiqrArNSxSM
      Proxy : just protects the client side
      Reverse Proxy : protects the server by serving as proxy for all backend services - routing, rate limitng, load balancing
      API gateway : rate limiting, routing, handle spikes
      Load balancer : only performs load balancing
      
      So Reverse proxy can perform both API Gateway and Load Balancing. Example nginx.
      A load balancer can never be API gateway or reverse proxy.
      
     Always choose Nginx ? Why ?
       Has triats of load balancing + reverse proxy + API gateway
         - supports load balancing
         - supports reverse proxy 
         - supports rate limiting
         - supports caching
         - supports request grouping
         - supports secure auth
       Nginx supports L7
================================================================

      
  - Caching
    Resources
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/caching.md
      Distributed cache on Application server VS Global cache like Redis, (mostly preferred)
      CDN is another form of cache for static content but its often expensive
      
          
       What are different caching strategies ? 
          https://codeahoy.com/2017/08/11/caching-strategies-and-how-to-choose-the-right-one/
          Cache-Aside (application server has cache aside like Redis, Memcache and DB separately)
            - Pros : logic controlled by application server, resilent to cache failures, data model in cache vs db could be different
            - Cons : stale data in cache for db writes
          Read-Through Cache (application server reads from cache only)
            - Pros : logic controlled by cache, read-heavy, good consistency when combined with write through 
            - Cons : needs data model to be consistent, first time data always results in cache miss

          Belpw are Cache writes invalidation policy 

          Write through (data is written to cache and synced with storage)
            - Pros : fast retreival and data consistent systems
            - Cons : slow writes though
          Write around (data is written to storage, not cache)
            - Pros : Saves write operation on the cache
            - Cons : recently written data creates a cache miss and higher latency.
          Write back (data is written to cache only, then synced later to storage)
            - Pros : fast retreival, low latency read / writes
            - Cons : Risk of data loss
            
        Real applications of 
          1. read-through, write-through for high consistency
                DynamoDB Accelerator (DAX) for dynamoDB
          2. read-through, write-around for high performance for situations where data is written once and read less frequently or never.
                real-time logs, chatroom messages
          3. cache-aside, write back cache to absorb spikes during peak load
                custom applications using redis
          4. write back cache
                InnoDB which is relational database storage engine. Queries are first written to memory and eventually flushed to the disk.
                
                
================================================================
    - Sharding / Partitioning 
        Resources
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/sharding.md
          https://coursehunters.online/t/educative-io-design-gurus-grokking-the-system-design-interview-part-5/584
          Horizontal, Vertical and Directory based paritioning
            Directory based paritioning has work around to solve some challenges with Horizontal and Vertical?
          What's the best Partitioning criteria ?
            Consitent hashing which is combination of hash and list based partitioning
          Commom problems with Sharding / Partitioning
            Joins -> Denormalization
            Referential Integrity
            Hot shard problem needs rebalancing
              How to rebalance ? 
              It would take downtime, check directory based partitioning as per our educative.io resource.
              But consistent hashing can solve this problem better - https://medium.com/nerd-for-tech/consistent-hashing-6524e48ac648
              Great guide above - check consistent hashing with gossip protocol so that each partition knows where to fetch data from db
          Applications of consistent hashing ?
          Apache Cassandra, Dynamo DB
 
 ================================================================

    - Indexing
        Resources
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/indexes.md
          What is a database index ? 
            https://www.codecademy.com/article/sql-indexes
          Pros
            helps in speeding up the search 
          Cons
            lowers write/update/delete performance
          When to use indexes ?
            In the case of data sets that are many terabytes in size but with very small payloads (e.g., 1 KB), 
            indexes are a necessity for optimizing data access. Finding a small payload in such a large dataset can be a real challenge since
            we can’t possibly iterate over that much data in any reasonable time. Furthermore, it is very likely that such a large data set is 
            spread over several physical devices—this means we need some way to find the correct physical location of the desired data. 
            Indexes are the best way to do this.
            
   ================================================================

     - Proxy 
        Resources
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/proxies.md
          
          Usage : 
          filter requests, log requests, cache, encryption and most important batch request. Further, it can get smarter to o collapse requests 
          for data that is spatially close together in the storage (consecutively on disk). This strategy will result in decreasing request latency. 
          For example, let’s say a bunch of servers request parts of file: part1, part2, part3, etc. We can set up our proxy in such a way 
          that it can recognize the spatial locality of the individual requests, thus collapsing them into a single request and reading complete file, 
          which will greatly minimize the reads from the data origin.
          
          Proxy are of two types 
          Client proxy to protect the clients
          Server proxy to protect the servers. Also called reverse proxy
          
          Should we use proxy then?
          Only use proxy to protect the client.
          For server side, Always Nginx since its open source and supports both reverse proxy + load balancing + API gateway.
          
          
   
   ==============================================================================

          Queues
          
            Resource :
            - https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/queues.md
            
            When to use queue :
            Queues are implemented on the asynchronous communication protocol, meaning when a client submits a task to a queue they 
            are no longer required to wait for the results; instead, they need only acknowledgment that the request was properly received.
            
            Queues are also used for fault tolerance as they can provide some protection from service outages and failures. For example, 
            we can create a highly robust queue that can retry service requests that have failed due to transient system failures
            
            When not to use queue :
            When client expects respone in real-time
            
            Example of queues : 
             RabbitMQ and Kafka (which is open source)
             
  ====================================================================================
  
        Redundancy and Replication
        
          Resource : 
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/redundancy.md
          
          When to use Redundant system :
          Always. its a key concept of distributed system.
          Always prefer shared nothing architecture so that each node can operate independently and can scale.
          
          
    ====================================================================================
        SQL vs NoSQL
        
          Resource : 
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/sql-vs-nosql.md
          
          SQL 
          Relational databases store data in rows and columns. Example, MySQL, Oracle, MS SQL Server, SQLite, Postgres
          
          NoSQL 
          Key-value :  Example, DynamoDB, Redis, Memcache
          Document DB : Example, MongoDB, 
          Wide-Column Database : Example, HBase, Cassandra - https://hevodata.com/learn/columnar-databases/
          Graph Databases : Example, Neo4j
          
          Differences between both 
          Storage : SQL is tables but NoSQL has different storage models
          Schema : changing schema with SQL is possible but requires whole database modification
          Querying : using SQL for SQL dbs. 
          Scalability : SQL is vertically scalable and possible to scale it horizontally but has limitations. NoSQL is horizontally scalable
          Reliability or ACID : Definitely SQL ensures ACID but NoSQL solutions sacrifice ACID compliance for performance and scalability.
          
          When to use SQL
            ACID compliance for e-commerce and finanicial transactions
            Your data is structured and unchanging.
            
          When to use NoSQL
            - Storing large volumes of data that often have little to no structure. 
            - Making the most of cloud computing and storage. Cloud-based storage is an excellent cost-saving solution 
            but requires data to be easily spread across multiple servers to scale up. 
            - Rapid development. NoSQL is extremely useful for rapid development as it doesn’t need to be prepped ahead of time. 
            
          What about write performance ? 
            I think SQL provides more flexibility on write performance than NoSQL.
            
            
  ====================================================================================
  
    CAP Theorem
    Resource : 
    https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/cap-theorem.md
    
    CAP theorem states that it is impossible for a distributed software system to simultaneously provide more than two out of three of 
    the following guarantees (CAP): Consistency, Availability and Partition tolerance.
    
    We can only build a system that has any two of these three properties. Because, to be consistent, all nodes should see the same set of updates 
    in the same order. But if the network suffers a partition, updates in one partition might not make it to the other partitions before a client 
    reads from the out-of-date partition after having read from the up-to-date one. The only thing that can be done to cope with this possibility 
    is to stop serving requests from the out-of-date partition, but then the service is no longer 100% available.

    When to choose consistency over availability ? 
      Banking, Payment systems
    
    When to choose consistency over availability ? 
      Most distributed system use cases like Google Maps, Twitter, etc.
      
    Since reliability consists of both consistency and availability, ASK YOUR interviewer what it means for system to be 99.99% reliable
    
    
   ====================================================================================
 
    Consistent Hashing
    
    Resource 
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/consistent-hashing.md
      Recommended ones 
        https://medium.com/nerd-for-tech/consistent-hashing-6524e48ac648 - Explains well about rebalancing
        https://bytebytego.com/courses/system-design-interview/design-consistent-hashing
        
      How does it work ? 
        Hashing is done in the range where it maps the key to the integer in that range. Example 0 -> 255 are the integers placed in ring form 
        such that values are wrapped around.
        
        1. the servers are mapped to the integers in that range
        2. to map key to a server, simply hash the key to the integer in that range and move clockwise till you find the server (could be binary search)
        
        Now how this is better :
        1. adding a server : say S1 is added at position 20 and is near S2 at position 25. Then keys from before 20 wuld map to S1.
        2. removing a server : say S1 is removed at position 20 and was near S2 at position 25. Then  all keys even before 20 would map to S2. 
        3. uniform distribution : add “virtual replicas”. Instead of mapping each server to a single point on the ring, 
            we map it to multiple points on the ring, i.e. replicas. 
        4. easy to rebalance : when server is added or removed, only its next clockwise neighbouring server is affected and requires re-mapping of keys.
        
        
    ====================================================================================
    
    Client-Server Communication
    
      Resource : 
        https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/client-server-communication.md
        
      Types : 
      1. Standard HTTP Web Request : client opens the connection, gets response from the server and terminate the connection 
      2. Ajax Polling : client performs HTTP Web Request above periodically (say every 1 s). May create unecessary empty responses if server is not ready.
      3. Long Polling : client performs HTTP Web Request and waits for long time till response is received (say 1 min). Then sends request again.
      4. Web Socket: persistent connection between a client and a server that both parties can use to start sending data at any time.
      5. SSE :  persistent onnection between a client and a server where server send data to the client but not the reverse
      
      When to use Long Polling vs Web Socket ? 
        https://ably.com/blog/websockets-vs-long-polling
        https://dev.to/kevburnsjr/websockets-vs-long-polling-3a0o
        
        Clearly Websockets have many advantages over long polling and thus are appropriate for many applications which require consistent low latency 
        full duplex high frequency communication such as chat applications and real time applications.
        
        Scaling up 
        Websockets have problems with load distribution due to persistent connection.
        Long polling will have equal load distribution after its long timeout
        
     When to use Web Socker vs Server Sent Events ?
        Websocket for bi-directional communication whole SSE for server to client communication
        https://blog.bitsrc.io/websockets-vs-server-sent-events-968659ab0870
        
        WebSockets are widely used and valued in technological solutions such as real-time polling, chat, media players, multiplayer games, etc.
        Server-Sent Events: There are many applications where sending data from the client isn’t necessary. 
        SSEs are especially useful in status updates, social-media news feeds, push notifications, newsletters, etc.
     

      


        
        
        
        
      


    
    
    

          
          
          
          
          
          

          

          
        
        
  
  

      
      



