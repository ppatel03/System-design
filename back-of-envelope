Links :

https://bytebytego.com/courses/system-design-interview/back-of-the-envelope-estimation

https://matthewdbill.medium.com/back-of-envelope-calculations-cheat-sheet-d6758d276b05

Tips :
- Ask functional requirements first (example, ability to post tweet, view tweet)
- Ask DAU. If MAU is provided, atleast ask % of daily users. 
- Ask user activity : example read vs write ratio, or number of writes (example, 2 tweets per day)
- Ask non-functional requirements 
  These are very important and can convert entire inteview in your favor. Example if latency is 1s, then you do not need complex caching architecture if your data transfer is less than 100 MB (100 MBps max bandwidth today)
    -- Latency 
           this would help decide whether you really need low latency optimizations like cache, or not. 
           Remember today mosts have 1 Gbps network which has 10 us to transfer 1 KB
    -- Availability 
          this would help decide the redundancy and reliability - example mostly 99.99% a day means you can compromise 10s (10 ^ 5 * 10 ^ -4)
          of unavailability or being unrelible. With this you can even decide that, your system could be out of sync from third part servers for 10s in a
          day 
    

QPS estimate
  DO NOT divided by no. of seconds to get QPS if its explictly mentioned about concurrent users say 10% of DAU. In such cases, QPS = 10% DAU
- 1 day ~ 10^5 s
- 1 month ~ 3 * 10 ^ 6s and further ~ 10 ^ 6s
- 1 week ~ 7 * 10 ^ 5s and further ~ 10 ^ 6s
----- For time constraints, do not hesitate to round off large amounts

Read : Write ratio


Distributed Systems estimate

Estimates in KB

Send 1 KB sequential from 1 Gbps network = 10 us  
Send 1 KB sequential from memory  = 0.25 us   (4 GB/s = 0.25 *  10 ^ -6 s) 
Send 1 KB sequential from disk    = 30 us     (30 MB/s = 0.03 * 10 ^ -3 s ) 
Send 1 KB sequential from cache   = 1 us    (1 GB/s = 1 * 10 ^ -6 s )
Round trip within the same datacenter	 = 500 us 
Send packet CA (California) ->Netherlands->CA	= 150 ms 

So network is 3 times faster than disk
memory is 40 times faster than network

Benchmarks on common systems

Cache benchmarking
Azure Redis Cache : 
  53 GB, with 99.9% availability (https://github.com/Huachao/azure-content/blob/master/articles/redis-cache/cache-faq.md)
  230 us (https://redis.io/docs/management/optimization/latency/)

SQL DB benchmarking - Relational
    1. MySQL benchmarking
       (https://dev.mysql.com/blog-archive/mysql-connection-handling-and-scaling/)
       Recommend 4 * number of cores parallel for ~ 200 us based on simple primary key look ups. For example, 120 parellel requests for 32 cores will take 200 us
       1K requests on 32 will take ~ 1ms for simply select query

       - Max size set by MySQL
         256 TB (https://dev.mysql.com/doc/mysql-reslimits-excerpt/8.0/en/table-size-limit.html#:~:text=You%20are%20using%20a%20MyISAM,2567%20%E2%88%92%201%20bytes)

       Some facts
       - MySQL client <-> server is socket based connection
       - per connection -> one user thread
       - Max connection limit for MySQL = 100 K
       - size of user thread depends on THD connection data structure which is per connection ~ 10 MB 
       - max THD ~ 10K 
       - So recommended min size of your MySQL = 10 MB * 10 K = 100 GB ??

       - MySQL sorting (https://www.pankajtanwar.in/blog/what-is-the-sorting-algorithm-behind-order-by-query-in-mysql)
          External merge sort (quick sort + merge sort) if data doesn’t fits into the memory
          Quick sort, if data fits into the memory and we want all of it
          Heap sort, if data fits into the memory but we are using LIMIT to fetch only some results
          Index lookup (not exactly a sorting algorithm, just a pre-calculated binary tree)

       - MySQL query cache (https://docs.oracle.com/cd/E17952_01/mysql-5.1-en/query-cache.html)
          As of MySQL 5.1.63, the query cache is not supported for partitioned tables, and is automatically disabled for queries involving partitioned tables. The query cache cannot be enabled for such queries. 

       - MySQL sharding (https://stackoverflow.com/questions/1610887/how-to-partition-mysql-across-multiple-servers)
          this is different from mysql partitioning (https://vertabelo.com/blog/everything-you-need-to-know-about-mysql-partitions/)



NoSQL value DB benchmarking
    1. Timeseries : OpenTSDB / InfluxDB
          8 cores and 32GB RAM can handle over 250,000 writes per second, greater than 1 M series
          (https://bytebytego.com/courses/system-design-interview/metrics-monitoring-and-alerting-system)

    2. Object storage / cold storage benchmarking
        S3 Standard is designed for 99.99% data availability and durability of 99.999999999% of objects across multiple Availability Zones in a given year. AWS S3 provides a great performance. It automatically scales to high request rates, with a very low latency of 100–200 milliseconds.Your application can achieve at least 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket (https://aws.plainenglish.io/optimize-your-aws-s3-performance-27b057f231a3 )

    3. Cassandra

          100s video : https://www.youtube.com/watch?v=ziq7FUKpCS8

    4. MongoDB

Attempt to standardized DB selection criteria 
    MySQL vs MongoDB : https://kinsta.com/blog/mongodb-vs-mysql/

    When MySQL
    if ACID, 
      then MYSQL (high confidence)
    if reasonable size (< 1 TB) not expected to scale rapidly,
      if heavy reads (reading from read replicas is faster) 
        if low writes 
          then MySQL (high confidence)
        if heavy writes (else have to keep synchronizing)
          then MySQL (low confidence)
    If db needs rapid scale (size keeps growing)
      if manual sharding is needed ( say by user id )
        if sharding has all related rows (try to denormalize)
          if there are less aggregations 
            then MySQL (high confidence)
          if there are more aggregations 
            then MySQL (medium confidence since aggregations with sharding could be achieved with noSQL DBs like Cassandra)
    If db needs better search performance (not full text search)  
      if index-ing on fixed columns,
        then MySQL (high confidence)
      if db experience heavy writes
        then MySQL (low confidence since indexes increases write latency)   

  Open questions
    1. Why is MySQL better for read heavy low write workload ? answered above
    2. MySQL is better for ACID systems and structured ? How ? 
    2. Why is Cassandra (noSQL) better for read less write heavy workload ?
    3. Aggregation on sharded MySQL needs Application level handling vs Cassandra or HBase(Column) can handle this since they are built on top of distributed systems . Fact and derived above
    4. How MySQL indexing works ? 


================================================================
================================================================
================================================================

References

Dump : https://coursehunters.online/t/educative-io-design-gurus-grokking-the-system-design-interview-part-5/584
Quick read : https://github.com/Jeevan-kumar-Raj/Grokking-System-Design


================================================================

- Key characteristics
    Resources
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/key-characteristics.md 
    Scalability, Availability, Reliability, Efficiency and Servicability/Manageability
    -> Remember an Avaiability system does not means Reliable but Reliable system is always available. 

  How to handle burst / spiky reads ? 
    Use Nignx to do request collapsing for similar type of content
    Use queue if content does not need to be delivered real-time
    Other strategies : Data Cache, CDN (https://www.onecloudsystems.com/2016/10/25/how-to-ensure-site-can-handle-traffic-spikes/)

  Pull vs push models
    In Pull approach, the metrics collector needs to know the complete list of service endpoints to pull data from. The good news is that we have a reliable, scalable, and maintainable solution available through Service Discovery, provided by etcd [14], Zookeeper [15], etc., wherein services register their availability and the metrics collector can be notified by the Service Discovery component whenever the list of service endpoints changes. 
    If a server goes down, then you can re-try in the next pull. But here you need dedupe logic or  store the offset in S3 to know what messages to retry.

    In a push model, a collection agent is commonly installed on every server being monitored. Aggregation is an effective way to reduce the volume of data sent to the metrics collector. If the push traffic is high and the metrics collector rejects the push with an error, the agent could keep a small buffer of data locally (possibly by storing them locally on disk), and resend them later.
    If a server goes down or cannot handle burst traffic, then you loose the message.

  Tips to ensure consistency 
    - To maintain data consistency between internal services, ensuring exactly-once processing is very important.
    - To maintain data consistency between the internal service and external service (PSP), we usually rely on idempotency and reconciliation.
    - For replicas, ensure acknowledge only if all replicas respond to read/write update. 

    
   
================================================================================================================================
- Key learnings from system design


    
    Proximity Service 
    https://bytebytego.com/courses/system-design-interview/proximity-service
    
        Different types of address look up
        two dimensional search query, Geohash, Quadtree

        High level storage based on read : write ratio
          If Read ratio is high than writes, remember to apply master/slave architecture for storage  replica for reads and master/primary for write 

        Sharding and Replicating 
          These are two different concepts and usually you need both. Sharding is very common though.
          However, if your DB requirements are not huge (say 10 GB), you can simply create replicas to distribute read load than sharding it

        Operation complexity with in-memory data
          - If your in-memory data is large (in GBs), then it takes time to start up and serve traffic. Needs gradual roll out of new releases or 
          blue green deployment technique (https://martinfowler.com/bliki/BlueGreenDeployment.html)
          - Locking mechanism is needed for updates in a multithread environment
      
      
    NearBy Friends
    https://bytebytego.com/courses/system-design-interview/nearby-friends
    
        Back of envelope
        Req is 10% of DAU are concurrent. In that case, QPS is 10% of DAU and do not need additional calculations

        High level design vs API design + data model 
          Questions like this needs high level design first
        
        Update and Notify system like this 
          Always prefer Websocket connection which also works well with load balancer. Attach the user to web socket servers.
          Leverage impressive lightweight cache systems like Redis pub/sub which allows millions of channels https://redis.io/docs/manual/pubsub/
          
        How does API design for websocket works
          For any request, server is not obligated to send the response.
          Example, 1. for periodic location update -> initiated by client but no response.
                    2. for receiving update from friends -> initiated by server without client request

        WEBsocket servers operational complexity
          Since websocket servers are stateful, hard to auto-scale up / down needs extra care with load balancer help.

        Scaling up/down with stateful servers 
        With stateful clusters, scaling up or down has some operational overhead and risks, so it should be done with careful planning. The cluster is normally over-provisioned to make sure it can handle daily peak traffic with some comfortable headroom to avoid unnecessary resizing of the cluster.

                    
        Cache considerations
          only store most recent user location update. 
          also if cache goes down, no need to pre-fill since it will be having new updates via periodic mechanism


        Suggesting Cassandra for heavy-write horizontal scaled db ????

        There are many service discovery packages available, with etcd [4] and Zookeeper [5] among the most popular ones


        
        TODO : 
        Study more about Websocket connection working
        Study about relational database sharding
        Study about Cassandra (why columnar db is preferrable for write heavy)
        Study about redis well
        Data structure about pub/sub model : queue and hashmap for tracking purpose
        Alternative to Redis pub/sub is "Earlang" - is a general programming language and runtime environment built for highly distributed and concurrent applications.

         
    Distributed Message Queue
    https://bytebytego.com/courses/system-design-interview/distributed-message-queue

      Strictly speaking, Apache Kafka and Pulsar are not message queues as they are event streaming platforms. However, there is a convergence of features that starts to blur the distinction between message queues (RocketMQ, ActiveMQ, RabbitMQ, ZeroMQ, etc.) and event streaming platforms (Kafka, Pulsar). 

      Messaging models
        The most popular messaging models are point-to-point and publish-subscribe. Our distributed message queue supports both models. The publish-subscribe model is implemented by topics, and the point-to-point model can be simulated by the concept of the consumer group

        Point-to-point
          This model is commonly found in traditional message queues. In a point-to-point model, a message is sent to a queue and consumed by one and only one consumer.

        Publish-subscribe
          First, let’s introduce a new concept, the topic. Topics are the categories used to organize messages. Each topic has a name that is unique across the entire message queue service. Messages are sent to and read from a specific topic.

          In the publish-subscribe model, a message is sent to a topic and received by the consumers subscribing to this topic.

        Data storage
        Option 1: Database
          The first option is to use a database.
          Relational database: create a topic table and write messages to the table as rows.
          NoSQL database: create a collection as a topic and write messages as documents.

        Option 2: Write-ahead log (WAL)
          The second option is write-ahead log (WAL). WAL is just a plain file where new entries are appended to an append-only log. WAL is used in many systems, such as the redo log in MySQL and the WAL in ZooKeeper.

        A note on disk performance

          To meet the high data retention requirement, our design relies heavily on disk drives to hold a large amount of data. There is a common misconception that rotational disks are slow, but this is really only the case for random access. For our workload, as long as we design our on-disk data structure to take advantage of the sequential access pattern, the modern disk drives in a RAID configuration (i.e., with disks striped together for higher performance) could comfortably achieve several hundred MB/sec of read and write speed. This is more than enough for our needs, and the cost structure is favorable.

        Batching
          Batching is pervasive in this design. 
          There is a tradeoff between throughput and latency. If the system is deployed as a traditional message queue where latency might be more important, the system could be tuned to use a smaller batch size.

        Producer flow
          This is very important since it will answer the question why Kafka can scale better.
          Basically, zookeeper takes care of which broker to assign 

        Consumer flow
          most message queues choose the pull model instead of push  

        TODO : 
        Study more about Apache Zookeeper
        Working of Apacke Kafka  


      
    Design Fund raising App
        https://excalidraw.com/#room=f014e3afcbd4ddc43b20,SKkDU1jvFs-vLQNoH7pHmg

        Asymmetric encryption could be used here for payment gateway
          https://www.youtube.com/watch?v=AQDCe585Lnc
          also used in HTTPS, SSH, Bitcoin, Emails using PGP protocol

        How HTTPS works ? How it uses SSL and TLS
          https://www.youtube.com/watch?v=hExRDVZHhig
          HTTPS uses public key encryption to secure data using SSL (Secure socket layer) protocol. Basically server gives SSL certicate to client and acknowledgement is established between the two. Then info can be transferred securely using encryption.
          TLS is latest and successor of SSL
          Today most websites supports https because of google standards

          ByteByteGo : https://www.youtube.com/watch?v=j9QmMEWmcfo
            Moderm HTTPS uses TLS
            1. first establishes TCP handshake at transport 
            2. then client sends hello and gets the certificate from server which has public key of server (Asymmetric) 
            3. then client encryptes his/her session key with server's public key and then on server, it gets clients sesssion key by decrypting with server's private key. This is Asymmetric encryption
            4. Now both has session key and uses session key as cipher to encrypt and decrypt at both sides. This is Symmetric encryption.

            SSL uses public key encryption (Asymmetric encryption) only

          How SSO work ? 
            https://www.youtube.com/watch?v=O1cRJWYF-g4
            uses public key encryption
          
          Oauth 2.0 work ? 
            https://www.youtube.com/watch?v=CPbvxxslDTU

            Related to above 
            How payment gateway work ?
              https://www.youtube.com/watch?v=GUurzvS3DlY


        TODO : 
          Learn about public Asymmetric encryption (key / private key) encryprion - https://www.youtube.com/watch?v=AQDCe585Lnc
          Learn about HTTPS encryption (uses Asymmetric encryption)
          Learn about end to end encrytion
          Learn about how SSO work

    Metrics Monitoring and Alerting System
        https://bytebytego.com/courses/system-design-interview/metrics-monitoring-and-alerting-system

        Candidate: What is the scale of the infrastructure we are monitoring with this system?
            Interviewer: 100 million daily active users, 1,000 server pools, and 100 machines per pool.
        The infrastructure being monitored is large-scale.
            100 million daily active users
            Assume we have 1,000 server pools, 100 machines per pool, 100 metrics per machine => ~10 million metrics

            Storage : 
            1 month : ~ 10 ^ 6 (10 mil metrics) * 10 ^ 6  * 10 KB 
                    : ~ 10 ^ 13 KB
                    : ~10 ^ 4 TB
            > 1 month : ~ 10 ^ 3 TB
            1 year : 12 * 1 month ~ 10 ^ 5 TB

            Burst read  : 
            UI QPS : (100 million DAU / 10 ^ 5) * 2 (peak) 
                   : 2K QPS
            Bandwidth QPS : 2000 QPS * (100 metrics) * 10 ^ 6 (1 week) * 1 KB
                          :  ~ 10 ^ 11 KB
                          :  ~ 10 ^ 5 GB 
                          : internal datacenter bandwith needs to be  ~ 10 ^ 5 GB -> needs high compression and request collapsing
                          : per user data transfer 10 ^ 2 GB

        Data storage system
          a relational database is not optimized for operations you would commonly perform against time-series data. For example, computing the moving average in a rolling time window requires complicated SQL that is difficult to read. Moreover, a general-purpose relational database does not perform well under constant heavy write load. 

          Again Cassandra was brought up in discussion for write heavy ??? Why ? 
            May be its good for bulk new writes 
          
          OpenTSDB is a distributed time-series database, but since it is based on Hadoop and HBase, running a Hadoop/HBase cluster adds complexity. Another feature of a strong time-series database is efficient aggregation and analysis of a large amount of time-series data by labels, also known as tags in some databases. For example, InfluxDB builds indexes on labels to facilitate the fast lookup of time-series by labels

        Metrics collection - Pull vs push models
          In Pull approach, the metrics collector needs to know the complete list of service endpoints to pull data from. The good news is that we have a reliable, scalable, and maintainable solution available through Service Discovery, provided by etcd [14], Zookeeper [15], etc., wherein services register their availability and the metrics collector can be notified by the Service Discovery component whenever the list of service endpoints changes.

          In a push model, a collection agent is commonly installed on every server being monitored. Aggregation is an effective way to reduce the volume of data sent to the metrics collector. If the push traffic is high and the metrics collector rejects the push with an error, the agent could keep a small buffer of data locally (possibly by storing them locally on disk), and resend them later.

        Scale through Kafka
          There are a couple of ways that we can leverage Kafka’s built-in partition mechanism to scale our system.
            Configure the number of partitions based on throughput requirements.
            Partition metrics data by metric names, so consumers can aggregate data by metrics names.
            Further partition metrics data with tags/labels.
            Categorize and prioritize metrics so that important metrics can be processed first.
        
        Where aggregations can happen
          Metrics can be aggregated in different places; in the collection agent (on the client-side), the ingestion pipeline (before writing to storage), and the query side (after writing to storage). Let’s take a closer look at each of them.

        Space optimization in time-series DB
          Data encoding and compression
            Example, rather than storing absolute values, the delta of the values can be stored along with one base value like: 1610087371, 10, 10, 9, 11
          Downsampling
            Since our data retention is 1 year, we can downsample old data.
          Cold storage
            Cold storage is the storage of inactive data that is rarely used. The financial cost for cold storage is much lower.


        Alerting system - build vs buy 
          There are many industrial-scale alerting systems available off-the-shelf, and most provide tight integration with the popular time-series databases. Many of these alerting systems integrate well with existing notification channels, such as email and PagerDuty. In the real world, it is a tough call to justify building your own alerting system. In interview settings, especially for a senior position, be ready to justify your decision

        TODO : 
        TCP vs UDP
        Hot vs Cold Storage : https://www.youtube.com/watch?v=90EBp9wkoYM
 

  Ad Click Event Aggregation
      https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation

      Raw data vs Aggregated Data
        Should we store raw data or aggregated data? Our recommendation is to store both. Let’s take a look at why.
          1. It’s a good idea to keep the raw data. If something goes wrong, we could use the raw data for debugging. If the aggregated data is corrupted due to a bad bug, we can recalculate the aggregated data from the raw data, after the bug is fixed.
          2. Aggregated data should be stored as well. The data size of the raw data is huge. The large size makes querying raw data directly very inefficient. To mitigate this problem, we run read queries on aggregated data.
          3. Raw data serves as backup data. We usually don’t need to query raw data unless recalculation is needed. Old raw data could be moved to cold storage to reduce costs.
          4. Aggregated data serves as active data. It is tuned for query performance.


      Choose the right database
        When it comes to choosing the right database, we need to evaluate the following:
        What does the data look like? Is the data relational? Is it a document or a blob?
        Is the workflow read-heavy, write-heavy, or both?
        Is transaction support needed?
        Do the queries rely on many online analytical processing (OLAP) functions [3] like SUM, COUNT?

        As shown in the back of the envelope estimation, the average write QPS is 10,000, and the peak QPS can be 50,000, so the system is write-heavy. On the read side, raw data is used as backup and a source for recalculation, so in theory, the read volume is low.
        Relational databases can do the job, but scaling the write can be challenging. NoSQL databases like Cassandra and InfluxDB are more suitable because they are optimized for write and time-range queries.
      
      Asynchronous processing
        The design we currently have is synchronous. This is not good because the capacity of producers and consumers is not always equal. Consider the following case; if there is a sudden increase in traffic and the number of events produced is far beyond what consumers can handle, consumers might get out-of-memory errors or experience an unexpected shutdown. If one component in the synchronous link is down, the whole system stops working.

        A common solution is to adopt a message queue (Kafka) to decouple producers and consumers. This makes the whole process asynchronous and producers/consumers can be scaled independently.
        You might be wondering why we don’t write the aggregated results to the database directly. The short answer is that we need the second message queue like Kafka to achieve end-to-end exactly-once semantics (atomic commit). "Exactly-once" feature in distributed queues.

      Aggregation service
        The MapReduce framework is a good option to aggregate ad click events. The directed acyclic graph (DAG) is a good model for it [9]. The key to the DAG model is to break down the system into small computing units, like the Map/Aggregate/Reduce nodes
          Map node
            You might be wondering why we need the Map node. An alternative option is to set up Kafka partitions or tags and let the aggregate nodes subscribe to Kafka directly. This works, but the input data may need to be cleaned or normalized, and these operations can be done by the Map node. Another reason is that we may not have control over how data is produced and therefore events with the same ad_id might land in different Kafka partitions.
          Aggregate node
            An Aggregate node counts ad click events by ad_id in memory every minute. In the MapReduce paradigm, the Aggregate node is part of the Reduce. So the map-aggregate-reduce process really means map-reduce-reduce.
          Reduce node
            A Reduce node reduces aggregated results from all “Aggregate” nodes to the final result. 
        Main use cases
          Use case 1: aggregate the number of clicks
            input events are partitioned by ad_id (ad_id % 3) in Map nodes and are then aggregated by Aggregation nodes.
          Use case 2: return top N most clicked ads
            Input events are mapped using ad_id and each Aggregate node maintains a heap data structure to get the top 3 ads within the node efficiently. In the last step, the Reduce node reduces N ads 
          Use case 3: data filtering
            To support data filtering like “show me the aggregated click count for ad001 within the USA only”, we can pre-define filtering criteria and aggregate based on them.

      Streaming vs batching
        In our design, both stream processing and batch processing are used. We utilized stream processing to process data as it arrives and generates aggregated results in a near real-time fashion. We utilized batch processing for historical data backup.
        For a system that contains two processing paths (batch and streaming) simultaneously, this architecture is called lambda [14]. A disadvantage of lambda architecture is that you have two processing paths, meaning there are two codebases to maintain. Kappa architecture [15], which combines the batch and streaming in one processing path, solves the problem.

        Data recalculation
          Sometimes we have to recalculate the aggregated data, also called historical data replay. For example, if we discover a major bug in the aggregation service, we would need to recalculate the aggregated data from raw data starting at the point where the bug was introduced.
      
      Time
        We need a timestamp to perform aggregation. The timestamp can be generated in two different places:
          Event time: when an ad click happens.
          Processing time: refers to the system time of the aggregation server that processes the click event.
          
          Due to network delays and asynchronous environments (data go through a message queue), the gap between event time and processing time can be large.

      Aggregation window
          According to the “Designing data-intensive applications” book by Martin Kleppmann [16], there are four types of window functions: tumbling (also called fixed) window, hopping window, sliding window, and session window. We will discuss the tumbling window and sliding window
          In the tumbling window (highlighted in Figure 15), time is partitioned into same-length, non-overlapping chunks.
          In the sliding window (highlighted in Figure 16), events are grouped within a window that slides across the data stream, according to a specified interval.
      
      Delivery guarantees
          Since the aggregation result is utilized for billing, data accuracy and completeness are very important. The system needs to be able to answer questions such as:

          How to avoid processing duplicate events?
          How to ensure all events are processed?
        Solution : use queue with "exactly once" policy

      Data deduplication
        Client-side. For example, a client might resend the same event multiple times. Duplicated events sent with malicious intent are best handled by ad fraud/risk control components. If this is of interest, please refer to the reference material [18].
        Server outage. If an aggregation service node goes down in the middle of aggregation and the upstream service hasn’t yet received an acknowledgment, the same events might be sent and aggregated again. Let’s take a closer look
        
        The most straightforward solution (Figure 18) is to use external file storage, such as HDFS or S3, to record the offset.
        
        To achieve “exactly-once” processing, we need to put operations between step 4 to step 6 in one distributed transaction. A distributed transaction is a transaction that works across several nodes. If any of the operations fails, the whole transaction is rolled back.

      Scale the system
        Scale the message queue
          We have already discussed how to scale the message queue extensively in the “Distributed Message Queue” chapter, so we’ll only briefly touch on a few points.

          Producers. We don’t limit the number of producer instances, so the scalability of producers can be easily achieved.

          Consumers. Inside a consumer group, the rebalancing mechanism helps to scale the consumers by adding or removing nodes.When there are hundreds of Kafka consumers in the system, consumer rebalance can be quite slow and could take a few minutes or even more. Therefore, if more consumers need to be added, try to do it during off-peak hours to minimize the impact.
        
        Brokers (Remember each broker has multiple topics and and each topic has multiple partitions for parallelization. Each consumer group can be attached to many topics but no two consumers in same consumer group is attached to one partitions. Each consumer in the group maintains its own offset to the partition)

          Hashing key

            Using ad_id as hashing key for Kafka partition to store events from the same ad_id in the same Kafka partition. In this case, an aggregation service can subscribe to all events of the same ad_id from one single partition.

          The number of partitions

            If the number of partitions changes, events of the same ad_id might be mapped to a different partition. Therefore, it’s recommended to pre-allocate enough partitions in advance, to avoid dynamically increasing the number of partitions in production.

          Topic physical sharding

            One single topic is usually not enough. We can split the data by geography (topic_north_america, topic_europe, topic_asia, etc.,) or by business type (topic_web_ads, topic_mobile_ads, etc).

        Scale the aggregation service
            In the high-level design, we talked about the aggregation service being a map/reduce operation.

              Option 1: In same host, Allocate events with different ad_ids to different threads,
              Option 2: Deploy aggregation service nodes on resource providers like Apache Hadoop

        Scale the database
          Cassandra natively supports horizontal scaling, in a way similar to consistent hashing.

      
      Hotspot issue
        A shard or service that receives much more data than the others is called a hotspot. This occurs because major companies have advertising budgets in the millions of dollars and their ads are clicked more often. Since events are partitioned by ad_id, some aggregation service nodes might receive many more ad click events than others, potentially causing server overload.
        This problem can be mitigated by allocating more aggregation nodes to process popular ads. Map-Reduce architecure uses master slave model to handle this resource management.

      Fault tolerance
        Replaying data from the beginning of Kafka is slow. A good practice is to save the “system status” like upstream offset to a snapshot and recover from the last saved status. In our design, the “system status” is more than just the upstream offset because we need to store data like top N most clicked ads in the past M minutes.

      Reconciliation
        Reconciliation means comparing different sets of data in order to ensure data integrity. 
        What we can do is to sort the ad click events by event time in every partition at the end of the day, by using a batch job and reconciling between raw data and  aggregation result.

      Alternative design
        In a generalist system design interview, you are not expected to know the internals of different pieces of specialized software used in a big data pipeline. Explaining your thought process and discussing trade-offs is very important, which is why we propose a generic solution. Another option is to store ad click data in Hive, with an ElasticSearch layer built for faster queries.


      TODO:
      Study about different types of database 
      s3 
      Streaming vs Batching table : https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation




  Hotel Reservation System
  https://bytebytego.com/courses/system-design-interview/hotel-reservation-system

      High-level design
        We use the microservice architecture for this hotel reservation system. Over the past few years, microservice architecture has gained great popularity. Companies that use microservice include Amazon, Netflix, Uber, Airbnb, Twitter, etc. If you want to learn more about the benefits of a microservice architecture, you can check out some good resources [1] [2].

      Data model
          Before we decide which database to use, let’s take a close look at the data access patterns. For the hotel reservation system, we need to support the following queries:
          Query 1: View detailed information about a hotel.
          Query 2: Find available types of rooms given a date range.
          Query 3: Record a reservation.
          Query 4: Look up a reservation or past history of reservations.

          From the back-of-the-envelope estimation, we know the scale of the system is not large but we need to prepare for traffic surges during big events. With these requirements in mind, we choose a relational database because:

          A relational database works well with read-heavy and write less frequently workflows. This is because the number of users who visit the hotel website/apps is a few orders of magnitude higher than those who actually make reservations. NoSQL databases are generally optimized for writes and the relational database works well enough for read-heavy workflow.

          A relational database provides ACID (atomicity, consistency, isolation, durability) guarantees. ACID properties are important for a reservation system. Without those properties, it’s not easy to prevent problems such as negative balance, double charge, double reservations, etc. ACID properties make application code a lot simpler and make the whole system easier to reason about. A relational database usually provides these guarantees.

          A relational database can easily model the data. The structure of the business data is very clear and the relationship between different entities (hotel, room, room_type, etc) is stable. This kind of data model is easily modeled by a relational database.

      Concurrency issues
        Another important problem to look at is double booking. We need to solve two problems: 1) the same user clicks on the “book” button multiple times. 2) multiple users try to book the same room at the same time.

          Option 1: Pessimistic locking
            Pessimistic locking [6], also called pessimistic concurrency control, prevents simultaneous updates by placing a lock on a record as soon as one user starts to update it. Other users who attempt to update the record have to wait until the first user has released the lock 
              Pros:
                Prevents applications from updating data that is being – or has been – changed.
                It is easy to implement and it avoids conflict by serializing updates. Pessimistic locking is useful when data contention is heavy.

              Cons:
                Deadlocks may occur when multiple resources are locked. Writing deadlock-free application code could be challenging.
                This approach is not scalable. If a transaction is locked for too long, other transactions cannot access the resource. This has a significant impact on database performance, especially when transactions are long-lived or involve a lot of entities.


          Option 2: Optimistic locking
            Optimistic locking [7], also referred to as optimistic concurrency control, allows multiple concurrent users to attempt to update the same resource.

            There are two common ways to implement optimistic locking: version number and timestamp. Version number is generally considered to be a better option because the server clock can be inaccurate over time. We explain how optimistic locking works with version number.

            Optimistic locking is usually faster than pessimistic locking because we do not lock the database. However, the performance of optimistic locking drops dramatically when concurrency is high.
            To understand why, consider the case when many clients try to reserve a hotel room at the same time. Because there is no limit on how many clients can read the available room count, all of them read back the same available room count and the current version number. When different clients make reservations and write back the results to the database, only one of them will succeed, and the rest of the clients receive a version check failure message. These clients have to retry. In the subsequent round of retries, there is only one successful client again, and the rest have to retry. Although the end result is correct, repeated retries cause a very unpleasant user experience.

              Pros
                Easy to implement.
                It works well when data contention is minimal.

              Cons:
                Performance is poor when data contention is heavy.

          Option 3: Database constraints
              This approach is very similar to optimistic locking. Let’s explore how it works. In the room_type_inventory table, add the following constraint:

              CONSTRAINT `check_room_count` CHECK((`total_inventory - total_reserved` >= 0))

              Cons
                Similar to optimistic locking, when data contention is heavy, it can result in a high volume of failures. Users could see there are rooms available, but when they try to book one, they get the “no rooms available” response. This experience can be frustrating to users.
                The database constraints cannot be version-controlled easily like the application code.
                Not all databases support constraints. It might cause problems when we migrate from one database solution to another.

      Caching
        New challenges posed by the cache
          Adding a cache layer significantly increases the system scalability and throughput, but it also introduces a new challenge: how to maintain data consistency between the database and the cache.

          When a user books a room, two operations are executed in the happy path:

          Query room inventory to find out if there are enough rooms left. The query runs on the Inventory cache.

          Update inventory data. The inventory DB is updated first. The change is then propagated to the cache asynchronously. This asynchronous cache update could be invoked by the application code, which updates the inventory cache after data is saved to the database. It could also be propagated using change data capture (CDC) [8]. CDC is a mechanism that reads data changes from the database and applies the changes to another data system. One common solution is Debezium [9]. It uses a source connector to read changes from a database and applies them to cache solutions such as Redis [10].

          If you think carefully, you find that the inconsistency between inventory cache and database actually does not matter, as long as the database does the final inventory validation check.

          Let’s take a look at an example. Let’s say the cache states there is still an empty room, but the database says there is not. In this case, when the user queries the room inventory, they find there is still room available, so they try to reserve it. When the request reaches the inventory database, the database does the validation and finds that there is no room left. In this case, the client receives an error, indicating someone else just booked the last room before them. When a user refreshes the website, they probably see there is no room left because the database has synchronized inventory data to the cache, before they click the refresh button.

      Data consistency among services
          In a traditional monolithic architecture [11], a shared relational database is used to ensure data consistency. In our microservice design, we chose a hybrid approach of different microservice with shared relational database.
          However, if your interviewer is a microservice purist, they might challenge this hybrid approach. In their mind, for a microservice architecture, each microservice has its own databases.

          To address the data inconsistency, here is a high-level summary of industry-proven techniques. If you want to read the details, please refer to the reference materials.

            Two-phase commit (2PC) [12]. 2PC is a database protocol used to guarantee atomic transaction commit across multiple nodes, i.e., either all nodes succeeded or all nodes failed. Because 2PC is a blocking protocol, a single node failure blocks the progress until the node has recovered. It’s not performant.

            Saga. A saga is a sequence of local transactions. Each transaction updates and publishes a message to trigger the next transaction step. If a step fails, the saga executes compensating transactions to undo the changes that were made by preceding transactions [13]. 2PC works as a single commit to perform ACID transactions while Saga consists of multiple steps and relies on eventual consistency.

    TODO
      Read about acid system design ?
      Read about 2 phase commit ? 
      Ensure Acid with distributed systems ? 
      Mysql and MySQL benchmarking ?


  
  Payment System
    https://bytebytego.com/courses/system-design-interview/payment-system

    Back-of-the-envelope estimation
        The system needs to process 1 million transactions per day, which is 1,000,000 transactions / 10^5 seconds = 10 transactions per second (TPS). 10 TPS is not a big number for a typical database, which means the focus of this system design interview is on how to correctly handle payment transactions, rather than aiming for high throughput.


    Step 2 - Propose High-Level Design and Get Buy-In
        At a high level, the payment flow is broken down into two steps to reflect how money flows:

        Pay-in flow
        Pay-out flow

        Pay-in flow

          Payment service
            The payment service accepts payment events from users and coordinates the payment process. The first thing it usually does is a risk check, assessing for compliance with regulations such as AML/CFT [2], and for evidence of criminal activity such as money laundering or financing of terrorism.

          Payment Service Provider (PSP)
            A PSP moves money from account A to account B. In this simplified example, the PSP moves the money out of the buyer’s credit card account.

          Ledger
            The ledger keeps a financial record of the payment transaction. he ledger system is very important in post-payment analysis, such as calculating the total revenue of the e-commerce website or forecasting future revenue.

          Wallet (for seller)
            The wallet keeps the account balance of the merchant. It may also record how much a given user has paid in total.


    APIs for payment service
        POST /v1/payments
            This endpoint executes a payment event.
            You may have noticed that the data type of the “amount” field is “string,” rather than “double”. Double is not a good choice because:
              Different protocols, software, and hardware may support different numeric precisions in serialization and deserialization. This difference might cause unintended rounding errors.
              The number could be extremely big (for example, Japan’s GDP is around 5x1014 yen for the calendar year 2020), or extremely small (for example, a satoshi of Bitcoin is 10-8).
        GET /v1/payments/{:id}
            This endpoint returns the execution status of a single payment order based on payment_order_id.

    The data model for payment service
        When we select a storage solution for a payment system, performance is usually not the most important factor.
        Usually, we prefer a traditional relational database with ACID transaction support over NoSQL/NewSQL.
        payment event 
                    Name	              Type
                    checkout_id	        string PK
                    buyer_info	        string
                    credit_card_info	  depends on the card provider
                    is_payment_done	    boolean

        payment order
                            Name	                Type
                        payment_order_id	      String PK
                        checkout_id	          string FK
                        buyer_account	          string
                        amount	                string
                        currency	                string
                        payment_order_status	    string
                        ledger_updated	          boolean
                        wallet_updated	          boolean
        Double-entry ledger system
            There is a very important design principle in the ledger system: the double-entry principle (also called double-entry accounting/bookkeeping [6]). Double-entry system is fundamental to any payment system and is key to accurate bookkeeping. It records every payment transaction into two separate ledger accounts with the same amount. 
            The double-entry system states that the sum of all the transaction entries must be 0. One cent lost means someone else gains a cent.

      Hosted payment page
          Most companies prefer not to store credit card information internally because if they do, they have to deal with complex regulations such as Payment Card Industry Data Security Standard (PCI DSS) [8] in the United States. To avoid handling credit card information, companies use hosted credit card pages provided by PSPs. For websites, it is a widget or an iframe, while for mobile applications, it may be a pre-built page from the payment SDK. 

      Pay-out flow
          The components of the pay-out flow are very similar to the pay-in flow. One difference is that instead of using PSP to move money from the buyer’s credit card to the e-commerce website’s bank account, the pay-out flow uses a third-party pay-out provider to move money from the e-commerce website’s bank account to the seller’s bank account.

      Step 3 - Design Deep Dive
          PSP integration
            If a company chooses not to store sensitive payment information due to complex regulations and security concerns, PSP provides a hosted payment page to collect card payment details and securely store them in PSP. This is the approach most companies take.

          Reconciliation
            When system components communicate asynchronously, there is no guarantee that a message will be delivered, or a response will be returned.
            External systems, such as PSPs or banks, prefer asynchronous communication as well. So how can we ensure correctness in this case?
            The answer is reconciliation. This is a practice that periodically compares the states among related services in order to verify that they are in agreement. It is usually the last line of defense in the payment system.
            Every night the PSP or banks send a settlement file to their clients. The settlement file contains the balance of the bank account, together with all the transactions that took place on this bank account during the day. The reconciliation system parses the settlement file and compares the details with the ledger system.

          Handling payment processing delays
            The payment service must be able to handle these payment requests that take a long time to process. If the buy page is hosted by an external PSP, which is quite common these days, the PSP would handle these long-running payment requests in the following ways:

              The PSP would return a pending status to our client. Our client would display that to the user. Our client would also provide a page for the customer to check the current payment status.

              The PSP tracks the pending payment on our behalf, and notifies the payment service of any status update via the webhook the payment service registered with the PSP.
              Alternatively, instead of updating the payment service via a webhook, some PSP would put the burden on the payment service to poll the PSP for status updates on any pending payment requests.
          
          Communication among internal services
              Synchronous communication

                Synchronous communication like HTTP works well for small-scale systems, but its shortcomings become obvious as the scale increases. It creates a long request and response cycle that depends on many services. The drawbacks of this approach are:
                Low performance. If any one of the services in the chain doesn’t perform well, the whole system is impacted.
                Poor failure isolation. If PSPs or any other services fail, the client will no longer receive a response.
                Tight coupling. The request sender needs to know the recipient.
                Hard to scale. Without using a queue to act as a buffer, it’s not easy to scale the system to support a sudden increase in traffic.
          
              Asynchronous 
                Single receiver (point to point model in distributed message queue)
                Multiple receivers (pub sub model) .. eg. kafka
                  This model maps well to the payment system, as the same request might trigger multiple side effects such as sending push notifications, updating financial reporting, analytics,

          Handling failed payments
              Tracking payment state
              Retry queue and dead letter queue

          Exactly-once 
              We will explain how to implement at-least-once using retry, and at-most-once using idempotency check.

              Retry
                Immediate retry OR Fixed intervals OR increment intervals OR exponential backoff OR Cancel
                Determining the appropriate retry strategy is difficult. There is no “one size fits all” solution. As a general guideline, use exponential backoff if the network issue is unlikely to be resolved in a short amount of time.

              Scenario 1: The payment system integrates with PSP using a hosted payment page, and the client clicks the pay button twice.
              Scenario 2: The payment is successfully processed by the PSP, but the response fails to reach our payment system due to network errors. The user clicks the “pay” button again or the client retries the payment.

              Idempotency
                From an API standpoint, idempotency means clients can make the same call repeatedly and produce the same result.
                For communication between clients (web and mobile applications) and servers, an idempotency key is usually a unique value that is generated by the client and expires after a certain period of time. A UUID is commonly used as an idempotency key and it is recommended by many tech companies such as Stripe [19] and PayPal [20]. To perform an idempotent payment request, an idempotency key is added to the HTTP header: <idempotency-key: key_value>.

                Scenario 1: what if a customer clicks the “pay” button quickly twice?

                  To support idempotency, we can use the database's unique key constraint. For example, the primary key of the database table is served as the idempotency key. Here is how it works:

                      When the payment system receives a payment, it tries to insert a row into the database table.

                      A successful insertion means we have not seen this payment request before.

                      If the insertion fails because the same primary key already exists, it means we have seen this payment request before. The second request will not be processed.

                Scenario 2: The payment is successfully processed by the PSP, but the response fails to reach our payment system due to network errors. Then the user clicks the “pay” again.
                    When the user clicks the “pay” button again, the payment order is the same, so the token sent to the PSP is the same. Because the token is used as the idempotency key on the PSP side, it is able to identify the double payment and return the status of the previous execution.

          Consistency
              To maintain data consistency between internal services, ensuring exactly-once processing is very important.

              To maintain data consistency between the internal service and external service (PSP), we usually rely on idempotency and reconciliation. If the external service supports idempotency, we should use the same idempotency key for payment retry operations. Even if an external service supports idempotent API, reconciliation is still needed because we shouldn’t assume the external system is always right.

          Payment security
              use HTTPS with SSL (public key encryption), API gateway for rate limiting to avoid DDOS, use tokens to avoid Card theft 


      TODO : 
        Note about synchronous and asynchronous communication, importance of reconciliation
        Ensure all replicas are always in-sync. We could use consensus algorithms such as Paxos [21] and Raft [22], or use consensus-based distributed databases such as YugabyteDB [23] or CockroachDB [24].

  Distributed Email Service
      https://bytebytego.com/courses/system-design-interview/distributed-email-service

        Back-of-the-envelope estimation
            
            Assume metadata is stored in a database. Storage requirement for maintaining metadata in 1 year: 1 billion users * 40 (avg) emails / day * 365 days * 50 KB = 730 PB.
            Assume 20% of emails contain an attachment and the average attachment size is 500 KB.
            Storage for attachments in 1 year is: 1 billion users * 40 emails / day * 365 days * 20% * 500 KB = 1,460 PB

            From this back-of-the-envelope calculation, it’s clear we would deal with a lot of data. So, it’s likely that we need a distributed database solution.


        Step 2 - Propose High-Level Design and Get Buy-In
            Email knowledge 101
              Email protocols
              Domain name service (DNS)
              Attachment
            Traditional mail servers
              Traditional mail server architecture
                  Alice logs in to her Outlook client, composes an email, and presses the “send” button. The email is sent to the Outlook mail server. The communication protocol between the Outlook client and the mail server is SMTP.

                  Outlook mail server queries the DNS (not shown in the diagram) to find the address of the recipient’s SMTP server. In this case, it is Gmail’s SMTP server. Next, it transfers the email to the Gmail mail server. The communication protocol between the mail servers is SMTP.

                  The Gmail server stores the email and makes it available to Bob, the recipient.

                  Gmail client fetches new emails through the IMAP/POP server when Bob logs in to Gmail.

              Storage
                  In a traditional mail server, emails were stored in local file directories and each email was stored in a separate file with a unique name.
                  As the email volume grew and the file structure became more complex, disk I/O became a bottleneck. The local directories also don’t satisfy our high availability and reliability requirements.


            Distributed mail servers
                Email APIs
                  Due to the length limitations of this book, we cover only some of the most important APIs for webmail. A common way for webmail to communicate is through the HTTP protocol.

                  1. Endpoint: POST /v1/messages
                  2. Endpoint: GET /v1/folders
                  3. Endpoint: GET /v1/folders/{:folder_id}/messages
                  4. Endpoint: GET /v1/messages/{:message_id}

                Distributed mail server architecture
                  While it is easy to set up an email server that handles a small number of users, it is difficult to scale beyond one server. This is mainly because traditional email servers were designed to work with a single server only.


        TODO : 
            learn about Email protocols : SMTP for sending, POP and IMAP for retreiving

    
================================================================================================================================

    
- Load balancing
    Resources
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/load-balancing.md
      https://www.cloudflare.com/learning/performance/types-of-load-balancing-algorithms/
      https://igotanoffer.com/blogs/tech/load-balancing-system-design-interview
    Static load balancing - round robin, weight round robin, IP hash
    Dynamic load balanacing - least connection, least resource, least response time, least bandwidht
    When to use Static ?
      If the server pool and the requests are both homogeneous, and there is only one balancer
    When to use Dynamic ?
      if the requests are heterogenous, Least Load is helpful to prevent servers from being intermittently overloaded.
      
    What are Hardware load balancers ?
      They have high performant hardware resources like L4 and L7. 
      Read about L4 and L7 here https://levelup.gitconnected.com/l4-vs-l7-load-balancing-d2012e271f56
      
    What are Software load balancers ?
      They run on standard servers and are less hardware optimized, but cheaper to set up and run. Example, Nginx or HAProxy. For Nginx 100s video, 
      refer https://www.youtube.com/watch?v=JKxlsvZXG7c. Nginx can serve routing to different servers, rate limiting, handle spikes,
      reverse proxy, security, cache, etc. 
      
    When to use Software or Hardware ?
      Tip : Since software load balancing can run on ordinary hardware and supports, always prefer software load balancer. Example Nginx
       
      
    What is proxy server, reverse proxy and API Gateway ? How is different from load balancer
      proxy vs reverse proxy vs load balancer : https://www.youtube.com/watch?v=MiqrArNSxSM
      Proxy : just protects the client side
      Reverse Proxy : protects the server by serving as proxy for all backend services - routing, rate limitng, load balancing
      API gateway : rate limiting, routing, handle spikes
      Load balancer : only performs load balancing
      
      So Reverse proxy can perform both API Gateway and Load Balancing. Example nginx.
      A load balancer can never be API gateway or reverse proxy.
      
     Always choose Nginx ? Why ?
       Has triats of load balancing + reverse proxy + API gateway
         - supports load balancing
         - supports reverse proxy 
         - supports rate limiting
         - supports caching
         - supports request grouping
         - supports secure auth
       Nginx supports L7
================================================================

      
  - Caching
    Resources
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/caching.md
      Distributed cache on Application server VS Global cache like Redis, (mostly preferred)
      CDN is another form of cache for static content but its often expensive
      
          
       What are different caching strategies ? 
          https://codeahoy.com/2017/08/11/caching-strategies-and-how-to-choose-the-right-one/
          Cache-Aside (application server has cache aside like Redis, Memcache and DB separately)
            - Pros : logic controlled by application server, resilent to cache failures, data model in cache vs db could be different
            - Cons : stale data in cache for db writes
          Read-Through Cache (application server reads from cache only)
            - Pros : logic controlled by cache, read-heavy, good consistency when combined with write through 
            - Cons : needs data model to be consistent, first time data always results in cache miss

          Belpw are Cache writes invalidation policy 

          Write through (data is written to cache and synced with storage)
            - Pros : fast retreival and data consistent systems
            - Cons : slow writes though
          Write around (data is written to storage, not cache)
            - Pros : Saves write operation on the cache
            - Cons : recently written data creates a cache miss and higher latency.
          Write back (data is written to cache only, then synced later to storage)
            - Pros : fast retreival, low latency read / writes
            - Cons : Risk of data loss
            
        Real applications of 
          1. read-through, write-through for high consistency
                DynamoDB Accelerator (DAX) for dynamoDB
          2. read-through, write-around for high performance for situations where data is written once and read less frequently or never.
                real-time logs, chatroom messages
          3. cache-aside, write back cache to absorb spikes during peak load
                custom applications using redis
          4. write back cache
                InnoDB which is relational database storage engine. Queries are first written to memory and eventually flushed to the disk.
                
                
================================================================
    - Sharding / Partitioning 
        Resources
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/sharding.md
          https://coursehunters.online/t/educative-io-design-gurus-grokking-the-system-design-interview-part-5/584
          Horizontal, Vertical and Directory based paritioning
            Directory based paritioning has work around to solve some challenges with Horizontal and Vertical?
          What's the best Partitioning criteria ?
            Consitent hashing which is combination of hash and list based partitioning
          Commom problems with Sharding / Partitioning
            Joins -> Denormalization
            Referential Integrity
            Hot shard problem needs rebalancing
              How to rebalance ? 
              It would take downtime, check directory based partitioning as per our educative.io resource.
              But consistent hashing can solve this problem better - https://medium.com/nerd-for-tech/consistent-hashing-6524e48ac648
              Great guide above - check consistent hashing with gossip protocol so that each partition knows where to fetch data from db
          Applications of consistent hashing ?
          Apache Cassandra, Dynamo DB
 
 ================================================================

    - Indexing
        Resources
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/indexes.md
          What is a database index ? 
            https://www.codecademy.com/article/sql-indexes
          Pros
            helps in speeding up the search 
          Cons
            lowers write/update/delete performance
          When to use indexes ?
            In the case of data sets that are many terabytes in size but with very small payloads (e.g., 1 KB), 
            indexes are a necessity for optimizing data access. Finding a small payload in such a large dataset can be a real challenge since
            we can’t possibly iterate over that much data in any reasonable time. Furthermore, it is very likely that such a large data set is 
            spread over several physical devices—this means we need some way to find the correct physical location of the desired data. 
            Indexes are the best way to do this.
            
   ================================================================

     - Proxy 
        Resources
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/proxies.md
          
          Usage : 
          filter requests, log requests, cache, encryption and most important batch request. Further, it can get smarter to o collapse requests 
          for data that is spatially close together in the storage (consecutively on disk). This strategy will result in decreasing request latency. 
          For example, let’s say a bunch of servers request parts of file: part1, part2, part3, etc. We can set up our proxy in such a way 
          that it can recognize the spatial locality of the individual requests, thus collapsing them into a single request and reading complete file, 
          which will greatly minimize the reads from the data origin.
          
          Proxy are of two types 
          Client proxy to protect the clients
          Server proxy to protect the servers. Also called reverse proxy
          
          Should we use proxy then?
          Only use proxy to protect the client.
          For server side, Always Nginx since its open source and supports both reverse proxy + load balancing + API gateway.
          
          
   
   ==============================================================================

          Queues
          
            Resource :
            - https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/queues.md
            
            When to use queue :
            Queues are implemented on the asynchronous communication protocol, meaning when a client submits a task to a queue they 
            are no longer required to wait for the results; instead, they need only acknowledgment that the request was properly received.
            
            Queues are also used for fault tolerance as they can provide some protection from service outages and failures. For example, 
            we can create a highly robust queue that can retry service requests that have failed due to transient system failures
            
            When not to use queue :
            When client expects respone in real-time
            
            Example of queues : 
             RabbitMQ and Kafka (which is open source)
             
  ====================================================================================
  
        Redundancy and Replication
        
          Resource : 
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/redundancy.md
          
          When to use Redundant system :
          Always. its a key concept of distributed system.
          Always prefer shared nothing architecture so that each node can operate independently and can scale.
          
          
    ====================================================================================
        SQL vs NoSQL
        
          Resource : 
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/sql-vs-nosql.md
          
          SQL 
          Relational databases store data in rows and columns. Example, MySQL, Oracle, MS SQL Server, SQLite, Postgres
          
          NoSQL 
          Key-value :  Example, DynamoDB, Redis, Memcache
          Document DB : Example, MongoDB, 
          Wide-Column Database : Example, HBase, Cassandra - https://hevodata.com/learn/columnar-databases/
          Graph Databases : Example, Neo4j
          
          Differences between both 
          Storage : SQL is tables but NoSQL has different storage models
          Schema : changing schema with SQL is possible but requires whole database modification
          Querying : using SQL for SQL dbs. 
          Scalability : SQL is vertically scalable and possible to scale it horizontally but has limitations. NoSQL is horizontally scalable
          Reliability or ACID : Definitely SQL ensures ACID but NoSQL solutions sacrifice ACID compliance for performance and scalability.
          
          When to use SQL
            ACID compliance for e-commerce and finanicial transactions
            Your data is structured and unchanging.
            Often Read heavy and low-write

            
          When to use NoSQL
            - Storing large volumes of data that often have little to no structure. 
            - Making the most of cloud computing and storage. Cloud-based storage is an excellent cost-saving solution 
            but requires data to be easily spread across multiple servers to scale up. 
            - Rapid development. NoSQL is extremely useful for rapid development as it doesn’t need to be prepped ahead of time. 
            
          What about write performance ? 
            I think SQL provides more flexibility on write performance than NoSQL. But this is contradicting
            However, Cassandra seems to be the choice

            
            
  ====================================================================================
  
    CAP Theorem
    Resource : 
    https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/cap-theorem.md
    
    CAP theorem states that it is impossible for a distributed software system to simultaneously provide more than two out of three of 
    the following guarantees (CAP): Consistency, Availability and Partition tolerance.
    
    We can only build a system that has any two of these three properties. Because, to be consistent, all nodes should see the same set of updates 
    in the same order. But if the network suffers a partition, updates in one partition might not make it to the other partitions before a client 
    reads from the out-of-date partition after having read from the up-to-date one. The only thing that can be done to cope with this possibility 
    is to stop serving requests from the out-of-date partition, but then the service is no longer 100% available.

    When to choose consistency over availability ? 
      Banking, Payment systems
    
    When to choose consistency over availability ? 
      Most distributed system use cases like Google Maps, Twitter, etc.
      
    Since reliability consists of both consistency and availability, ASK YOUR interviewer what it means for system to be 99.99% reliable
    
    
   ====================================================================================
 
    Consistent Hashing
    
    Resource 
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/consistent-hashing.md
      Recommended ones 
        https://medium.com/nerd-for-tech/consistent-hashing-6524e48ac648 - Explains well about rebalancing
        https://bytebytego.com/courses/system-design-interview/design-consistent-hashing
        
      How does it work ? 
        Hashing is done in the range where it maps the key to the integer in that range. Example 0 -> 255 are the integers placed in ring form 
        such that values are wrapped around.
        
        1. the servers are mapped to the integers in that range
        2. to map key to a server, simply hash the key to the integer in that range and move clockwise till you find the server (could be binary search)
        
        Now how this is better :
        1. adding a server : say S1 is added at position 20 and is near S2 at position 25. Then keys from before 20 wuld map to S1.
        2. removing a server : say S1 is removed at position 20 and was near S2 at position 25. Then  all keys even before 20 would map to S2. 
        3. uniform distribution : add “virtual replicas”. Instead of mapping each server to a single point on the ring, 
            we map it to multiple points on the ring, i.e. replicas. 
        4. easy to rebalance : when server is added or removed, only its next clockwise neighbouring server is affected and requires re-mapping of keys.
        
        
    ====================================================================================
    
    Client-Server Communication
    
      Resource : 
        https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/client-server-communication.md
        
      Types : 
      1. Standard HTTP Web Request : client opens the connection, gets response from the server and terminate the connection 
      2. Ajax Polling : client performs HTTP Web Request above periodically (say every 1 s). May create unecessary empty responses if server is not ready.
      3. Long Polling : client performs HTTP Web Request and waits for long time till response is received (say 1 min). Then sends request again.
      4. Web Socket: persistent connection between a client and a server that both parties can use to start sending data at any time.
      5. SSE :  persistent onnection between a client and a server where server send data to the client but not the reverse
      
      When to use Long Polling vs Web Socket ? 
        https://ably.com/blog/websockets-vs-long-polling
        https://dev.to/kevburnsjr/websockets-vs-long-polling-3a0o
        
        Clearly Websockets have many advantages over long polling and thus are appropriate for many applications which require consistent low latency 
        full duplex high frequency communication such as chat applications and real time applications.
        
        Scaling up 
        Websockets have problems with load distribution due to persistent connection.
        Long polling will have equal load distribution after its long timeout
        
     When to use Web Socker vs Server Sent Events ?
        Websocket for bi-directional communication whole SSE for server to client communication
        https://blog.bitsrc.io/websockets-vs-server-sent-events-968659ab0870
        
        WebSockets are widely used and valued in technological solutions such as real-time polling, chat, media players, multiplayer games, etc.
        Server-Sent Events: There are many applications where sending data from the client isn’t necessary. 
        SSEs are especially useful in status updates, social-media news feeds, push notifications, newsletters, etc.
     

      


        
        
        
        
      


    
    
    

          
          
          
          
          
          

          

          
        
        
  
  

      
      



