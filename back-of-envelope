Links :

https://bytebytego.com/courses/system-design-interview/back-of-the-envelope-estimation

https://matthewdbill.medium.com/back-of-envelope-calculations-cheat-sheet-d6758d276b05

Tips :
- Ask functional requirements first (example, ability to post tweet, view tweet)
- Ask DAU. If MAU is provided, atleast ask % of concurrent users. 
- Ask user activity : example read vs write ratio, or number of writes (example, 2 tweets per day)
- Ask non-functional requirements 
  These are very important and can convert entire inteview in your favor. Example if latency is 1s, then you do not need complex caching architecture if your data transfer is less than 100 MB (100 MBps max bandwidth today)
    -- Latency 
           this would help decide whether you really need low latency optimizations like cache, or not. 
           Remember today mosts have 1 Gbps network which has 10 us to transfer 1 KB
    -- Availability 
          this would help decide the redundancy and reliability - example mostly 99.99% a day means you can compromise 10s (10 ^ 5 * 10 ^ -4)
          of unavailability or being unrelible. With this you can even decide that, your system could be out of sync from third part servers for 10s in a
          day 
    

QPS estimate
- Read : Write ratio
  DO NOT divided by no. of seconds to get QPS if its explictly mentioned about concurrent users say 10% of DAU. In such cases, QPS = 10% DAU
- 1 day ~ 10^5 s
- 1 month ~ 3 * 10 ^ 6s and further ~ 10 ^ 6s
- 1 week ~ 7 * 10 ^ 5s and further ~ 10 ^ 6s
----- For time constraints, do not hesitate to round off large amounts

How to use non-functional requirements in decison making

  Bandwidth (assume to be 1 GBps ~ 100 MBps which means 1 KB = 10us) 
    - can help reason your latency
    - can help reason your replication or multiple servers
    - can help reason your sharding for database (remember internal data center max bandwidth to 25 Gbps ~ 2.5 GBps which means 1 KB = 1us ) 

  Durability  (I even refer as availability + data correctness with techniques like checksum, parity)
    - can help reason how much to replicate. 
        Example, say 99.999999% (six 9's) durability can be achieved assuming failure rate of one server is 0.01% (say 99.99% guarantee for ec2 on aws). This means you need 2 servers atleast which means 1 replica (0.0001 ^ 2 = 0.00000001. So 1 - 0.00000001 ~ 0.99999999 * 100 ~ 99.999999% ) 
    - can help to reason out if you need checksum for data integrity

  Availability
    - can help reason how much to replicate. See above
    - can help reason for latency 

  Consistency
    - can help reason for high latency 
    - can help reason for low replication
    - can help reason for MySQL if complicated with ACID


Distributed Systems estimate

    Estimates in KB

    Send 1 KB sequential from 1 Gbps network = 10 us  
    Send 1 KB sequential from memory  = 0.25 us   (4 GB/s = 0.25 *  10 ^ -6 s) 
    Send 1 KB sequential from disk    = 30 us     (30 MB/s = 0.03 * 10 ^ -3 s ) 
    Send 1 KB sequential from cache   = 1 us    (1 GB/s = 1 * 10 ^ -6 s )
    Round trip within the same datacenter	 = 500 us 
    Send packet CA (California) ->Netherlands->CA	= 150 ms 
    Mutex lock/unlock =	100 ns
    Disk seek	= 10 ms
    SSD latency = 16 us
    Memory read = 100 ns

    So network is 3 times faster than disk
    memory is 40 times faster than network

    Disk accesses can be eliminated using mmap. `mmap(2)` provides a mechanism for high-performance sharing of memory between processes. Modern exchanges take advantage of this to eliminate as much disk access from the critical path as possible. `mmap(2)` is used in the server to implement a message bus over which the components on the critical path communicate. The communication pathway has no network or disk access, and sending a message on this mmap message bus takes sub-microsecond. By leveraging mmap to build an event store, coupled with the event sourcing design paradigm which we will discuss next, modern exchanges can build low-latency microservices inside a server.


Choices with protocol
    - TCP vs UDP
        https://www.spiceworks.com/tech/networking/articles/tcp-vs-udp/#:~:text=UDP%20is%20faster%20and%20more,connection%20to%20start%20sending%20packets.
        1. TCP is connection-oriented while UDP is connectionless
        The UDP protocol is not suitable for sending electronic mail, viewing a web page, or downloading a file. However, it is preferred mainly for real-time applications like broadcasting or multitasking network traffic.

Concensus algo 
    - gossip prototocol
    - to keep replicas in sync
       Ensure all replicas are always in-sync and master->slave is elected correctly when master goes down. We could use consensus algorithms such as Paxos [21] and Raft [22], or use consensus-based distributed databases such as YugabyteDB [23] or CockroachDB [24].
       - Raft 
            https://bytebytego.com/courses/system-design-interview/stock-exchange 
          The leader sends heartbeat messages (AppendEnties with no content as shown in Figure 21) to its followers. If a follower has not received heartbeat messages for a period of time, it triggers an election timeout that initiates a new election. The first follower that reaches election timeout becomes a candidate, and it asks the rest of the followers to vote (RequestVote). If the first follower receives a majority of votes, it becomes the new leader.


Distributed systems best practices

  Event sourcing
      https://bytebytego.com/courses/system-design-interview/digital-wallet
    One design philosophy that systematically answers those questions is event sourcing, which is a technique developed in Domain-Driven Design (DDD) [9].

  Using LSMTree + SSTable are efficienct for heavy writes and heavy reads on noSQL databases. B tree is not
      Great video : https://www.youtube.com/watch?v=I6jB0nM9SKU 
      Good read : https://rahulpradeep.medium.com/sstables-and-lsm-trees-5ba6c5529325
      This technique is used by Cassandra, RocksDB, Google BigTable, Apache HBase
      MongoDB can be tuned to use LSM treee


Distributed systems today and Benchmarking on common systems

    Map Reduce
      https://www.youtube.com/watch?v=MAJ0aW5g17c (awesome)
        Example of GFS performing map function on worker servers and reduce using worker servers co-ordinated by master

    Zookeeper 
     - centralized configuration
     - The number of partitions and addresses of all Redis nodes can be stored in a centralized place. We could use Zookeeper [4] as a highly-available configuration storage solution. (https://bytebytego.com/courses/system-design-interview/digital-wallet)
     - Apache Zookeeper [7] is a popular open-source solution for service discovery. It registers all the available chat servers and picks the best chat server for a client based on predefined criteria. (https://bytebytego.com/courses/system-design-interview/design-a-chat-system)

    Cache benchmarking

      Azure Redis Cache : 
        53 GB, with 99.9% availability (https://github.com/Huachao/azure-content/blob/master/articles/redis-cache/cache-faq.md)
        230 us (https://redis.io/docs/management/optimization/latency/)
        Redis QPS - 50K https://redis.io/docs/management/optimization/benchmarks/
        1 million TPS - https://bytebytego.com/courses/system-design-interview/digital-wallet 
        Redis also privides sorted set data structure. Our leaderboard use case maps perfectly to sorted sets. Internally, a sorted set is implemented by two data structures: a hash table and a skip list [1]. The hash table maps users to scores and the skip list maps scores to users. In sorted sets, users are sorted by scores. (https://bytebytego.com/courses/system-design-interview/real-time-gaming-leaderboard)
        Redis cluster also supports sharding with hash slot ( different from consistent hashing)
        Redis cluster uses master-replica model
        Redis Cluster does not guarantee strong consistency

    SQL DB benchmarking - Relational
        1. MySQL benchmarking
          - Indexed Read
              a. with primary key reads which is already indexed
                (https://dev.mysql.com/blog-archive/mysql-connection-handling-and-scaling/)
                Recommend 4 * number of cores parallel for ~ 200 us based on simple primary key look ups. For example, 120 (~4 * 32) parellel requests for 32 cores will take 200 us
                1K requests on 32 will take ~ 1ms for simply select query
          
          - Write
            7 K writes/s for 48 core http://minervadb.com/wp-content/uploads/2020/10/MySQL-8-Performance-Benchmarking-on-Amazon-EC2.pdf 
            impact of write performance on indexing
              4 times slower : https://logicalread.com/impact-of-adding-mysql-indexes-mc12/#.Y__7d-zMLuU 
              ~1 K writes/s for 48 core

          - General benchmarking
            Today, a relational database running on a typical data center node can support a few thousand transactions per second.


          - Max size set by MySQL
            256 TB (https://dev.mysql.com/doc/mysql-reslimits-excerpt/8.0/en/table-size-limit.html#:~:text=You%20are%20using%20a%20MyISAM,2567%20%E2%88%92%201%20bytes)
            AWS limit : 64 TB
            According to Amazon Relational Database Service (RDS) [12], you can get a database server with 24 TB of RAM.

          Some facts
          - MySQL client <-> server is socket based connection
          - Max connection limit for MySQL = 100 K
          - per connection -> one user thread
          - size of user thread depends on THD connection data structure which is per connection ~ 10 MB 
          - max THD ~ 10K 
          - So recommended min size of your MySQL = 10 MB * 10 K = 100 GB ??

          - MySQL sorting (https://www.pankajtanwar.in/blog/what-is-the-sorting-algorithm-behind-order-by-query-in-mysql)
              External merge sort (quick sort + merge sort) if data doesn’t fits into the memory
              Quick sort, if data fits into the memory and we want all of it
              Heap sort, if data fits into the memory but we are using LIMIT to fetch only some results
              Index lookup (not exactly a sorting algorithm, just a pre-calculated binary tree)

          - MySQL query cache (https://docs.oracle.com/cd/E17952_01/mysql-5.1-en/query-cache.html)
              As of MySQL 5.1.63, the query cache is not supported for partitioned tables, and is automatically disabled for queries involving partitioned tables. The query cache cannot be enabled for such queries. 

          - MySQL sharding (https://stackoverflow.com/questions/1610887/how-to-partition-mysql-across-multiple-servers)
              this is different from mysql partitioning (https://vertabelo.com/blog/everything-you-need-to-know-about-mysql-partitions/)
          
          - Moreover, MySQL involves no standard implementation for sharding. (https://kinsta.com/blog/mongodb-vs-mysql/)

          - MySQL indexing 
              https://www.youtube.com/watch?v=YuRO9-rOgv4 using B tree
              B tree performance : https://www.youtube.com/watch?v=FgWbADOG44s 
              Its inefficient as compared to B tree for writes : https://www.youtube.com/watch?v=I6jB0nM9SKU 

          - Practical
              how MySQL gurantees ACID : https://www.youtube.com/watch?v=0OYFsJ1-1YA
              Nice theoretical : https://www.youtube.com/watch?v=clPPKgYJC10 
                Atomicity : begin trans --> comitt statement
                Consistency : integrity constraints (guessing)
                Isolation : locks
                Durability : ? 

            



    NoSQL value DB benchmarking
        1. Timeseries : OpenTSDB / InfluxDB
              8 cores and 32GB RAM can handle over 250,000 writes per second, greater than 1 M series
              (https://bytebytego.com/courses/system-design-interview/metrics-monitoring-and-alerting-system)

        2. Object storage / cold storage benchmarking
            S3 Standard is designed for 99.99% data availability and durability of 99.999999999% of objects across multiple Availability Zones in a given year. AWS S3 provides a great performance. It automatically scales to high request rates, with a very low latency of 100–200 milliseconds.Your application can achieve at least 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket (https://aws.plainenglish.io/optimize-your-aws-s3-performance-27b057f231a3)

        3. Cassandra

              100s video : https://www.youtube.com/watch?v=ziq7FUKpCS8
              Slower version of 100s : https://www.youtube.com/watch?v=YjYWsN1vek8 
              
              Not super helpful though : 
                Cassandra data model : https://www.youtube.com/watch?v=X-_vS8q4nu4&list=PLGkHHA2RcrgeV35CBtIlUjiXJzVXLxMbL&index=3
                How Cassandra data is  stored :  https://www.youtube.com/watch?v=69dLARZxIVw&list=PLGkHHA2RcrgeV35CBtIlUjiXJzVXLxMbL&index=4 
                How Cassandra read / write data : https://www.youtube.com/watch?v=t276nhhkkI8&t=523s
              
              Cassandra indexing
                Paritioning : https://www.youtube.com/watch?v=Np5RJpCiCzM
                Clustering used as sort key : https://www.youtube.com/watch?v=OCakxrzwuU4 
                  Remember data in cassandra on disk supports SSTable which is sorted string table.
                  So searching could be done in logN times like relational or mongoDB

              Practical 
                    Cassandra indexing (partition + clustering which is sort key) : https://www.youtube.com/watch?v=S9rmf4X7E_E

              Garbage collection
                4.0 uses Z GC instead of G1 GC (java8 to java 16)

              Cassandra and map-reduce
                https://subscription.packtpub.com/book/data/9781787127296/1/ch01lvl1sec5/mapreduce-and-spark
                Unlike MongoDB, Cassandra does not offer built-in MapReduce capabilities. But it can be integrated with Hadoop in order to perform MapReduce operations across Cassandra data sets, or Spark for real-time data analysis. 

              Cassandra performance on writes + reads on indexes 
                - LSM tree + SS tables
                Great video : https://www.youtube.com/watch?v=I6jB0nM9SKU 
                Good read : https://rahulpradeep.medium.com/sstables-and-lsm-trees-5ba6c5529325 
                

              Benchmarking
                - On Cheap heardware https://benchant.com/blog/cassandra-4-performance
                  m5.large ( 2 core, 8 GB) 
                  Read : 7 K ops/s
                  Write : 7 K ops/s
                  Read Latency : ~10 ma
                  Write Latency : ~7 ms

                - Expensive hardware https://www.scylladb.com/2021/08/19/cassandra-4-0-vs-cassandra-3-11-comparing-performance/
                  i3.4xlarge (16 vCPUs, 122 GB)
                   Read : 30 K ops/s
                   Write : 20 K ops/s
                   Read Latency : ~10 ms
                   Write Latency : ~10 ms

        4. MongoDB
              - How Mongo DB search works : https://www.youtube.com/watch?v=tSgPhxZdhLk&t=96s 
                  using index in B tree
              - MongoDB supports range-based or hash-based sharding - https://kinsta.com/blog/mongodb-vs-mysql/
              - For aggregations, Instead of map-reduce, you should use an aggregation pipeline. Aggregation pipelines provide better performance and usability than map-reduce. https://www.mongodb.com/docs/manual/core/map-reduce/
              - How Mongo DB sharding works : https://kinsta.com/blog/mongodb-vs-mysql/ 
              - Practical
                  set up mongo based sharding https://www.youtube.com/watch?v=mjSNKjTzeao 
                  mongoDB gridFS https://www.youtube.com/watch?v=mZE3aBdr010 

        6. Dynamo DB
            Nice intro in https://bytebytego.com/courses/system-design-interview/real-time-gaming-leaderboard

            DynamoDB is a fully managed NoSQL database that offers reliable performance and great scalability. To allow efficient access to data with attributes other than the primary key, we can leverage global secondary indexes [16] in DynamoDB. A global secondary index contains a selection of attributes from the parent table, but they are organized using a different primary key. 
            So Every global secondary index must have a partition key, and can have an optional sort key. The index key schema can be different from the base table schema. 

            DynamoDB splits data across multiple nodes using consistent hashing.


  Attempt to standardized DB selection criteria 
      1. MySQL vs noSQL 

            When MySQL
            if ACID, 
              then MYSQL (high confidence)
            if reasonable size (< 1 TB) not expected to scale rapidly,
              if heavy reads (reading from read replicas is faster) 
                if low writes 
                  then MySQL (high confidence)
                if heavy writes (else have to keep synchronizing)
                  then MySQL (low confidence)
            If db needs rapid scale (size keeps growing)
              if manual sharding is needed ( say by user id )
                if sharding has all related rows (try to denormalize)
                  if there are less aggregations 
                    then MySQL (medium confidence)
                  if there are more aggregations 
                    then MySQL (low confidence since aggregations with sharding could be better achieved with noSQL DBs like MongoDB, Cassandra)
            If db needs better search performance (not full text search)  
              if index-ing on fixed columns,
                then MySQL (high confidence)
              if db experience heavy writes
                then MySQL (low confidence since indexes increases write latency) 

      2.  MySQL vs MongoDB

      3.  MongoDB vs Cassandra
            https://www.youtube.com/watch?v=3z2EzILA3Rk 

                Cassandra                          vs               MongoDB 
            column data model                                   json document data model
            no config server(ring structure)                    mongo master, slave and mongo config servers for replication
            high availability due to multiple masters           master -> slave takes time
            accept writes in parallel                           write capacity is limited since they just go to master
            similar to SQL                                        based on json formatting

            When mongo DB
              query also based on secondary indexing and flexible querying
              built in data aggregation framework
              read-heavy workload

            When Cassandra 
                query based on primary key indexing 
                100% uptime guarantee due to replication and inconsistency resolution
                high write speed
                language support for SQL
                write-heavy, read-heavy workload
                eventual consistency (as per bytebytego - https://bytebytego.com/courses/system-design-interview/design-a-key-value-store )

      4. When S3

      5. When Neo4j

      Open questions
            1. Why is MySQL better for read heavy low write workload ? answered above
            2. MySQL is better for ACID systems and structured ? How ? 
            2. How MySQL syncs with its replicas
            2. Why is Cassandra (noSQL) better for read less write heavy workload ?
            3. Aggregation on sharded MySQL needs Application level handling vs Cassandra or HBase(Column) can handle this since they are built on top of distributed systems . Fact and derived above
            4. How MySQL indexing works ? 
            5. Cassandra benchmarking ? 
            6. Study about graphdbs ? 


================================================================
================================================================
================================================================

References

Dump : https://coursehunters.online/t/educative-io-design-gurus-grokking-the-system-design-interview-part-5/584
Quick read : https://github.com/Jeevan-kumar-Raj/Grokking-System-Design


================================================================

- Key characteristics
    Resources
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/key-characteristics.md 
    Scalability, Availability, Reliability, Efficiency and Servicability/Manageability
    -> Remember an Avaiability system does not means Reliable but Reliable system is always available. 

  How to handle burst / spiky reads ? 
    Use Nignx to do request collapsing for similar type of content
    Use queue if content does not need to be delivered real-time
    Other strategies : Data Cache, CDN (https://www.onecloudsystems.com/2016/10/25/how-to-ensure-site-can-handle-traffic-spikes/)

  Pull vs push models
    In Pull approach, the metrics collector needs to know the complete list of service endpoints to pull data from. The good news is that we have a reliable, scalable, and maintainable solution available through Service Discovery, provided by etcd [14], Zookeeper [15], etc., wherein services register their availability and the metrics collector can be notified by the Service Discovery component whenever the list of service endpoints changes. 
    If a server goes down, then you can re-try in the next pull. But here you need dedupe logic or  store the offset in S3 to know what messages to retry.

    In a push model, a collection agent is commonly installed on every server being monitored. Aggregation is an effective way to reduce the volume of data sent to the metrics collector. If the push traffic is high and the metrics collector rejects the push with an error, the agent could keep a small buffer of data locally (possibly by storing them locally on disk), and resend them later.
    If a server goes down or cannot handle burst traffic, then you loose the message.

  Tips to ensure consistency 
    - To maintain data consistency between internal services, ensuring exactly-once processing is very important.
    - To maintain data consistency between the internal service and external service (PSP), we usually rely on idempotency and reconciliation.
    - For replicas, ensure acknowledge only if all replicas respond to read/write update. 

    
   
================================================================================================================================
- Key learnings from system design

  Scale for Zero to millions of users 
      https://bytebytego.com/courses/system-design-interview/scale-from-zero-to-millions-of-users
    
      - If the master database goes offline, a slave database will be promoted to be the new master. All the database operations will be temporarily executed on the new master database. A new slave database will replace the old one for data replication immediately. In production systems, promoting a new master is more complicated as the data in a slave database might not be up to date. The missing data needs to be updated by running data recovery scripts. Although some other replication methods like multi-masters and circular replication could help, those setups are more complicated; and their discussions are beyond the scope of this course. Interested readers should refer to the listed reference materials [4] [5].
        https://en.wikipedia.org/wiki/Multi-master_replication 

      Cache

        - Interacting with cache servers is simple because most cache servers provide APIs for common programming languages. 

        - Consistency: This involves keeping the data store and the cache in sync. Inconsistency can happen because data-modifying operations on the data store and cache are not in a single transaction. When scaling across multiple regions, maintaining consistency between the data store and cache is challenging. For further details, refer to the paper titled “Scaling Memcache at Facebook” published by Facebook [7].

        - Eviction Policy: Once the cache is full, any requests to add items to the cache might cause existing items to be removed. This is called cache eviction. Least-recently-used (LRU) is the most popular cache eviction policy. 

      CDN (also known as PoP - point of presence)

        Considerations of using a CDN 
          CDN fallback: You should consider how your website/application copes with CDN failure. If there is a temporary CDN outage, clients should be able to detect the problem and request resources from the origin.

          Invalidating files: You can remove a file from the CDN before it expires by performing one of the following operations:

          Invalidate the CDN object using APIs provided by CDN vendors.

          Use object versioning to serve a different version of the object. To version an object, you can add a parameter to the URL, such as a version number. For example, version number 2 is added to the query string: image.png?v=2.

          Pull vs Push CDN : https://www.belugacdn.com/push-cdn/

      Stateful architecture

        The issue is that every request from the same client must be routed to the same server. This can be done with sticky sessions in most load balancers [10]; however, this adds the overhead. Adding or removing servers is much more difficult with this approach. It is also challenging to handle server failures.

      Data centers
        Figure 15 shows an example setup with two data centers. In normal operation, users are geoDNS-routed, also known as geo-routed, to the closest data center, with a split traffic of x% in US-East and (100 – x)% in US-West. geoDNS is a DNS service that allows domain names to be resolved to IP addresses based on the location of a user.
        Geo-routing is done through load balancers

        Several technical challenges must be resolved to achieve multi-data center setup:

          Traffic redirection: Effective tools are needed to direct traffic to the correct data center. GeoDNS can be used to direct traffic to the nearest data center depending on where a user is located.

          Test and deployment: With multi-data center setup, it is important to test your website/application at different locations. Automated deployment tools are vital to keep services consistent through all the data centers [11].

      Message queue
        To further scale our system, we need to decouple different components of the system so they can be scaled independently. Messaging queue is a key strategy employed by many real-world distributed systems to solve this problem.
        A message queue is a durable component, stored in memory, that supports asynchronous communication. It serves as a buffer and distributes asynchronous requests.

      Logging, metrics, automation
        Logging: Monitoring error logs is important
        Metrics: Collecting different types of metrics help us to gain business insights and understand the health status of the system. Some of the following metrics are useful:
          Host level metrics: CPU, Memory, disk I/O, etc.
          Aggregated level metrics: for example, the performance of the entire database tier, cache tier, etc.
          Key business metrics: daily active users, retention, revenue, etc.
        Automation: When a system gets big and complex, we need to build or leverage automation tools to improve productivity. Continuous integration is a good practice, in which each code check-in is verified through automation, allowing teams to detect problems early. Besides, automating your build, test, deploy process, etc. could improve developer productivity significantly.

      Vertical scaling
        Vertical scaling, also known as scaling up, is the scaling by adding more power (CPU, RAM, DISK, etc.) to an existing machine. There are some powerful database servers. According to Amazon Relational Database Service (RDS) [12], you can get a database server with 24 TB of RAM.

      Horizontal scaling
        Horizontal scaling, also known as sharding, is the practice of adding more servers.


  Design A Rate Limiter
    https://bytebytego.com/courses/system-design-interview/design-a-rate-limiter

      Where to put the rate limiter?
        Besides the client and server-side implementations, there is an alternative way. Instead of putting a rate limiter at the API servers, we create a rate limiter middleware, which throttles requests to your APIs as shown in Figure 2.
        the rate limiter middleware throttles the third request and returns a HTTP status code 429. The HTTP 429 response status code indicates a user has sent too many requests.

        Cloud microservices [4] have become widely popular and rate limiting is usually implemented within a component called API gateway.

      Algorithms for rate limiting

        Token bucket algorithm
          The token bucket algorithm is widely used for rate limiting. It is simple, well understood and commonly used by internet companies. Both Amazon [5] and Stripe [6] use this algorithm to throttle their API requests.

          The token bucket algorithm takes two parameters:
            Bucket size: the maximum number of tokens allowed in the bucket
            Refill rate: number of tokens put into the bucket every second

          How many buckets do we need? This varies, and it depends on the rate-limiting rules. Here are a few examples.
            It is usually necessary to have different buckets for different API endpoints. For instance, if a user is allowed to make 1 post per second, add 150 friends per day, and like 5 posts per second, 3 buckets are required for each user.
            If we need to throttle requests based on IP addresses, each IP address requires a bucket.
            If the system allows a maximum of 10,000 requests per second, it makes sense to have a global bucket shared by all requests.

        Leaking bucket algorithm
          Shopify, an ecommerce company, uses leaky buckets for rate-limiting [7].

          The leaking bucket algorithm is similar to the token bucket except that requests are processed at a fixed rate. It is usually implemented with a first-in-first-out (FIFO) queue. The algorithm works as follows:

            When a request arrives, the system checks if the queue is full. If it is not full, the request is added to the queue.
            Otherwise, the request is dropped.
            Requests are pulled from the queue and processed at regular intervals.

        Fixed window counter algorithm
            Fixed window counter algorithm works as follows:

              The algorithm divides the timeline into fix-sized time windows and assign a counter for each window.
              Each request increments the counter by one.
              Once the counter reaches the pre-defined threshold, new requests are dropped until a new time window starts.

              Cons:
                Spike in traffic at the edges of a window could cause more requests than the allowed quota to go through.

        Sliding window log algorithm
            The sliding window log algorithm fixes the issue. It works as follows:

                  The algorithm keeps track of request timestamps. Timestamp data is usually kept in cache, such as sorted sets of Redis [8].
                  When a new request comes in, remove all the outdated timestamps. Outdated timestamps are defined as those older than the start of the current time window.
                  Add timestamp of the new request to the log.
                  If the log size is the same or lower than the allowed count, a request is accepted. Otherwise, it is rejected.

            Pros:     
              Rate limiting implemented by this algorithm is very accurate. In any rolling window, requests will not exceed the rate limit.
            Cons:
              The algorithm consumes a lot of memory because even if a request is rejected, its timestamp might still be stored in memory.

        Sliding window counter algorithm (too complicated)

      High-level architecture
          The basic idea of rate limiting algorithms is simple. At the high-level, we need a counter to keep track of how many requests are sent from the same user, IP address, etc. If the counter is larger than the limit, the request is disallowed.
          Where shall we store counters? Using the database is not a good idea due to slowness of disk access. In-memory cache is chosen because it is fast and supports time-based expiration strategy. For instance, Redis [11] is a popular option to implement rate limiting.

        Rate limiting rules
          Lyft open-sourced their rate-limiting component [12]. We will peek inside of the component and look at some examples of rate limiting rules:
          This rule shows that clients are not allowed to login more than 5 times in 1 minute. Rules are generally written in configuration files and saved on disk.

        Exceeding the rate limit
          Rate limiter headers
            How does a client know whether it is being throttled? And how does a client know the number of allowed remaining requests before being throttled? The answer lies in HTTP response headers. 
            When a user has sent too many requests, a 429 too many requests error and X-Ratelimit-Retry-After header are returned to the client.

        Rate limiter in a distributed environment
          Race condition
            Locks are the most obvious solution for solving race condition. However, locks will significantly slow down the system. Two strategies are commonly used to solve the problem: Lua script [13] and sorted sets data structure in Redis [8]. 

          Synchronization issue
            As the web tier is stateless, clients can send requests to a different rate limiter as shown on the right side of Figure 15. If no synchronization happens, rate limiter 1 does not contain any data about client 2. Thus, the rate limiter cannot work properly.
            One possible solution is to use sticky sessions that allow a client to send traffic to the same rate limiter. This solution is not advisable because it is neither scalable nor flexible. A better approach is to use centralized data stores like Redis. 

          Performance optimization
            First, multi-data center setup is crucial for a rate limiter because latency is high for users located far away from the data center. 
            Second, synchronize data with an eventual consistency model. If you are unclear about the eventual consistency model, refer to the “Consistency” section in the “Design a Key-value Store” chapter.

          Monitoring
            After the rate limiter is put in place, it is important to gather analytics data to check whether the rate limiter is effective. Primarily, we want to make sure:
              The rate limiting algorithm is effective.
              The rate limiting rules are effective.

          Rate limiting at different levels. 
            In this chapter, we only talked about rate limiting at the application level (HTTP: layer 7). It is possible to apply rate limiting at other layers. For example, you can apply rate limiting by IP addresses using Iptables [15] (IP: layer 3). Note: The Open Systems Interconnection model (OSI model) has 7 layers [16]: Layer 1: Physical layer, Layer 2: Data link layer, Layer 3: Network layer, Layer 4: Transport layer, Layer 5: Session layer, Layer 6: Presentation layer, Layer 7: Application layer.

      TODO
        - Study how Lua Script or Sorted Set prevents race conditions ? 



  Proximity Service 
    https://bytebytego.com/courses/system-design-interview/proximity-service
    
        Different types of address look up
        two dimensional search query, Geohash, Quadtree

        High level storage based on read : write ratio
          If Read ratio is high than writes, remember to apply master/slave architecture for storage  replica for reads and master/primary for write 

        Sharding and Replicating 
          These are two different concepts and usually you need both. Sharding is very common though.
          However, if your DB requirements are not huge (say 10 GB), you can simply create replicas to distribute read load than sharding it

        Operation complexity with in-memory data
          - If your in-memory data is large (in GBs), then it takes time to start up and serve traffic. Needs gradual roll out of new releases or 
          blue green deployment technique (https://martinfowler.com/bliki/BlueGreenDeployment.html)
          - Locking mechanism is needed for updates in a multithread environment
      
      
    NearBy Friends
    https://bytebytego.com/courses/system-design-interview/nearby-friends
    
        Back of envelope
        Req is 10% of DAU are concurrent. In that case, QPS is 10% of DAU and do not need additional calculations

        High level design vs API design + data model 
          Questions like this needs high level design first
        
        Update and Notify system like this 
          Always prefer Websocket connection which also works well with load balancer. Attach the user to web socket servers.
          Leverage impressive lightweight cache systems like Redis pub/sub which allows millions of channels https://redis.io/docs/manual/pubsub/
          
        How does API design for websocket works
          For any request, server is not obligated to send the response.
          Example, 1. for periodic location update -> initiated by client but no response.
                    2. for receiving update from friends -> initiated by server without client request

        WEBsocket servers operational complexity
          Since websocket servers are stateful, hard to auto-scale up / down needs extra care with load balancer help.

        Scaling up/down with stateful servers 
        With stateful clusters, scaling up or down has some operational overhead and risks, so it should be done with careful planning. The cluster is normally over-provisioned to make sure it can handle daily peak traffic with some comfortable headroom to avoid unnecessary resizing of the cluster.

                    
        Cache considerations
          only store most recent user location update. 
          also if cache goes down, no need to pre-fill since it will be having new updates via periodic mechanism


        Suggesting Cassandra for heavy-write horizontal scaled db ????

        There are many service discovery packages available, with etcd [4] and Zookeeper [5] among the most popular ones


        
        TODO : 
        Study more about Websocket connection working
        Study about relational database sharding
        Study about Cassandra (why columnar db is preferrable for write heavy)
        Study about redis well
        Data structure about pub/sub model : queue and hashmap for tracking purpose
        Alternative to Redis pub/sub is "Earlang" - is a general programming language and runtime environment built for highly distributed and concurrent applications.

         
    Distributed Message Queue
    https://bytebytego.com/courses/system-design-interview/distributed-message-queue

      Strictly speaking, Apache Kafka and Pulsar are not message queues as they are event streaming platforms. However, there is a convergence of features that starts to blur the distinction between message queues (RocketMQ, ActiveMQ, RabbitMQ, ZeroMQ, etc.) and event streaming platforms (Kafka, Pulsar). 

      Messaging models
        The most popular messaging models are point-to-point and publish-subscribe. Our distributed message queue supports both models. The publish-subscribe model is implemented by topics, and the point-to-point model can be simulated by the concept of the consumer group

        Point-to-point
          This model is commonly found in traditional message queues. In a point-to-point model, a message is sent to a queue and consumed by one and only one consumer.

        Publish-subscribe
          First, let’s introduce a new concept, the topic. Topics are the categories used to organize messages. Each topic has a name that is unique across the entire message queue service. Messages are sent to and read from a specific topic.

          In the publish-subscribe model, a message is sent to a topic and received by the consumers subscribing to this topic.

        Data storage
        Option 1: Database
          The first option is to use a database.
          Relational database: create a topic table and write messages to the table as rows.
          NoSQL database: create a collection as a topic and write messages as documents.

        Option 2: Write-ahead log (WAL)
          The second option is write-ahead log (WAL). WAL is just a plain file where new entries are appended to an append-only log. WAL is used in many systems, such as the redo log in MySQL and the WAL in ZooKeeper.

        A note on disk performance

          To meet the high data retention requirement, our design relies heavily on disk drives to hold a large amount of data. There is a common misconception that rotational disks are slow, but this is really only the case for random access. For our workload, as long as we design our on-disk data structure to take advantage of the sequential access pattern, the modern disk drives in a RAID configuration (i.e., with disks striped together for higher performance) could comfortably achieve several hundred MB/sec of read and write speed. This is more than enough for our needs, and the cost structure is favorable.

        Batching
          Batching is pervasive in this design. 
          There is a tradeoff between throughput and latency. If the system is deployed as a traditional message queue where latency might be more important, the system could be tuned to use a smaller batch size.

        Producer flow
          This is very important since it will answer the question why Kafka can scale better.
          Basically, zookeeper takes care of which broker to assign 

        Consumer flow
          most message queues choose the pull model instead of push  

        TODO : 
        Study more about Apache Zookeeper
        Working of Apacke Kafka  


      
    Design Fund raising App
        https://excalidraw.com/#room=f014e3afcbd4ddc43b20,SKkDU1jvFs-vLQNoH7pHmg

        Asymmetric encryption could be used here for payment gateway
          https://www.youtube.com/watch?v=AQDCe585Lnc
          also used in HTTPS, SSH, Bitcoin, Emails using PGP protocol

        How HTTPS works ? How it uses SSL and TLS
          https://www.youtube.com/watch?v=hExRDVZHhig
          HTTPS uses public key encryption to secure data using SSL (Secure socket layer) protocol. Basically server gives SSL certicate to client and acknowledgement is established between the two. Then info can be transferred securely using encryption.
          TLS is latest and successor of SSL
          Today most websites supports https because of google standards

          ByteByteGo : https://www.youtube.com/watch?v=j9QmMEWmcfo
            Moderm HTTPS uses TLS
            1. first establishes TCP handshake at transport 
            2. then client sends hello and gets the certificate from server which has public key of server (Asymmetric) 
            3. then client encryptes his/her session key with server's public key and then on server, it gets clients sesssion key by decrypting with server's private key. This is Asymmetric encryption
            4. Now both has session key and uses session key as cipher to encrypt and decrypt at both sides. This is Symmetric encryption.

            SSL uses public key encryption (Asymmetric encryption) only

          How SSO work ? 
            https://www.youtube.com/watch?v=O1cRJWYF-g4
            uses public key encryption
          
          Oauth 2.0 work ? 
            https://www.youtube.com/watch?v=CPbvxxslDTU

            Related to above 
            How payment gateway work ?
              https://www.youtube.com/watch?v=GUurzvS3DlY


        TODO : 
          Learn about public Asymmetric encryption (key / private key) encryprion - https://www.youtube.com/watch?v=AQDCe585Lnc
          Learn about HTTPS encryption (uses Asymmetric encryption)
          Learn about end to end encrytion
          Learn about how SSO work

    Metrics Monitoring and Alerting System
        https://bytebytego.com/courses/system-design-interview/metrics-monitoring-and-alerting-system

        Candidate: What is the scale of the infrastructure we are monitoring with this system?
            Interviewer: 100 million daily active users, 1,000 server pools, and 100 machines per pool.
        The infrastructure being monitored is large-scale.
            100 million daily active users
            Assume we have 1,000 server pools, 100 machines per pool, 100 metrics per machine => ~10 million metrics

            Storage : 
            1 month : ~ 10 ^ 6 (10 mil metrics) * 10 ^ 6  * 10 KB 
                    : ~ 10 ^ 13 KB
                    : ~10 ^ 4 TB
            > 1 month : ~ 10 ^ 3 TB
            1 year : 12 * 1 month ~ 10 ^ 5 TB

            Burst read  : 
            UI QPS : (100 million DAU / 10 ^ 5) * 2 (peak) 
                   : 2K QPS
            Bandwidth QPS : 2000 QPS * (100 metrics) * 10 ^ 6 (1 week) * 1 KB
                          :  ~ 10 ^ 11 KB
                          :  ~ 10 ^ 5 GB 
                          : internal datacenter bandwith needs to be  ~ 10 ^ 5 GB -> needs high compression and request collapsing
                          : per user data transfer 10 ^ 2 GB

        Data storage system
          a relational database is not optimized for operations you would commonly perform against time-series data. For example, computing the moving average in a rolling time window requires complicated SQL that is difficult to read. Moreover, a general-purpose relational database does not perform well under constant heavy write load. 

          Again Cassandra was brought up in discussion for write heavy ??? Why ? 
            May be its good for bulk new writes 
          
          OpenTSDB is a distributed time-series database, but since it is based on Hadoop and HBase, running a Hadoop/HBase cluster adds complexity. Another feature of a strong time-series database is efficient aggregation and analysis of a large amount of time-series data by labels, also known as tags in some databases. For example, InfluxDB builds indexes on labels to facilitate the fast lookup of time-series by labels

        Metrics collection - Pull vs push models
          In Pull approach, the metrics collector needs to know the complete list of service endpoints to pull data from. The good news is that we have a reliable, scalable, and maintainable solution available through Service Discovery, provided by etcd [14], Zookeeper [15], etc., wherein services register their availability and the metrics collector can be notified by the Service Discovery component whenever the list of service endpoints changes.

          In a push model, a collection agent is commonly installed on every server being monitored. Aggregation is an effective way to reduce the volume of data sent to the metrics collector. If the push traffic is high and the metrics collector rejects the push with an error, the agent could keep a small buffer of data locally (possibly by storing them locally on disk), and resend them later.

        Scale through Kafka
          There are a couple of ways that we can leverage Kafka’s built-in partition mechanism to scale our system.
            Configure the number of partitions based on throughput requirements.
            Partition metrics data by metric names, so consumers can aggregate data by metrics names.
            Further partition metrics data with tags/labels.
            Categorize and prioritize metrics so that important metrics can be processed first.
        
        Where aggregations can happen
          Metrics can be aggregated in different places; in the collection agent (on the client-side), the ingestion pipeline (before writing to storage), and the query side (after writing to storage). Let’s take a closer look at each of them.

        Space optimization in time-series DB
          Data encoding and compression
            Example, rather than storing absolute values, the delta of the values can be stored along with one base value like: 1610087371, 10, 10, 9, 11
          Downsampling
            Since our data retention is 1 year, we can downsample old data.
          Cold storage
            Cold storage is the storage of inactive data that is rarely used. The financial cost for cold storage is much lower.


        Alerting system - build vs buy 
          There are many industrial-scale alerting systems available off-the-shelf, and most provide tight integration with the popular time-series databases. Many of these alerting systems integrate well with existing notification channels, such as email and PagerDuty. In the real world, it is a tough call to justify building your own alerting system. In interview settings, especially for a senior position, be ready to justify your decision

        TODO : 
        TCP vs UDP
        Hot vs Cold Storage : https://www.youtube.com/watch?v=90EBp9wkoYM
 

  Ad Click Event Aggregation
      https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation

      Raw data vs Aggregated Data
        Should we store raw data or aggregated data? Our recommendation is to store both. Let’s take a look at why.
          1. It’s a good idea to keep the raw data. If something goes wrong, we could use the raw data for debugging. If the aggregated data is corrupted due to a bad bug, we can recalculate the aggregated data from the raw data, after the bug is fixed.
          2. Aggregated data should be stored as well. The data size of the raw data is huge. The large size makes querying raw data directly very inefficient. To mitigate this problem, we run read queries on aggregated data.
          3. Raw data serves as backup data. We usually don’t need to query raw data unless recalculation is needed. Old raw data could be moved to cold storage to reduce costs.
          4. Aggregated data serves as active data. It is tuned for query performance.


      Choose the right database
        When it comes to choosing the right database, we need to evaluate the following:
        What does the data look like? Is the data relational? Is it a document or a blob?
        Is the workflow read-heavy, write-heavy, or both?
        Is transaction support needed?
        Do the queries rely on many online analytical processing (OLAP) functions [3] like SUM, COUNT?

        As shown in the back of the envelope estimation, the average write QPS is 10,000, and the peak QPS can be 50,000, so the system is write-heavy. On the read side, raw data is used as backup and a source for recalculation, so in theory, the read volume is low.
        Relational databases can do the job, but scaling the write can be challenging. NoSQL databases like Cassandra and InfluxDB are more suitable because they are optimized for write and time-range queries.
      
      Asynchronous processing
        The design we currently have is synchronous. This is not good because the capacity of producers and consumers is not always equal. Consider the following case; if there is a sudden increase in traffic and the number of events produced is far beyond what consumers can handle, consumers might get out-of-memory errors or experience an unexpected shutdown. If one component in the synchronous link is down, the whole system stops working.

        A common solution is to adopt a message queue (Kafka) to decouple producers and consumers. This makes the whole process asynchronous and producers/consumers can be scaled independently.
        You might be wondering why we don’t write the aggregated results to the database directly. The short answer is that we need the second message queue like Kafka to achieve end-to-end exactly-once semantics (atomic commit). "Exactly-once" feature in distributed queues.

      Aggregation service
        The MapReduce framework is a good option to aggregate ad click events. The directed acyclic graph (DAG) is a good model for it [9]. The key to the DAG model is to break down the system into small computing units, like the Map/Aggregate/Reduce nodes
          Map node
            You might be wondering why we need the Map node. An alternative option is to set up Kafka partitions or tags and let the aggregate nodes subscribe to Kafka directly. This works, but the input data may need to be cleaned or normalized, and these operations can be done by the Map node. Another reason is that we may not have control over how data is produced and therefore events with the same ad_id might land in different Kafka partitions.
          Aggregate node
            An Aggregate node counts ad click events by ad_id in memory every minute. In the MapReduce paradigm, the Aggregate node is part of the Reduce. So the map-aggregate-reduce process really means map-reduce-reduce.
          Reduce node
            A Reduce node reduces aggregated results from all “Aggregate” nodes to the final result. 
        Main use cases
          Use case 1: aggregate the number of clicks
            input events are partitioned by ad_id (ad_id % 3) in Map nodes and are then aggregated by Aggregation nodes.
          Use case 2: return top N most clicked ads
            Input events are mapped using ad_id and each Aggregate node maintains a heap data structure to get the top 3 ads within the node efficiently. In the last step, the Reduce node reduces N ads 
          Use case 3: data filtering
            To support data filtering like “show me the aggregated click count for ad001 within the USA only”, we can pre-define filtering criteria and aggregate based on them.

      Streaming vs batching
        In our design, both stream processing and batch processing are used. We utilized stream processing to process data as it arrives and generates aggregated results in a near real-time fashion. We utilized batch processing for historical data backup.
        For a system that contains two processing paths (batch and streaming) simultaneously, this architecture is called lambda [14]. A disadvantage of lambda architecture is that you have two processing paths, meaning there are two codebases to maintain. Kappa architecture [15], which combines the batch and streaming in one processing path, solves the problem.

        Data recalculation
          Sometimes we have to recalculate the aggregated data, also called historical data replay. For example, if we discover a major bug in the aggregation service, we would need to recalculate the aggregated data from raw data starting at the point where the bug was introduced.
      
      Time
        We need a timestamp to perform aggregation. The timestamp can be generated in two different places:
          Event time: when an ad click happens.
          Processing time: refers to the system time of the aggregation server that processes the click event.
          
          Due to network delays and asynchronous environments (data go through a message queue), the gap between event time and processing time can be large.

      Aggregation window
          According to the “Designing data-intensive applications” book by Martin Kleppmann [16], there are four types of window functions: tumbling (also called fixed) window, hopping window, sliding window, and session window. We will discuss the tumbling window and sliding window
          In the tumbling window (highlighted in Figure 15), time is partitioned into same-length, non-overlapping chunks.
          In the sliding window (highlighted in Figure 16), events are grouped within a window that slides across the data stream, according to a specified interval.
      
      Delivery guarantees
          Since the aggregation result is utilized for billing, data accuracy and completeness are very important. The system needs to be able to answer questions such as:

          How to avoid processing duplicate events?
          How to ensure all events are processed?
        Solution : use queue with "exactly once" policy

      Data deduplication
        Client-side. For example, a client might resend the same event multiple times. Duplicated events sent with malicious intent are best handled by ad fraud/risk control components. If this is of interest, please refer to the reference material [18].
        Server outage. If an aggregation service node goes down in the middle of aggregation and the upstream service hasn’t yet received an acknowledgment, the same events might be sent and aggregated again. Let’s take a closer look
        
        The most straightforward solution (Figure 18) is to use external file storage, such as HDFS or S3, to record the offset.
        
        To achieve “exactly-once” processing, we need to put operations between step 4 to step 6 in one distributed transaction. A distributed transaction is a transaction that works across several nodes. If any of the operations fails, the whole transaction is rolled back.

      Scale the system
        Scale the message queue
          We have already discussed how to scale the message queue extensively in the “Distributed Message Queue” chapter, so we’ll only briefly touch on a few points.

          Producers. We don’t limit the number of producer instances, so the scalability of producers can be easily achieved.

          Consumers. Inside a consumer group, the rebalancing mechanism helps to scale the consumers by adding or removing nodes.When there are hundreds of Kafka consumers in the system, consumer rebalance can be quite slow and could take a few minutes or even more. Therefore, if more consumers need to be added, try to do it during off-peak hours to minimize the impact.
        
        Brokers (Remember each broker has multiple topics and and each topic has multiple partitions for parallelization. Each consumer group can be attached to many topics but no two consumers in same consumer group is attached to one partitions. Each consumer in the group maintains its own offset to the partition)

          Hashing key

            Using ad_id as hashing key for Kafka partition to store events from the same ad_id in the same Kafka partition. In this case, an aggregation service can subscribe to all events of the same ad_id from one single partition.

          The number of partitions

            If the number of partitions changes, events of the same ad_id might be mapped to a different partition. Therefore, it’s recommended to pre-allocate enough partitions in advance, to avoid dynamically increasing the number of partitions in production.

          Topic physical sharding

            One single topic is usually not enough. We can split the data by geography (topic_north_america, topic_europe, topic_asia, etc.,) or by business type (topic_web_ads, topic_mobile_ads, etc).

        Scale the aggregation service
            In the high-level design, we talked about the aggregation service being a map/reduce operation.

              Option 1: In same host, Allocate events with different ad_ids to different threads,
              Option 2: Deploy aggregation service nodes on resource providers like Apache Hadoop

        Scale the database
          Cassandra natively supports horizontal scaling, in a way similar to consistent hashing.

      
      Hotspot issue
        A shard or service that receives much more data than the others is called a hotspot. This occurs because major companies have advertising budgets in the millions of dollars and their ads are clicked more often. Since events are partitioned by ad_id, some aggregation service nodes might receive many more ad click events than others, potentially causing server overload.
        This problem can be mitigated by allocating more aggregation nodes to process popular ads. Map-Reduce architecure uses master slave model to handle this resource management.

      Fault tolerance
        Replaying data from the beginning of Kafka is slow. A good practice is to save the “system status” like upstream offset to a snapshot and recover from the last saved status. In our design, the “system status” is more than just the upstream offset because we need to store data like top N most clicked ads in the past M minutes.

      Reconciliation
        Reconciliation means comparing different sets of data in order to ensure data integrity. 
        What we can do is to sort the ad click events by event time in every partition at the end of the day, by using a batch job and reconciling between raw data and  aggregation result.

      Alternative design
        In a generalist system design interview, you are not expected to know the internals of different pieces of specialized software used in a big data pipeline. Explaining your thought process and discussing trade-offs is very important, which is why we propose a generic solution. Another option is to store ad click data in Hive, with an ElasticSearch layer built for faster queries.


      TODO:
      Study about different types of database 
      s3 
      Streaming vs Batching table : https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation




  Hotel Reservation System
  https://bytebytego.com/courses/system-design-interview/hotel-reservation-system

      High-level design
        We use the microservice architecture for this hotel reservation system. Over the past few years, microservice architecture has gained great popularity. Companies that use microservice include Amazon, Netflix, Uber, Airbnb, Twitter, etc. If you want to learn more about the benefits of a microservice architecture, you can check out some good resources [1] [2].

      Data model
          Before we decide which database to use, let’s take a close look at the data access patterns. For the hotel reservation system, we need to support the following queries:
          Query 1: View detailed information about a hotel.
          Query 2: Find available types of rooms given a date range.
          Query 3: Record a reservation.
          Query 4: Look up a reservation or past history of reservations.

          From the back-of-the-envelope estimation, we know the scale of the system is not large but we need to prepare for traffic surges during big events. With these requirements in mind, we choose a relational database because:

          A relational database works well with read-heavy and write less frequently workflows. This is because the number of users who visit the hotel website/apps is a few orders of magnitude higher than those who actually make reservations. NoSQL databases are generally optimized for writes and the relational database works well enough for read-heavy workflow.

          A relational database provides ACID (atomicity, consistency, isolation, durability) guarantees. ACID properties are important for a reservation system. Without those properties, it’s not easy to prevent problems such as negative balance, double charge, double reservations, etc. ACID properties make application code a lot simpler and make the whole system easier to reason about. A relational database usually provides these guarantees.

          A relational database can easily model the data. The structure of the business data is very clear and the relationship between different entities (hotel, room, room_type, etc) is stable. This kind of data model is easily modeled by a relational database.

      Concurrency issues
        Another important problem to look at is double booking. We need to solve two problems: 1) the same user clicks on the “book” button multiple times. 2) multiple users try to book the same room at the same time.

          Option 1: Pessimistic locking
            Pessimistic locking [6], also called pessimistic concurrency control, prevents simultaneous updates by placing a lock on a record as soon as one user starts to update it. Other users who attempt to update the record have to wait until the first user has released the lock 
              Pros:
                Prevents applications from updating data that is being – or has been – changed.
                It is easy to implement and it avoids conflict by serializing updates. Pessimistic locking is useful when data contention is heavy.

              Cons:
                Deadlocks may occur when multiple resources are locked. Writing deadlock-free application code could be challenging.
                This approach is not scalable. If a transaction is locked for too long, other transactions cannot access the resource. This has a significant impact on database performance, especially when transactions are long-lived or involve a lot of entities.


          Option 2: Optimistic locking
            Optimistic locking [7], also referred to as optimistic concurrency control, allows multiple concurrent users to attempt to update the same resource.

            There are two common ways to implement optimistic locking: version number and timestamp. Version number is generally considered to be a better option because the server clock can be inaccurate over time. We explain how optimistic locking works with version number.

            Optimistic locking is usually faster than pessimistic locking because we do not lock the database. However, the performance of optimistic locking drops dramatically when concurrency is high.
            To understand why, consider the case when many clients try to reserve a hotel room at the same time. Because there is no limit on how many clients can read the available room count, all of them read back the same available room count and the current version number. When different clients make reservations and write back the results to the database, only one of them will succeed, and the rest of the clients receive a version check failure message. These clients have to retry. In the subsequent round of retries, there is only one successful client again, and the rest have to retry. Although the end result is correct, repeated retries cause a very unpleasant user experience.

              Pros
                Easy to implement.
                It works well when data contention is minimal.

              Cons:
                Performance is poor when data contention is heavy.

          Option 3: Database constraints
              This approach is very similar to optimistic locking. Let’s explore how it works. In the room_type_inventory table, add the following constraint:

              CONSTRAINT `check_room_count` CHECK((`total_inventory - total_reserved` >= 0))

              Cons
                Similar to optimistic locking, when data contention is heavy, it can result in a high volume of failures. Users could see there are rooms available, but when they try to book one, they get the “no rooms available” response. This experience can be frustrating to users.
                The database constraints cannot be version-controlled easily like the application code.
                Not all databases support constraints. It might cause problems when we migrate from one database solution to another.

      Caching
        New challenges posed by the cache
          Adding a cache layer significantly increases the system scalability and throughput, but it also introduces a new challenge: how to maintain data consistency between the database and the cache.

          When a user books a room, two operations are executed in the happy path:

          Query room inventory to find out if there are enough rooms left. The query runs on the Inventory cache.

          Update inventory data. The inventory DB is updated first. The change is then propagated to the cache asynchronously. This asynchronous cache update could be invoked by the application code, which updates the inventory cache after data is saved to the database. It could also be propagated using change data capture (CDC) [8]. CDC is a mechanism that reads data changes from the database and applies the changes to another data system. One common solution is Debezium [9]. It uses a source connector to read changes from a database and applies them to cache solutions such as Redis [10].

          If you think carefully, you find that the inconsistency between inventory cache and database actually does not matter, as long as the database does the final inventory validation check.

          Let’s take a look at an example. Let’s say the cache states there is still an empty room, but the database says there is not. In this case, when the user queries the room inventory, they find there is still room available, so they try to reserve it. When the request reaches the inventory database, the database does the validation and finds that there is no room left. In this case, the client receives an error, indicating someone else just booked the last room before them. When a user refreshes the website, they probably see there is no room left because the database has synchronized inventory data to the cache, before they click the refresh button.

      Data consistency among services
          In a traditional monolithic architecture [11], a shared relational database is used to ensure data consistency. In our microservice design, we chose a hybrid approach of different microservice with shared relational database.
          However, if your interviewer is a microservice purist, they might challenge this hybrid approach. In their mind, for a microservice architecture, each microservice has its own databases.

          To address the data inconsistency, here is a high-level summary of industry-proven techniques. If you want to read the details, please refer to the reference materials.

            Two-phase commit (2PC) [12]. 2PC is a database protocol used to guarantee atomic transaction commit across multiple nodes, i.e., either all nodes succeeded or all nodes failed. Because 2PC is a blocking protocol, a single node failure blocks the progress until the node has recovered. It’s not performant.

            Saga. A saga is a sequence of local transactions. Each transaction updates and publishes a message to trigger the next transaction step. If a step fails, the saga executes compensating transactions to undo the changes that were made by preceding transactions [13]. 2PC works as a single commit to perform ACID transactions while Saga consists of multiple steps and relies on eventual consistency.

    TODO
      Read about acid system design ?
      Read about 2 phase commit ? 
      Ensure Acid with distributed systems ? 
      Mysql and MySQL benchmarking ?


  
  Payment System
    https://bytebytego.com/courses/system-design-interview/payment-system

    Back-of-the-envelope estimation
        The system needs to process 1 million transactions per day, which is 1,000,000 transactions / 10^5 seconds = 10 transactions per second (TPS). 10 TPS is not a big number for a typical database, which means the focus of this system design interview is on how to correctly handle payment transactions, rather than aiming for high throughput.


    Step 2 - Propose High-Level Design and Get Buy-In
        At a high level, the payment flow is broken down into two steps to reflect how money flows:

        Pay-in flow
        Pay-out flow

        Pay-in flow

          Payment service
            The payment service accepts payment events from users and coordinates the payment process. The first thing it usually does is a risk check, assessing for compliance with regulations such as AML/CFT [2], and for evidence of criminal activity such as money laundering or financing of terrorism.

          Payment Service Provider (PSP)
            A PSP moves money from account A to account B. In this simplified example, the PSP moves the money out of the buyer’s credit card account.

          Ledger
            The ledger keeps a financial record of the payment transaction. he ledger system is very important in post-payment analysis, such as calculating the total revenue of the e-commerce website or forecasting future revenue.

          Wallet (for seller)
            The wallet keeps the account balance of the merchant. It may also record how much a given user has paid in total.


    APIs for payment service
        POST /v1/payments
            This endpoint executes a payment event.
            You may have noticed that the data type of the “amount” field is “string,” rather than “double”. Double is not a good choice because:
              Different protocols, software, and hardware may support different numeric precisions in serialization and deserialization. This difference might cause unintended rounding errors.
              The number could be extremely big (for example, Japan’s GDP is around 5x1014 yen for the calendar year 2020), or extremely small (for example, a satoshi of Bitcoin is 10-8).
        GET /v1/payments/{:id}
            This endpoint returns the execution status of a single payment order based on payment_order_id.

    The data model for payment service
        When we select a storage solution for a payment system, performance is usually not the most important factor.
        Usually, we prefer a traditional relational database with ACID transaction support over NoSQL/NewSQL.
        payment event 
                    Name	              Type
                    checkout_id	        string PK
                    buyer_info	        string
                    credit_card_info	  depends on the card provider
                    is_payment_done	    boolean

        payment order
                            Name	                Type
                        payment_order_id	      String PK
                        checkout_id	          string FK
                        buyer_account	          string
                        amount	                string
                        currency	                string
                        payment_order_status	    string
                        ledger_updated	          boolean
                        wallet_updated	          boolean
        Double-entry ledger system
            There is a very important design principle in the ledger system: the double-entry principle (also called double-entry accounting/bookkeeping [6]). Double-entry system is fundamental to any payment system and is key to accurate bookkeeping. It records every payment transaction into two separate ledger accounts with the same amount. 
            The double-entry system states that the sum of all the transaction entries must be 0. One cent lost means someone else gains a cent.

      Hosted payment page
          Most companies prefer not to store credit card information internally because if they do, they have to deal with complex regulations such as Payment Card Industry Data Security Standard (PCI DSS) [8] in the United States. To avoid handling credit card information, companies use hosted credit card pages provided by PSPs. For websites, it is a widget or an iframe, while for mobile applications, it may be a pre-built page from the payment SDK. 

      Pay-out flow
          The components of the pay-out flow are very similar to the pay-in flow. One difference is that instead of using PSP to move money from the buyer’s credit card to the e-commerce website’s bank account, the pay-out flow uses a third-party pay-out provider to move money from the e-commerce website’s bank account to the seller’s bank account.

      Step 3 - Design Deep Dive
          PSP integration
            If a company chooses not to store sensitive payment information due to complex regulations and security concerns, PSP provides a hosted payment page to collect card payment details and securely store them in PSP. This is the approach most companies take.

          Reconciliation
            When system components communicate asynchronously, there is no guarantee that a message will be delivered, or a response will be returned. So how can we ensure correctness in this case?
            External systems, such as PSPs or banks, prefer asynchronous communication as well. So how can we ensure correctness in this case?
            The answer is reconciliation. This is a practice that periodically compares the states among related services in order to verify that they are in agreement. It is usually the last line of defense in the payment system.
            Every night the PSP or banks send a settlement file to their clients. The settlement file contains the balance of the bank account, together with all the transactions that took place on this bank account during the day. The reconciliation system parses the settlement file and compares the details with the ledger system.

          Handling payment processing delays
            The payment service must be able to handle these payment requests that take a long time to process. If the buy page is hosted by an external PSP, which is quite common these days, the PSP would handle these long-running payment requests in the following ways:

              The PSP would return a pending status to our client. Our client would display that to the user. Our client would also provide a page for the customer to check the current payment status.

              The PSP tracks the pending payment on our behalf, and notifies the payment service of any status update via the webhook the payment service registered with the PSP.
              Alternatively, instead of updating the payment service via a webhook, some PSP would put the burden on the payment service to poll the PSP for status updates on any pending payment requests.
          
          Communication among internal services
              Synchronous communication

                Synchronous communication like HTTP works well for small-scale systems, but its shortcomings become obvious as the scale increases. It creates a long request and response cycle that depends on many services. The drawbacks of this approach are:
                Low performance. If any one of the services in the chain doesn’t perform well, the whole system is impacted.
                Poor failure isolation. If PSPs or any other services fail, the client will no longer receive a response.
                Tight coupling. The request sender needs to know the recipient.
                Hard to scale. Without using a queue to act as a buffer, it’s not easy to scale the system to support a sudden increase in traffic.
          
              Asynchronous 
                Single receiver (point to point model in distributed message queue)
                Multiple receivers (pub sub model) .. eg. kafka
                  This model maps well to the payment system, as the same request might trigger multiple side effects such as sending push notifications, updating financial reporting, analytics,

          Handling failed payments
              Tracking payment state
              Retry queue and dead letter queue

          Exactly-once 
              We will explain how to implement at-least-once using retry, and at-most-once using idempotency check.

              Retry
                Immediate retry OR Fixed intervals OR increment intervals OR exponential backoff OR Cancel
                Determining the appropriate retry strategy is difficult. There is no “one size fits all” solution. As a general guideline, use exponential backoff if the network issue is unlikely to be resolved in a short amount of time.

              Scenario 1: The payment system integrates with PSP using a hosted payment page, and the client clicks the pay button twice.
              Scenario 2: The payment is successfully processed by the PSP, but the response fails to reach our payment system due to network errors. The user clicks the “pay” button again or the client retries the payment.

              Idempotency
                From an API standpoint, idempotency means clients can make the same call repeatedly and produce the same result.
                For communication between clients (web and mobile applications) and servers, an idempotency key is usually a unique value that is generated by the client and expires after a certain period of time. A UUID is commonly used as an idempotency key and it is recommended by many tech companies such as Stripe [19] and PayPal [20]. To perform an idempotent payment request, an idempotency key is added to the HTTP header: <idempotency-key: key_value>.

                Scenario 1: what if a customer clicks the “pay” button quickly twice?

                  To support idempotency, we can use the database's unique key constraint. For example, the primary key of the database table is served as the idempotency key. Here is how it works:

                      When the payment system receives a payment, it tries to insert a row into the database table.

                      A successful insertion means we have not seen this payment request before.

                      If the insertion fails because the same primary key already exists, it means we have seen this payment request before. The second request will not be processed.

                Scenario 2: The payment is successfully processed by the PSP, but the response fails to reach our payment system due to network errors. Then the user clicks the “pay” again.
                    When the user clicks the “pay” button again, the payment order is the same, so the token sent to the PSP is the same. Because the token is used as the idempotency key on the PSP side, it is able to identify the double payment and return the status of the previous execution.

          Consistency
              To maintain data consistency between internal services, ensuring exactly-once processing is very important.

              To maintain data consistency between the internal service and external service (PSP), we usually rely on idempotency and reconciliation. If the external service supports idempotency, we should use the same idempotency key for payment retry operations. Even if an external service supports idempotent API, reconciliation is still needed because we shouldn’t assume the external system is always right.

          Payment security
              use HTTPS with SSL (public key encryption), API gateway for rate limiting to avoid DDOS, use tokens to avoid Card theft 


      TODO : 
        Note about synchronous and asynchronous communication, importance of reconciliation
        Ensure all replicas are always in-sync. We could use consensus algorithms such as Paxos [21] and Raft [22], or use consensus-based distributed databases such as YugabyteDB [23] or CockroachDB [24].

  Distributed Email Service
      https://bytebytego.com/courses/system-design-interview/distributed-email-service

        Back-of-the-envelope estimation
            
            Assume metadata is stored in a database. Storage requirement for maintaining metadata in 1 year: 1 billion users * 40 (avg) emails / day * 365 days * 50 KB = 730 PB.
            Assume 20% of emails contain an attachment and the average attachment size is 500 KB.
            Storage for attachments in 1 year is: 1 billion users * 40 emails / day * 365 days * 20% * 500 KB = 1,460 PB

            From this back-of-the-envelope calculation, it’s clear we would deal with a lot of data. So, it’s likely that we need a distributed database solution.


        Step 2 - Propose High-Level Design and Get Buy-In
            Email knowledge 101
              Email protocols
              Domain name service (DNS)
              Attachment
            Traditional mail servers
              Traditional mail server architecture
                  Alice logs in to her Outlook client, composes an email, and presses the “send” button. The email is sent to the Outlook mail server. The communication protocol between the Outlook client and the mail server is SMTP.

                  Outlook mail server queries the DNS (not shown in the diagram) to find the address of the recipient’s SMTP server. In this case, it is Gmail’s SMTP server. Next, it transfers the email to the Gmail mail server. The communication protocol between the mail servers is SMTP.

                  The Gmail server stores the email and makes it available to Bob, the recipient.

                  Gmail client fetches new emails through the IMAP/POP server when Bob logs in to Gmail.

              Storage
                  In a traditional mail server, emails were stored in local file directories and each email was stored in a separate file with a unique name.
                  As the email volume grew and the file structure became more complex, disk I/O became a bottleneck. The local directories also don’t satisfy our high availability and reliability requirements.


            Distributed mail servers
                Email APIs
                  Due to the length limitations of this book, we cover only some of the most important APIs for webmail. A common way for webmail to communicate is through the HTTP protocol.

                  1. Endpoint: POST /v1/messages
                  2. Endpoint: GET /v1/folders
                  3. Endpoint: GET /v1/folders/{:folder_id}/messages
                  4. Endpoint: GET /v1/messages/{:message_id}

                Distributed mail server architecture
                  While it is easy to set up an email server that handles a small number of users, it is difficult to scale beyond one server. This is mainly because traditional email servers were designed to work with a single server only.


        TODO : 
            learn about Email protocols : SMTP for sending, POP and IMAP for retreiving


  Key value store
      https://bytebytego.com/courses/system-design-interview/design-a-key-value-store

      consistency 
        R + W > N 
          Dynamo and Cassandra adopt eventual consistency, which is our recommended consistency model for our key-value store.
      
      inconsistency resolved with versioning

      Handling failures 
        detection
          gossip protocol
        temporary failure
          sloppy quorum
        permanent failure
          merkle tree

      Write path
        Please note the proposed designs for write/read paths are primary based on the architecture of Cassandra 
        1. The write request is persisted on a commit log file.
        2. Data is saved in the memory cache.
        3. When the memory cache is full or reaches a predefined threshold, data is flushed to SSTable [9] on disk. Note: A sorted-string table (SSTable) is a sorted list of <key, value> pairs. For readers interested in learning more about SStable, refer to the reference material [9].
      Read path 
        After a read request is directed to a specific node, it first checks if data is in the memory cache. If so, the data is returned to the client 
        If the data is not in memory, it will be retrieved from the disk instead. We need an efficient way to find out which SSTable contains the key. Bloom filter [10] is commonly used to solve this problem.

    TODO
      SSTable - Cassandra, RocksDB


  S3-like Object Storage
      https://bytebytego.com/courses/system-design-interview/s3-like-object-storage

      Storage System 101
          Block storage
            Block storage came first, in the 1960s. Common storage devices like hard disk drives (HDD) and solid-state drives (SSD) that are physically attached to servers are all considered as block storage.
            Some applications like a database or a virtual machine engine manage these blocks directly in order to squeeze every drop of performance out of them.

          File storage
            File storage is built on top of block storage. It provides a higher-level abstraction to make it easier to handle files and directories. Data is stored as files under a hierarchical directory structure. 

          Object storage
            Object storage is new. It makes a very deliberate tradeoff to sacrifice performance for high durability, vast scale, and low cost. It targets relatively “cold” data and is mainly used for archival and backup. Object storage stores all data as objects in a flat structure. There is no hierarchical directory structure. Data access is normally provided via a RESTful API. It is relatively slow compared to other storage types. Most public cloud service providers have an object storage offering, such as AWS S3, Google object storage, and Azure blob storage.

        Functional requirements
          Bucket creation.
          Object uploading and downloading.
          Object versioning.
          Listing objects in a bucket.
        
        Non-functional requirements
          100 PB of data
          Data durability is 6 nines
          Service availability is 4 nines
          Storage efficiency. Reduce storage costs while maintaining a high degree of reliability and performance.

        Back of envelope
            IOPS. Let’s assume one hard disk (SATA interface, 7200 rpm) is capable of doing 100~150 random seeks per second (100-150 IOPS).

            With those assumptions, we can estimate the total number of objects the system can persist. To simplify the calculation, let’s use the median size for each object type (0.5MB for small objects, 32MB for medium objects, and 200MB for large objects). A 40% storage usage ratio gives us:

            100 PB = 100*1000*1000*1000 MB = 10^11 MB

            10^11*0.4/( 0.2*0.5MB + 0.6 *32MB + 0.2*200MB) = 0.68 billion objects.

            If we assume the metadata of an object is about 1KB in size, we need 0.68 TB space to store all metadata information. 


      Step 2 - Propose High-Level Design and Get Buy-In
          few interesting properties of object storage
            Object immutability
            Key-value store.
            Write once, read many times.
            Support both small and large objects. 
              Object size may vary and we need to support both.
              The design philosophy of object storage is very similar to that of the UNIX file system. In UNIX, when we save a file in the local file system, it does not save the filename and file data together. Instead, the filename is stored in a data structure called “inode” [10], and the file data is stored in different disk locations. The inode contains a list of file block pointers that point to the disk locations of the file data. 
              The object storage works similarly. In object storage, the metadata store uses the ID of the object to find the corresponding object data in the data store, via a network request. 
              Separating metadata and object data simplifies the design. The data store contains immutable data while the metadata store contains mutable data. This separation enables us to implement and optimize these two components independently.

        High-level design
          Load balancer, 
          API service
          Identity and access management (IAM).
          Data store
          Metadata store

      Step 3 - Design Deep Dive

        Data store

            Data routing service
              This service has the following responsibilities: 
                Query the placement service to get the best data node to store data.
                Read data from data nodes and return it to the API service.
                Write data to data nodes.

            Placement service
              The placement service determines which data nodes (primary and replicas) should be chosen to store an object.
              The placement service continuously monitors all data nodes through heartbeats.

            Data node
              The data node stores the actual object data. It ensures reliability and durability by replicating data to multiple data nodes, also called a replication group.
              Each data node has a data service daemon running on it. The data service daemon continuously sends heartbeats to the placement service. The heartbeat message includes the following essential information:
                  How many disk drives (HDD or SSD) does the data node manage?
                  How much data is stored on each drive?

        Data persistence flow
            given a UUID for the object as an input, the placement service returns the replication group for the object. How does the placement service do this? Keep in mind that this lookup needs to be deterministic, and it must survive the addition or removal of replication groups. Consistent hashing is a common implementation of such a lookup function. Refer to [15] for more information.

        How data is organized
          A simple solution is to store each object in a stand-alone file. This works, but the performance suffers when there are many small files. Two issues arise when having too many small files on a file system. 
            Fragmentation : First, it wastes many data blocks. A file system stores files in discrete disk blocks. Disk blocks have the same size, and the size is fixed when the volume is initialized. The typical block size is around 4 KB.

            I-Node limits in OS : Second, it could exceed the system’s inode capacity. The file system stores the location and other information about a file in a special type of block called inode. For most file systems, the number of inodes is fixed when the disk is initialized. With millions of small files, it runs the risk of consuming all inodes. 

            To address these issues, we can merge many small objects into a larger file. It works conceptually like a write-ahead log (WAL). When we save an object, it is appended to an existing read-write file. When the read-write file reaches its capacity threshold – usually set to a few GBs – the read-write file is marked as read-only, and a new read-write file is created to receive new objects. Once a file is marked as read-only, it can only serve read requests. Figure 12 explains how this process works.

            Note that write access to the read-write file must be serialized. As shown in Figure 12, objects are stored in order, one after the other, in the read-write file. To maintain this on-disk layout, multiple cores processing incoming write requests in parallel must take their turns to write to the read-write file. For a modern server with a large number of cores processing many incoming requests in parallel, this seriously restricts write throughput. To fix this, we could provide dedicated read-write files, one for each core processing incoming requests.

        Object lookup
           With each data file holding many small objects, how does the data node locate an object by UUID? The data node needs the following information:
              The data file that contains the object
              The starting offset of the object in the data file
              The size of the object
            We considered two options for storing this mapping: a file-based key-value store such as RocksDB [16] or a relational database. RocksDB is based on SSTable [17], and it is fast for writes but slower for reads. A relational database usually uses a B+ tree [18] based storage engine, and it is fast for reads but slower for writes. As mentioned earlier, the data access pattern is write once and read multiple times. Since a relational database provides better read performance, it is a better choice than RocksDB.

            How should we deploy this relational database? At our scale, the data volume for the mapping table is massive. Deploying a single large cluster to support all data nodes could work, but is difficult to manage. Note that this mapping data is isolated within each data node. There is no need to share this across data nodes. To take advantage of this property, we could simply deploy a simple relational database on each data node. SQLite [19] is a good choice here. It is a file-based relational database with a solid reputation.

          Durability

            Hardware failure and failure domain
              Let’s assume the spinning hard drive has an annual failure rate of 0.81% [20]. This number highly depends on the model and make. Making 3 copies of data gives us 1-(0.0081)^3=~0.999999 reliability.
              For a complete durability evaluation, we also need to consider the impacts of different failure domains. A failure domain is a physical or logical section of the environment that is negatively affected when a critical service experiences problems.
              server (node) -> rack -> datacenter which means replication have to be for every failure domain

            Erasure coding (parity)
              Making three full copies of data gives us roughly 6 nines of data durability. Are there other options to further increase durability? Yes, erasure coding is one option.
              Erasure coding [22] deals with data durability differently. It chunks data into smaller pieces (placed on different servers) and creates parities for redundancy. In the event of failures, we can use chunk data and parities to reconstruct the data. 
              Its cheaper, more durable, but needs more compute resources and read + write performance

            How much extra space does erasure coding need? For every two chunks of data, we need one parity block, so the storage overhead is 50% (Figure 17). While in 3-copy replication, the storage overhead is 200%

            Correctness verification
              If a disk fails completely and the failure can be detected, it can be treated as a data node failure. In this case, we can reconstruct data using erasure coding. However, in-memory data corruption is a regular occurrence in large-scale systems. This problem can be addressed by verifying checksums.
              There are many checksum algorithms, such as MD5 [26], SHA1[27], HMAC [28], etc. A good checksum algorithm usually outputs a significantly different value even for a small change made to the input. For this chapter, we choose a simple checksum algorithm such as MD5.
              In our design, we append the checksum at the end of each object. Before a file is marked as read-only, we add a checksum of the entire file at the end.

            Metadata data model
              Schema
                The database schema needs to support the following 3 queries:
                Query 1: Find the object ID by object name.
                Query 2: Insert and delete an object based on the object name.
                Query 3: List objects in a bucket sharing the same prefix.

              Scale the bucket table (bandiwdth factor reasoned out to get replicas)
                Since there is usually a limit on the number of buckets a user can create, the size of the bucket table is small. Let’s assume we have 1 million customers, each customer owns 10 buckets and each record takes 1 KB. 
                That means we need 10 GB (1 million * 10 * 1KB) of storage space. The whole table can easily fit in a modern database server. However, a single database server might not have enough CPU or network bandwidth to handle all read requests. If so, we can spread the read load among multiple database replicas.
                
              Scale the object table
                The object table holds the object metadata. The dataset at our design scale will likely not fit in a single database instance. We can scale the object table by sharding.
                We choose to shard by a combination of bucket_name and object_name. This is because most of the metadata operations are based on the object URI, for example, finding the object ID by URI or uploading an object via URI. To evenly distribute the data, we can use the hash of the (bucket_name, object_name) as the sharding key. Remember you can have multiple versions of same object name

              Prefer denormalized table of bucket and object so that it can be sharded just by bucket id. Use Cassandra for high available, heavy reads, heavy writes and eventual consistency.

            Object versioning

                To support versioning, the object table for the metadata store has a column called object_version that is only used if versioning is enabled. Instead of overwriting the existing record, a new record is inserted with the same bucket_id and object_name as the old record, but with a new object_id and object_version. The object_id is the UUID for the new object returned in step 3. The object_version is a TIMEUUID [29] generated when the new row is inserted. No matter which database we choose for the metadata store, it should be efficient to look up the current version of an object. The current version has the largest TIMEUUID of all the entries with the same object_name. See Figure 23 for an illustration of how we store versioned metadata.

                * When we delete an object, all versions remain in the bucket and we insert a delete marker. This avoids going back to the file and delete.

            Optimizing uploads of large files
                In the back-of-the-envelope estimation, we estimated that 20% of the objects are large. Some might be larger than a few GBs. It is possible to upload such a large object file directly, but it could take a long time. If the network connection fails in the middle of the upload, we have to start over. A better solution is to slice a large object into smaller parts and upload them independently. After all the parts are uploaded, the object store re-assembles the object from the parts. This process is called multipart upload.

            Garbage collection
                  - deleted marked objects
                  - half uploaded data
                  - corrupted data
                The garbage collector does not remove objects from the data store, right away. Deleted objects will be periodically cleaned up with a compaction mechanism.

                The garbage collector is also responsible for reclaiming unused space in replicas. For replication, we delete the object from both primary and backup nodes. For erasure coding, if we use (8+4) setup, we delete the object from all 12 nodes.

      TODO
        Study about iSCSI, Fibre channel protocol
        Study about SMB and NFS protocol
        Study about Paxos [13] or Raft [14] consensus protocol.
        Study about write-ahead log (WAL) which is also used in queue data structure

  Real-time Gaming Leaderboard

      Back-of-the-envelope estimation
        With 5 million DAU, if the game had an even distribution of players during a 24-hour period, we would have an average of 50 users per second (5,000,000 DAU / 10^5 seconds = ~50).
        QPS for users scoring a point: if a user plays 10 games per day on average, the QPS for users scoring a point is: 50 * 10 = ~500. Peak QPS is 5x of the average: 500 * 5 = 2,500.

      Step 2 - Propose High-Level Design and Get Buy-In
      
        API design
            POST /v1/scores
            GET /v1/scores
            GET /v1/scores/{:user_id}

        High-level architecture

          When a player wins a game, the client sends a request to the game service.
          The game service ensures the win is valid and calls the leaderboard service to update the score.
          The leaderboard service updates the user’s score in the leaderboard store.
          A player makes a call to the leaderboard service directly to fetch leaderboard data, including:
            a. top 10 leaderboard.
            b. the rank of the player on the leaderboard.

          Should the client talk to the leaderboard service directly?
            In the alternative design, the score is set by the client. This option is not secure because it is subject to man-in-the-middle attack [x], where players can put in a proxy and change scores at will. Therefore, we need the score to be set on the server-side.

          Do we need a message queue between the game service and the leaderboard service?
            The answer to this question highly depends on how the game scores are used. If data can be consumed by multiple consumers, such as leaderboard service, analytics service, push notification service, etc. This is especially true when the game is a turn-based or multi-player game in which we need to notify other players about the score update.
            As this is not an explicit requirement based on the conversation with the interviewer, we do not use a message queue in our design.

          Data models
            One of the key components in the system is the leaderboard store. We will discuss three potential solutions: relational database, Redis, and NoSQL (NoSQL solution is explained in deep dive).

            Relational database solution
              This solution works when the data set is small, but the query becomes very slow when there are millions of rows. Let’s take a look at why.

              To figure out the rank of a user, we need to sort every single player into their correct spot on the leaderboard so we can determine exactly what the correct rank is. Remember that there can be duplicate scores as well, so the rank isn’t just the position of the user in the list.

              SQL databases are not performant when we have to process large amounts of continuously changing information. So even with indexes, your write performance could get compromised

            Redis solution
              Redis has a specific data type called sorted sets that are ideal for solving leaderboard system design problems.

              What are sorted sets?
                A sorted set is a data type similar to a set. Each member of a sorted set is associated with a score. The members of a set must be unique, but scores may repeat. The score is used to rank the sorted set in ascending order.

                Our leaderboard use case maps perfectly to sorted sets. Internally, a sorted set is implemented by two data structures: a hash table and a skip list [1]. The hash table maps users to scores and the skip list maps scores to users. In sorted sets, users are sorted by scores. 
                A skip list is a list structure that allows for fast search. It consists of a base sorted linked list and multi-level indexes.

                Another related factor to consider is CPU and I/O usage. Our peak QPS from the back-of-the-envelope estimation is 2500 updates/sec. This is well within the performance envelope of a single Redis server.

                One concern about the Redis cache is persistence, as a Redis node might fail. Luckily, Redis does support persistence, but restarting a large Redis instance from disk is slow. Usually, Redis is configured with a read replica, and when the main instance fails, the read replica is promoted, and a new read replica is attached.

          Step 3 - Design Deep Dive
            To use a cloud provider or not
                Manage our own services
                Build on the cloud
                  AWS Lambda is one of the most popular serverless computing platforms. It allows us to run code without having to provision or manage the servers ourselves. It runs only when needed and will scale automatically based on traffic. Serverless is one of the hottest topics in cloud services and is supported by all major cloud service providers. 
                  Advantage with AWS Lambda : autoscaling as per DAU growth and based on pay as you go model

            Scaling Redis
                Let’s imagine we have 500 million DAU, which is 100 times our original scale. Now our worst-case scenario for the size of the leaderboard goes up to 65 GB (650 MB *100), and our QPS goes up to 250,000 (2,500 * 100) queries per second. This calls for a sharding solution.

                Data sharding
                    We consider sharding in one of the following two ways: fixed or hash partitions.

                    Fixed partition

                    Hash partition
                      Redis cluster provides a way to shard data automatically across multiple Redis nodes. It doesn’t use consistent hashing but a different form of sharding, where every key is part of a hash slot. There are 16384 hash slots [11] and we can compute the hash slot of a given key by doing CRC16(key) % 16384

                      Retrieving the top 10 players on the leaderboard is more complicated. We need to gather the top 10 players from each shard and have the application sort the data. 

                      This approach has a few limitations:
                        When we need to return top K results (where K is a very large number) on the leaderboard, the latency is high because a lot of entries are returned from each shard and need to be sorted.
                        Latency is high if we have lots of partitions because the query has to wait for the slowest partition.
                        Another issue with this approach is that it doesn’t provide a straightforward solution for determining the rank of a specific user.

                    Therefore, we lean towards the first proposal: fixed partition.

            Sizing a Redis node
                Redis provides a tool called Redis-benchmark that allows us to benchmark the performance of the Redis setup, by simulating multiple clients executing multiple queries and returning the number of requests per second for the given hardware.

            Alternative solution: NoSQL
                An alternative solution to consider is NoSQL databases. 
                NoSQL databases such as Amazon’s DynamoDB [15], Cassandra, or MongoDB can be a good fit.

                DynamoDB is a fully managed NoSQL database that offers reliable performance and great scalability. To allow efficient access to data with attributes other than the primary key + sort key, we can leverage global secondary indexes [16] in DynamoDB.
                To avoid a linear scan, we need to add indexes. Our first attempt is to use “game_name#{year-month}” as the partition key and the score as the sort key. 
                This works, but it runs into issues at a high load. DynamoDB splits data across multiple nodes using consistent hashing. Each item lives in a corresponding node based on its partition key. We want to structure the data so that data is evenly distributed across partitions. In our table design (Figure 23), all the data for the most recent month would be stored in one partition and that partition becomes a hot partition. How can we solve this problem?

                    GSI : We can further split data into N partitions and append a partition number (user_id % number_of_partitions) to the partition key. 
                    What we end up with are N partitions that are all sorted within their own partition (locally sorted). If we assume we had 3 partitions, then in order to fetch the top 10 leaderboard, we would use the approach called “scatter-gather” mentioned earlier. We would fetch the top 10 results in each of the partitions (this is the “scatter” portion), and then we would allow the application to sort the results among all the partitions (this is the “gather” portion). 
                    --> Map- Reduce concept

                However, similar to the Redis partition solution mentioned earlier, this approach doesn’t provide a straightforward solution for determining the relative rank of a user. But it is possible to get the percentile of a user’s position, which could be good enough. In real life, telling a player that they are in the top 10-20% might be better than showing the exact rank at eg.1,200,001. Therefore, if the scale is large enough that we needed to shard, we could assume that the score distributions are roughly the same across all shards. If this assumption is true, we could have a cron job that analyzes the distribution of the score for each shard, and caches that result.

            Faster retrieval and breaking tie
                To store a map of the user id to the user object that we can display on the leaderboard. This allows for faster retrieval than having to go to the database to fetch the user object.

                In the case of two players having the same scores, we could rank the users based on who received that score first.

            System failure recovery
              The Redis cluster can potentially experience a large-scale failure. Given the design above, we could create a script that leverages the fact that the MySQL database records an entry with a timestamp each time a user won a game. We could iterate through all of the entries for each user, and call ZINCRBY once per entry, per user.


      TODO : 
       great learnings on custom partition vs uses existing sharding provided by Redis.
       Check if Redis can use Map-Reduce (which looks like it)
       Check scatter-gather for dynamo



  Stock Exchange
      https://bytebytego.com/courses/system-design-interview/stock-exchange

    Back-of-the-envelope estimation
      Let’s do some simple back-of-the-envelope calculations to understand the scale of the system:

      100 symbols
      1 billion orders per day
      NYSE Stock Exchange is open Monday through Friday from 9:30 am to 4:00 pm Eastern Time. That’s 6.5 hours in total.
      QPS: 1 billion / 6.5 / 3600 = ~43,000 or just 10K if rounded
      Peak QPS: 5 * QPS = 215,000. The trading volume is significantly higher when the market first opens in the morning and before it closes in the afternoon.

    Step 2 - Propose High-Level Design and Get Buy-In
      Business Knowledge 
        Broker
        Institutional client
        Limit order
        Market order
        Market data levels
        Candlestick chart
        FIX
          FIX protocol [8], which stands for Financial Information eXchange protocol, was created in 1991. It is a vendor-neutral communications protocol for exchanging securities transaction information. See below for an example of a securities transaction encoded in FIX.
      High-level design
        Note that the trading flow (steps 1 to 14) is on the critical path, while the market data flow and reporting flow are not. They have different latency requirements.
        * its Important to know what components in the design are latency 
        
        Trading flow
            Matching engine
              Maintain the order book for each symbol. An order book is a list of buy and sell orders for a symbol. We explain the construction of an order book in the Data models section later.
              Match buy and sell orders. A match results in two executions (fills), with one each for the buy and sell sides. The matching function must be fast and accurate.
              Distribute the execution stream as market data.
            Sequencer
              The sequencer is the key component that makes the matching engine deterministic. It stamps every incoming order with a sequence ID before it is processed by the matching engine. It also stamps every pair of executions (fills) completed by the matching engine with sequence IDs. 
              It is similar to having two Kafka event streams connected to the matching engine, one for incoming orders and the other for outgoing executions. In fact, we could have used Kafka if its latency was lower and more predictable. 
            Order manager
              It sends the order for risk checks. 
              It checks the order against the user’s wallet and verifies that there are sufficient funds to cover the trade.
              It sends the order to the sequencer where the order is stamped with a sequence ID. 
              The order manager should be fast, efficient, and accurate.
            Client gateway
              The client gateway is on the critical path and is latency-sensitive. It should stay lightweight. 
              May be Nginx
            Market data flow
              The market data publisher (MDP) receives executions (fills) from the matching engine and builds the order books and candlestick charts from the stream of executions.
            Reporting flow
              One essential part of the exchange is reporting. The reporter is not on the trading critical path, but it is a critical part of the system. It provides trading history, tax reporting, compliance reporting, settlements, etc. Efficiency and latency are critical for the trading flow, but the reporter is less sensitive to latency. Accuracy and compliance are key factors for the reporter.
            API Design
              POST /v1/order
              GET /execution?symbol={:symbol}&orderId={:orderId}&startTime={:startTime}&endTime={:endTime}
              GET /marketdata/orderBook/L2?symbol={:symbol}&depth={:depth}
              GET /marketdata/candles?symbol={:symbol}&resolution={:resolution}&startTime={:startTime}&endTime={:endTime}
            Data 
              Product, order, execution
                In the critical trading path, orders and executions are not stored in a database. To achieve high performance, this path executes trades in memory and leverages hard disk or shared memory to persist and share orders and executions. Specifically, orders and executions are stored in the sequencer for fast recovery, and data is archived after the market closes. We discuss an efficient implementation of the sequencer in the deep dive section.
              Order book
                An order book is a list of buy and sell orders for a specific security or financial instrument, organized by price level [12] [13]. It is a key data structure in the matching engine for fast order matching. An efficient data structure for an order book must satisfy these requirements:

                  Constant lookup time. Operation includes: getting volume at a price level or between price levels.

                  Fast add/cancel/execute operations, preferably O(1) time complexity. Operations include: placing a new order, canceling an order, and matching an order.
            Step 3 - Design Deep Dive

              Performance
                Latency = ∑executionTimeAlongCriticalPath
                  gateway -> order manager -> sequencer -> matching engine

                  To stay ahead of the competition, exchanges over time evolve their design to reduce the end-to-end latency on the critical path to tens of microseconds, primarily by exploring options to reduce or eliminate network and disk access latency. A time-tested design eliminates the network hops by putting everything on the same server. When all components are on the same server, they can communicate via mmap [17] as an event store (more on this later)

                  Next, let’s focus our attention on the long rectangle labeled “mmap” at the center of Figure 15. “mmap” refers to a POSIX-compliant UNIX system call named `mmap(2)` that maps a file into the memory of a process.

                  `mmap(2)` provides a mechanism for high-performance sharing of memory between processes. The performance advantage is compounded when the backing file is in `/dev/shm`. `/dev/shm` is a memory-backed file system. When `mmap(2)` is done over a file in `/dev/shm`, the access to the shared memory does not result in any disk access at all.

                  Modern exchanges take advantage of this to eliminate as much disk access from the critical path as possible. `mmap(2)` is used in the server to implement a message bus over which the components on the critical path communicate. The communication pathway has no network or disk access, and sending a message on this mmap message bus takes sub-microsecond. By leveraging mmap to build an event store, coupled with the event sourcing design paradigm which we will discuss next, modern exchanges can build low-latency microservices inside a server.`

              Event sourcing
                  Figure 18 shows an event sourcing design using the mmap event store as a message bus. This looks very much like the Pub-Sub model in Kafka. In fact, if there is no strict latency requirement, Kafka could be used.

                This design follows the high-level design closely, but there are some adjustments to make it work more efficiently in the event sourcing paradigm.

                  The first difference is the order manager. The order manager becomes a reusable library that is embedded in different components.
                  Another key difference is that the sequencer is nowhere to be seen. What happened to it?

                  With the event sourcing design, we have one single event store for all messages. Note that the event store entry contains a “sequence” field. This field is injected by the sequencer.    

                  Unlike the sequencer in the high-level design which also functions as a message store, the sequencer here only does one simple thing and is super fast. Figure 19 shows a design for the sequencer in a memory-map (MMap) environment.

                  The sequencer pulls events from the ring buffer that is local to each component. For each event, it stamps a sequence ID on the event and sends it to the event store. We can have backup sequencers for high availability in case the primary sequencer goes down.

              High availability
                  For stateless services such as the client gateway, they could easily be horizontally scaled by adding more servers.
                  For stateful components, such as the order manager and matching engine, we need to be able to copy state data across replicas. 
                  We could use reliable UDP [19] to efficiently broadcast the event messages to all warm servers.  

              Fault tolerance
                  Once the decision to failover is correctly made, how do we decide which server takes over? Fortunately, this is a well-understood problem. There are many battle-tested leader-election algorithms. We use Raft [22] as an example.

              Matching algorithms
                  The pseudocode uses the FIFO (First In First Out) matching algorithm. The order that comes in first at a certain price level gets matched first, and the last one gets matched last.

              Determinism
                  With functional determinism, the actual time when the event happens does not matter most of the time. What matters is the order of the events. In Figure 23, event timestamps from discrete uneven dots in the time dimension are converted to continuous dots, and the time spent on replay/recovery can be greatly reduced.
                  
                  Latency determinism means having almost the same latency through the system for each trade. This is key to the business. There is a mathematical way to measure this: the 99th percentile latency, or even more strictly, the 99.99th percentile latency. We can leverage HdrHistogram [26] to calculate latency. If the 99th percentile latency is low, the exchange offers stable performance across almost all the trades.
              Market data publisher optimizations
                  This design utilizes ring buffers. A ring buffer, also called a circular buffer, is a fixed-size queue with the head connected to the tail. A producer continuously produces data and one or more consumers pull data off it. The space in a ring buffer is pre-allocated. There is no object creation or deallocation necessary. The data structure is also lock-free.
              Distribution fairness of market data
                  Multicast using reliable UDP is a good solution to broadcast updates to many participants at once. The MDP could also assign a random order when the subscriber connects to it. We look at multicast in more detail.
              Network security




  Digital Wallet
      https://bytebytego.com/courses/system-design-interview/digital-wallet

      Step 2 - Propose High-Level Design and Get Buy-In

        API Design

          API	Detail
          POST /v1/wallet/balance_transfer

        In-memory sharding solution
          For in-memory stores, one popular choice is Redis. One Redis node is not enough to handle 1 million TPS. We need to set up a cluster of Redis nodes and evenly distribute user accounts among them. This process is called partitioning or sharding.

          The number of partitions and addresses of all Redis nodes can be stored in a centralized place. We could use Zookeeper [4] as a highly-available configuration storage solution.

          The final component of this solution is a service that handles the transfer commands. We call it the wallet service and it has several key responsibilities.
              Receives the transfer command
              Validates the transfer command
              If the command is valid, it updates the account balances for the two users involved in the transfer. In a cluster, the account balances are likely to be in different Redis nodes

          In this example, we have 3 Redis nodes. There are three clients, A, B, and C. Their account balances are evenly spread across these three Redis nodes. There are two wallet service nodes in this example that handle the balance transfer requests. 
          This design works, but it does not meet our correctness requirement. The wallet service updates two Redis nodes for each transfer. There is no guarantee that both updates would succeed.

        Distributed transactions

          Database sharding

            The first step is to replace each Redis node with a transactional relational database node. 

            Using transactional databases only solves part of the problem. As mentioned in the last section, it is very likely that one transfer command will need to update two accounts in two different databases. There is no guarantee that two update operations will be handled at exactly the same time. If the wallet service restarted right after it updated the first account balance, how can we make sure the second account will be updated as well?

          Distributed transaction: two-phase commit

            There are two ways to implement a distributed transaction: a low-level solution and a high-level solution. We will examine each of them.

            The low-level solution relies on the database itself. The most commonly used algorithm is called two-phase commit (2PC).
              - The coordinator, which in our case is the wallet service, performs read and write operations on multiple databases as normal. As shown in Figure 5, both databases A and C are locked.
              - When the application is about to commit the transaction, the coordinator asks all databases to prepare the transaction.

              - In the second phase, the coordinator collects replies from all databases and performs the following:

                    If all databases reply with a “yes”, the coordinator asks all databases to commit the transaction they have received.

                    If any database replies with a “no”, the coordinator asks all databases to abort the transaction.

              The biggest problem with 2PC is that it’s not performant, as locks can be held for a very long time while waiting for a message from the other nodes. Another issue with 2PC is that the coordinator can be a single point of failure, as shown in Figure 6.

          Distributed transaction: Try-Confirm/Cancel (TC/C)

            TC/C is a type of compensating transaction [7] that has two steps:
              In the first phase, the coordinator asks all databases to reserve resources for the transaction.
              In the second phase, the coordinator collects replies from all databases:
                If all databases reply with “yes”, the coordinator asks all databases to confirm the operation, which is the Try-Confirm process.
                If any database replies with “no”, the coordinator asks all databases to cancel the operation, which is the Try-Cancel process.
            It’s important to note that the two phases in 2PC are wrapped in the same transaction, but in TC/C each phase is a separate transaction.
                First phase: Try
                  For the database that contains account A, the coordinator starts a local transaction that reduces the balance of A by $1.
                  For the database that contains account C, the coordinator gives it a NOP (no operation.) To make the example adaptable for other scenarios, let’s assume the coordinator sends to this database a NOP command. The database does nothing for NOP commands and always replies to the coordinator with a success message.
                Second phase: Confirm
                  If both databases reply “yes”, the wallet service starts the next Confirm phase.
                Second phase: Cancel
                  What if the first Try phase fails? In the example above we have assumed the NOP operation on account C always succeeds, although in practice it may fail. For example, account C might be an illegal account, and the regulator has mandated that no money can flow into or out of this account. In this case, the distributed transaction must be canceled and we have to clean up.
                  Because the balance of account A has already been updated in the transaction in the Try phase, it is impossible for the wallet service to cancel a completed transaction. What it can do is to start another transaction that reverts the effect of the transaction in the Try phase, which is to add $1 back to account A.

                TC/C is also called a distributed transaction by compensation. It is a high-level solution because the compensation, also called the “undo,” is implemented in the business logic. The advantage of this approach is that it is database-agnostic. As long as a database supports transactions, TC/C will work. The disadvantage is that we have to manage the details and handle the complexity of the distributed transactions in the business logic at the application layer.

              Phase status table
                  We still have not yet answered the question asked earlier; what if the wallet service restarts in the middle of TC/C? When it restarts, all previous operation history might be lost, and the system may not know how to recover.
                  The solution is simple. We can store the progress of a TC/C as phase status in a transactional database.

              Unbalanced state
                  Have you noticed that by the end of the Try phase, $1 is missing (Figure 11)?
                  It violates a fundamental rule of accounting that the sum should remain the same after a transaction.
                  The good news is that the transactional guarantee is still maintained by TC/C. TC/C comprises several independent local transactions. Because TC/C is driven by application, the application itself is able to see the intermediate result between these local transactions. On the other hand, the database transaction or 2PC version of the distributed transaction was maintained by databases that are invisible to high-level applications.

              Out-of-order execution
                  Let’s assume that the database that handles account C has some network issues and it receives the Cancel instruction before the Try instruction. In this case, there is nothing to cancel.
                    The out-of-order Cancel operation leaves a flag in the database indicating that it has seen a Cancel operation, but it has not seen a Try operation yet.
                    The Try operation is enhanced so it always checks whether there is an out-of-order flag, and it returns a failure if there is.
                  This is why we added an out-of-order flag to the phase status table in the “Phase Status Table” section.

          Distributed transaction: Saga
              Linear order execution
                1. All operations are ordered in a sequence. Each operation is an independent transaction on its own database.

                2. Operations are executed from the first to the last. When one operation has finished, the next operation is triggered.

                3. When an operation has failed, the entire process starts to roll back from the current operation to the first operation in reverse order, using compensating transactions. So if a distributed transaction has n operations, we need to prepare 2n operations: n for the normal case and another n for the compensating transaction during rollback.

                How do we coordinate the operations? There are two ways to do it:

                  Choreography (De-centralized). In a microservice architecture, all the services involved in the Saga distributed transaction do their jobs by subscribing to other services’ events. So it is fully decentralized coordination.

                  Orchestration (Centralized). A single coordinator instructs all services to do their jobs in the correct order.

                  The choice of which coordination model to use is determined by the business needs and goals. The challenge of the choreography solution is that services communicate in a fully asynchronous way, so each service has to maintain an internal state machine in order to understand what to do when other services emit an event. It can become hard to manage when there are many services. The orchestration solution handles complexity well, so it is usually the preferred solution in a digital wallet system.

          Event sourcing
              Background
                In real life, a digital wallet provider may be audited. These external auditors might ask some challenging questions, for example:
                Do we know the account balance at any given time?
                How do we know the historical and current account balances are correct?
                How do we prove that the system logic is correct after a code change?

                One design philosophy that systematically answers those questions is event sourcing, which is a technique developed in Domain-Driven Design (DDD) [9].

          Wallet service example
              For the wallet service, the commands are balance transfer requests. These commands are put into a FIFO queue. One popular choice for the command queue is Kafka [10]
              Let us assume the state (the account balance) is stored in a relational database. The state machine examines each command one by one in FIFO order. For each command, it checks whether the account has a sufficient balance. If yes, the state machine generates an event for each account.
                Read commands from the command queue.
                Read balance state from the database.
                Validate the command. If it is valid, generate two events for each of the accounts.
                Read the next Event.
                Apply the Event by updating the balance in the database.

          Reproducibility
              The most important advantage that event sourcing has over other architectures is reproducibility.
              In the distributed transaction solutions mentioned earlier, a wallet service saves the updated account balance (the state) into the database. It is difficult to know why the account balance was changed. Meanwhile, historical balance information is lost during the update operation. In the event sourcing design, all changes are saved first as immutable history. The database is only used as an updated view of what balance looks like at any given point in time.

            Because of the audit capability, event sourcing is often chosen as the de facto solution for the wallet service.

          Command-query responsibility segregation (CQRS)
            So far, we have designed the wallet service to move money from one account to another efficiently. However, the client still does not know what the account balance is. 
            Rather than publishing the state (balance information), event sourcing publishes all the events. The external world could rebuild any customized state itself. This design philosophy is called CQRS [11].
            In CQRS, there is one state machine responsible for the write part of the state, but there can be many read-only state machines. For example, clients may want to know their balances and a read-only state machine could save state in a database to serve the balance query. Another state machine could build state for a specific time period to help investigate issues like possible double charges. The state information is an audit trail that could help to reconcile the financial records.

      Design Deep Dive
      
          File-based state
            In the previous design, state (balance information) is stored in a relational database. In a production environment, a database usually runs in a stand-alone server that can only be accessed through networks. Similar to the optimizations we did for command and event, state information can be saved to the local disk, as well.

            More specifically, we can use SQLite [14], which is a file-based local relational database or use RocksDB [15], which is a local file-based key-value store.

            RocksDB is chosen because it uses a log-structured merge-tree (LSM), which is optimized for write operations. To improve read performance, the most recent data is cached.


      TODO
        Study mmap

              



News Feed System
  https://bytebytego.com/courses/system-design-interview/design-a-news-feed-system

  TODOs
    Study about graphDBs

























              








    
================================================================================================================================

    
- Load balancing
    Resources
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/load-balancing.md
      https://www.cloudflare.com/learning/performance/types-of-load-balancing-algorithms/
      https://igotanoffer.com/blogs/tech/load-balancing-system-design-interview
    Static load balancing - round robin, weight round robin, IP hash
    Dynamic load balanacing - least connection, least resource, least response time, least bandwidth
    When to use Static ?
      If the server pool and the requests are both homogeneous, and there is only one balancer. Stateless servers handling single api can use static load balancer
      
    When to use Dynamic ?
      if the requests are heterogenous, Least Load is helpful to prevent servers from being intermittently overloaded.
      
    What are Hardware load balancers ?
      They have high performant hardware resources like L4 and L7. 
      Read about L4 and L7 here https://levelup.gitconnected.com/l4-vs-l7-load-balancing-d2012e271f56
      
    What are Software load balancers ?
      They run on standard servers and are less hardware optimized, but cheaper to set up and run. Example, Nginx or HAProxy. For Nginx 100s video, 
      refer https://www.youtube.com/watch?v=JKxlsvZXG7c. Nginx can serve routing to different servers, rate limiting, handle spikes,
      reverse proxy, security, cache, etc. 
      
    When to use Software or Hardware ?
      Tip : Since software load balancing can run on ordinary hardware and supports, always prefer software load balancer. Example Nginx
       
      
    What is proxy server, reverse proxy and API Gateway ? How is different from load balancer
      proxy vs reverse proxy vs load balancer : https://www.youtube.com/watch?v=MiqrArNSxSM
      Proxy : just protects the client side
      Reverse Proxy : protects the server by serving as proxy for all backend services - routing, rate limitng, load balancing
      API gateway : rate limiting, routing, handle spikes
      Load balancer : only performs load balancing
      
      So Reverse proxy can perform both API Gateway and Load Balancing. Example nginx.
      A load balancer can never be API gateway or reverse proxy.
      
     Always choose Nginx ? Why ?
       Has triats of load balancing + reverse proxy + API gateway
         - supports load balancing
         - supports reverse proxy 
         - supports rate limiting
         - supports caching
         - supports request grouping
         - supports secure auth
       Nginx supports L7
================================================================

      
  - Caching
    Resources
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/caching.md
      Distributed cache on Application server VS Global cache like Redis, (mostly preferred)
      CDN is another form of cache for static content but its often expensive
      
          
       What are different caching strategies ? 
          https://codeahoy.com/2017/08/11/caching-strategies-and-how-to-choose-the-right-one/
          Cache-Aside (application server has cache aside like Redis, Memcache and DB separately)
            - Pros : logic controlled by application server, resilent to cache failures, data model in cache vs db could be different
            - Cons : stale data in cache for db writes
          Read-Through Cache (application server reads from cache only)
            - Pros : logic controlled by cache, read-heavy, good consistency when combined with write through 
            - Cons : needs data model to be consistent, first time data always results in cache miss

          Belpw are Cache writes invalidation policy 

          Write through (data is written to cache and synced with storage)
            - Pros : fast retreival and data consistent systems
            - Cons : slow writes though
          Write around (data is written to storage, not cache)
            - Pros : Saves write operation on the cache
            - Cons : recently written data creates a cache miss and higher latency.
          Write back (data is written to cache only, then synced later to storage)
            - Pros : fast retreival, low latency read / writes
            - Cons : Risk of data loss
            
        Real applications of 
          1. read-through, write-through for high consistency
                DynamoDB Accelerator (DAX) for dynamoDB
          2. read-through, write-around for high performance for situations where data is written once and read less frequently or never.
                real-time logs, chatroom messages
          3. cache-aside, write back cache to absorb spikes during peak load
                custom applications using redis
          4. write back cache
                InnoDB which is relational database storage engine. Queries are first written to memory and eventually flushed to the disk.
                
                
================================================================
    - Sharding / Partitioning 
        Resources
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/sharding.md
          https://coursehunters.online/t/educative-io-design-gurus-grokking-the-system-design-interview-part-5/584
          Horizontal, Vertical and Directory based paritioning
            Directory based paritioning has work around to solve some challenges with Horizontal and Vertical?
          What's the best Partitioning criteria ?
            Consitent hashing which is combination of hash and list based partitioning
          Commom problems with Sharding / Partitioning
            Joins -> Denormalization
            Referential Integrity
            Hot shard problem needs rebalancing
              How to rebalance ? 
              It would take downtime, check directory based partitioning as per our educative.io resource.
              But consistent hashing can solve this problem better - https://medium.com/nerd-for-tech/consistent-hashing-6524e48ac648
              Great guide above - check consistent hashing with gossip protocol so that each partition knows where to fetch data from db
          Applications of consistent hashing ?
          Apache Cassandra, Dynamo DB
 
 ================================================================

    - Indexing
        Resources
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/indexes.md
          What is a database index ? 
            https://www.codecademy.com/article/sql-indexes
          Pros
            helps in speeding up the search 
          Cons
            lowers write/update/delete performance
          When to use indexes ?
            In the case of data sets that are many terabytes in size but with very small payloads (e.g., 1 KB), 
            indexes are a necessity for optimizing data access. Finding a small payload in such a large dataset can be a real challenge since
            we can’t possibly iterate over that much data in any reasonable time. Furthermore, it is very likely that such a large data set is 
            spread over several physical devices—this means we need some way to find the correct physical location of the desired data. 
            Indexes are the best way to do this.
            
   ================================================================

     - Proxy 
        Resources
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/proxies.md
          
          Usage : 
          filter requests, log requests, cache, encryption and most important batch request. Further, it can get smarter to o collapse requests 
          for data that is spatially close together in the storage (consecutively on disk). This strategy will result in decreasing request latency. 
          For example, let’s say a bunch of servers request parts of file: part1, part2, part3, etc. We can set up our proxy in such a way 
          that it can recognize the spatial locality of the individual requests, thus collapsing them into a single request and reading complete file, 
          which will greatly minimize the reads from the data origin.
          
          Proxy are of two types 
          Client proxy to protect the clients
          Server proxy to protect the servers. Also called reverse proxy
          
          Should we use proxy then?
          Only use proxy to protect the client.
          For server side, Always Nginx since its open source and supports both reverse proxy + load balancing + API gateway.
          
          
   
   ==============================================================================

          Queues
          
            Resource :
            - https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/queues.md
            
            When to use queue :
            Queues are implemented on the asynchronous communication protocol, meaning when a client submits a task to a queue they 
            are no longer required to wait for the results; instead, they need only acknowledgment that the request was properly received.
            
            Queues are also used for fault tolerance as they can provide some protection from service outages and failures. For example, 
            we can create a highly robust queue that can retry service requests that have failed due to transient system failures
            
            When not to use queue :
            When client expects respone in real-time
            
            Example of queues : 
             RabbitMQ and Kafka (which is open source)
             
  ====================================================================================
  
        Redundancy and Replication
        
          Resource : 
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/redundancy.md
          
          When to use Redundant system :
          Always. its a key concept of distributed system.
          Always prefer shared nothing architecture so that each node can operate independently and can scale.
          
          
    ====================================================================================
        SQL vs NoSQL
        
          Resource : 
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/sql-vs-nosql.md
          
          SQL 
          Relational databases store data in rows and columns. Example, MySQL, Oracle, MS SQL Server, SQLite, Postgres
          
          NoSQL 
          Key-value :  Example, DynamoDB, Redis, Memcache
          Document DB : Example, MongoDB, 
          Wide-Column Database : Example, HBase, Cassandra - https://hevodata.com/learn/columnar-databases/
          Graph Databases : Example, Neo4j
          
          Differences between both 
          Storage : SQL is tables but NoSQL has different storage models
          Schema : changing schema with SQL is possible but requires whole database modification
          Querying : using SQL for SQL dbs. 
          Scalability : SQL is vertically scalable and possible to scale it horizontally but has limitations. NoSQL is horizontally scalable
          Reliability or ACID : Definitely SQL ensures ACID but NoSQL solutions sacrifice ACID compliance for performance and scalability.
          
          When to use SQL
            ACID compliance for e-commerce and finanicial transactions
            Your data is structured and unchanging.
            Often Read heavy and low-write

            
          When to use NoSQL
            - Storing large volumes of data that often have little to no structure. 
            - Making the most of cloud computing and storage. Cloud-based storage is an excellent cost-saving solution 
            but requires data to be easily spread across multiple servers to scale up. 
            - Rapid development. NoSQL is extremely useful for rapid development as it doesn’t need to be prepped ahead of time. 
            
          What about write performance ? 
            I think SQL provides more flexibility on write performance than NoSQL. But this is contradicting
            However, Cassandra seems to be the choice

            
            
  ====================================================================================
  
    CAP Theorem
    Resource : 
    https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/cap-theorem.md
    
    CAP theorem states that it is impossible for a distributed software system to simultaneously provide more than two out of three of 
    the following guarantees (CAP): Consistency, Availability and Partition tolerance.
    
    We can only build a system that has any two of these three properties. Because, to be consistent, all nodes should see the same set of updates 
    in the same order. But if the network suffers a partition, updates in one partition might not make it to the other partitions before a client 
    reads from the out-of-date partition after having read from the up-to-date one. The only thing that can be done to cope with this possibility 
    is to stop serving requests from the out-of-date partition, but then the service is no longer 100% available.

    When to choose consistency over availability ? 
      Banking, Payment systems
    
    When to choose consistency over availability ? 
      Most distributed system use cases like Google Maps, Twitter, etc.
      
    Since reliability consists of both consistency and availability, ASK YOUR interviewer what it means for system to be 99.99% reliable
    
    
   ====================================================================================
 
    Consistent Hashing
    
    Resource 
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/consistent-hashing.md
      Recommended ones 
        https://medium.com/nerd-for-tech/consistent-hashing-6524e48ac648 - Explains well about rebalancing
        https://bytebytego.com/courses/system-design-interview/design-consistent-hashing
        
      How does it work ? 
        Hashing is done in the range where it maps the key to the integer in that range. Example 0 -> 255 are the integers placed in ring form 
        such that values are wrapped around.
        
        1. the servers are mapped to the integers in that range
        2. to map key to a server, simply hash the key to the integer in that range and move clockwise till you find the server (could be binary search)
        
        Now how this is better :
        1. adding a server : say S1 is added at position 20 and is near S2 at position 25. Then keys from before 20 wuld map to S1.
        2. removing a server : say S1 is removed at position 20 and was near S2 at position 25. Then  all keys even before 20 would map to S2. 
        3. uniform distribution : add “virtual replicas”. Instead of mapping each server to a single point on the ring, 
            we map it to multiple points on the ring, i.e. replicas. 
        4. easy to rebalance : when server is added or removed, only its next clockwise neighbouring server is affected and requires re-mapping of keys.
        
        
    ====================================================================================
    
    Client-Server Communication
    
      Resource : 
        https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/client-server-communication.md
        
      Types : 
      1. Standard HTTP Web Request : client opens the connection, gets response from the server and terminate the connection 
      2. Ajax Polling : client performs HTTP Web Request above periodically (say every 1 s). May create unecessary empty responses if server is not ready.
      3. Long Polling : client performs HTTP Web Request and waits for long time till response is received (say 1 min). Then sends request again.
      4. Web Socket: persistent connection between a client and a server that both parties can use to start sending data at any time.
      5. SSE :  persistent onnection between a client and a server where server send data to the client but not the reverse
      
      When to use Long Polling vs Web Socket ? 
        https://ably.com/blog/websockets-vs-long-polling
        https://dev.to/kevburnsjr/websockets-vs-long-polling-3a0o
        
        Clearly Websockets have many advantages over long polling and thus are appropriate for many applications which require consistent low latency 
        full duplex high frequency communication such as chat applications and real time applications.
        
        Scaling up 
        Websockets have problems with load distribution due to persistent connection.
        Long polling will have equal load distribution after its long timeout
        
     When to use Web Socker vs Server Sent Events ?
        Websocket for bi-directional communication whole SSE for server to client communication
        https://blog.bitsrc.io/websockets-vs-server-sent-events-968659ab0870
        
        WebSockets are widely used and valued in technological solutions such as real-time polling, chat, media players, multiplayer games, etc.
        Server-Sent Events: There are many applications where sending data from the client isn’t necessary. 
        SSEs are especially useful in status updates, social-media news feeds, push notifications, newsletters, etc.
     

      


        
        
        
        
      


    
    
    

          
          
          
          
          
          

          

          
        
        
  
  

      
      



