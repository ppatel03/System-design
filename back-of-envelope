THESE ARE IMPORTANT NOTES WHICH CAN HELP A LEAD ENGINEER IN DAY TO DAY JOB. I RECOMMEND TO EVEN ENTRY LEVEL ENGINEERS UNDERSTAND AND DESIGN SOFTWARE DESIGN BASED ON TRADE-OFFS.

THIS NOTES ARE NOT MEANT FOR ANY INTERVIEW PREP BUT MORE IN  DAY TO DAY JOB FOR BUILDING REAL-WORLD SYSTEMS. ANY COMMENTS/SUGGESTIONS ARE WELCOME.

Links :

https://bytebytego.com/courses/system-design-interview/back-of-the-envelope-estimation

https://matthewdbill.medium.com/back-of-envelope-calculations-cheat-sheet-d6758d276b05


https://docs.google.com/document/d/1wUCqhVHydWiDk6FJdFLSMpgigNrGcs4OFZg0Wa7JGEw/edit

Framework - 
  1. List all Functional requirements and assumptions
      requirements : in form of say, ability to xyz
      assumptions : types of users, are users already auth, types of devices (for device synchronization)
  2. Scale 
    2.1 Estimate QPS 
      a. Estimate Read vs Write QPS 
              
    2.2 Estimate bandwith quickly
      b. Estimate bandwidth per user for read and write QPS 
      c. Estimate bandwidth in total for read and write QPS
        tell why you are estimating.
        High total bandwidth means you need distributed system with high availability and replication in DBs to spread out the load
        High user bandwidth say 100 MB/s means latency could be very high 

    2.3 Estimate Total Storage bytes
      tell why you are estimating. 
        Just a high level conclusion if you need DB sharding which depends on size and bandwidth. 

  3. Non-Functional Requirements
    3.1 - Availability vs Consistency based on CAP (since consistency has different context for ACID, )
        - Availability %
        - Eventual Consistency (most of the times)
          - Data freshness SLA if eventual consistency
          - CAN PUT Strong consistency for few APIs like offline alerting, billing info
    3.2. Latency 
    3.3. Fault tolerant + Durability (Use pull) + Resilency 
        - Statelessness (CD)
        - Replication
    3.4. Secondary stuff : 
        a.4.1. Accuracy 
        a.5.1. Security : Auth, VPC, Data encryption if personal data 

  4. Metrics monitoring & Alerting for oncall using datadog
    a. Business success 
    b. Engagement metrics
    c. Reliability metrics

  From above, Conclude at high level from Back of envelope
   Example, High availability, bandwidth, large storage
        - multiple instances 
        - sharded storage with replicas  
  
     
Lets take a look into details : 

1. List all Functional requirements and assumptions
    - Ask detailed functional requirements 
        Why ? 
          - first since it helps with API design (example, ability to post tweet, view tweet)
  
    Assumptions
    - Ask types of users (admin, general) 
        Why ? 
          - since it helps with data access patterns and security checks (admin can edit personal info/block users )
    - Ask about interaction points - API, UI 
        Why ? 
          - since it helps you think end to end from UX perspective 
    - Ask about devices - Mobile or WEB 
        (though need reasoning why you asked this question :) . 
          1. mobile could be low bandwidth network ~ 100 Mbps or 10 MBps) -   https://www.quora.com/From-what-maximum-speed-can-a-smartphone-download-or-upload-some-data
          2. Consistency between devices ( example, chats read)
    - if its okay to assume user is authenticated

Time to understand scalability, latency aspects

2. Estimate QPS 
  a. Estimate Read vs Write QPS
    1. Ask about # of users - DAU/MAU
        QPS estimate
          - Read : Write ratio
            DO NOT divided by no. of seconds to get QPS if its explictly mentioned about concurrent users say 10% of DAU. In such cases, QPS = 10% DAU
          - 1 day ~ 10^5 s
          - 1 month ~ 3 * 10 ^ 6s and further ~ 10 ^ 6s
          - 1 week ~ 7 * 10 ^ 5s and further ~ 10 ^ 6s
          - 1 year ~ 365 * 10 ^ 5s and further ~ 10 ^ 8s
          For sake of time constraints, do not hesitate to round off large amount

  b. Estimate bandwidth for read and write QPS 
      say 1 KB for 100K QPS ~ 100 MBps
      - can help reason your latency
      - can help reason your replication in DBs or multiple servers
      - can help reason your sharding for database (remember internal data center max bandwidth to 25 Gbps ~ 2.5 GBps which means 1 KB = 1us ) https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-network-bandwidth.html 

3. Estimate Total Storage bytes (a year, 10 years)
    a. tell why you are estimating. Just a high level conclusion if you need DB sharding which depends on size and bandwidth. 
      - a year, 10 years 
    example, to help decide if sharding is need or master-replica model. Example more than 64 TB of storage definitely needs sharding (most AWS DBs or even cache like Redis has max 64 - 128 TB of storage https://aws.amazon.com/about-aws/whats-new/2020/09/amazon-aurora-increases-maximum-storage-size-128tb/ )

4. Non-Functional Requirements (https://www.datacore.com/blog/availability-durability-reliability-resilience-fault-tolerance/ )

    a. Reliability Standards
      https://www.pagerduty.com/resources/learn/reliability/ 
          Reliability, on the other hand, is a measure of the probability that the system will meet defined performance standards in performing its intended function during a specified interval.  Example of not meeting reliability stands is Twitter was hacked in 2019.
          means you need to meet certain standards. 

      a.1 - Availability vs Consistency 
        If high availability, then ask availability nos example, 99.999 or 3 9s
          - Availability %
            - https://www.pagerduty.com/resources/learn/reliability/ 
                Availability is a measure of the percentage of time that an IT service or component is in an operable state. Example of not meeting availability is Facebook was down in 2021 for about 3-5 hrs.
                this would help decide the redundancy and reliability - example mostly 99.99% a day means you can compromise 10s (10 ^ 5 * 10 ^ -4).
                of unavailability (its different from being unrelible). 
                With this you can even decide that, your system/part of the sytem could be offline for 10s in a day for reasons like roll out, stateful servers scaling, upgrading the database or even doing DB sync etc. 
                4 9s -> 10s
                5 9s -> 1 s
                6 9s -> 100 ms or less than 1s
                8 9s -> 1 ms or very high (no downtime)
                > 6 ps -> in us or super high (no downtime else SLA violated)
            - if high availability, then ask availability nos example, 99.99 or 4 9s

          - Consistency 
            - if Consistency over availability, it means high consitency.
            - Eventual Consistency (most of the times)
              - Data freshness SLA if eventual consistency 
                - With this in mind, it provides as good reasoning about latency 
                - Example, building search engine or yelp. With this you can even decide that, your system could be out of sync from third part servers for 10s in a day
            - Latency + quorum to replica if high consistency 
            Strong or Eventual consistency ? How does it help ? 
            - can help reason for low latency 
            - can help reason for quorum concensus
            - can help reason for MySQL if combined with ACID

            Tips to ensure consistency 
            - To maintain data consistency between internal services, ensuring exactly-once processing is very important.
            - To maintain data consistency between the internal service and external service (PSP), we usually rely on idempotency and reconciliation.
            - For replicas, ensure acknowledge only if all replicas respond to read/write update.
                  
      a.2. Latency 
        - this would help decide whether you really need low latency optimizations like cache, or not. Remember today mosts have 1 Gbps network which has 10 us to transfer 1 KB

      a.3. Fault tolerant + Durability + Resilency
        - Fault tolerance
        - Durability 
            - can help reason how much to replicate. 
                Example, say 99.999999% (six 9's) durability can be achieved assuming failure rate of one server is  0.01% (say 99.99% guarantee for ec2 on aws). This means you need 2 servers atleast which means 1 replica 
                  0.0001 ^ 2    = 0.0000001    or  10 ^ -8
                  1 - 0.0000001 = 0.9999999 or  8 9s  
                  % * 100       = 99.999999%      or  6 9s 
            - can help to reason using pull architectures
            - can help to reason out if you need erase for data recovery
            - can help to reason out if you need checksum for data integrity
            - can help to reason out if you need commit log (WAL) like Postgresql, (Row based log called binLog) MySQL and even Cassandra

        - Resiliency
                https://www.datacore.com/blog/availability-durability-reliability-resilience-fault-tolerance/ 
                Resiliency describes the ability of a storage system to self-heal, recover, and continue operating after encountering failure, outage, security incidents, etc. High resiliency doesn’t mean there is high data availability. It just means that the storage infrastructure is equipped enough to overcome disruptions.
                One indication of resiliency is measuring mean time to repair (MTTR), which captures how long it takes to get the storage infrastructure up and running after a failure. Lower the MTTR, better the resiliency. 
    

  5. Metrics monitoring & Alerting for oncall using datadog
    a. Business success 
    b. Engagement metrics
    c. Reliability metrics 

  6. Best practices if possible in deployment & scaling 
    a. Stateless services
      - try to add stateless servers if possible
      - stateful server only for few use cases like socket connections. Make sure you know how to deal with scalability for socket connections.

  7. High level design ( super important and help in broad understanding of the system)

  Conclude at high level from Back of envelope
    - High availability, bandwidth, large storage
      - multiple instances 
      - sharded storage 
      - replicas 
    
  Key components in high level design 

    a. Separate read and write service 
      - different bandwidth requirements
      - easier to scale independently
    b. separate out different components of the system (eg. video recommendation system )
      - avoids single point of failure
      - avoids monolithic 
      - map-reduce if needs to be sequential 
    c. data layer (just add the box data layer) 
    d. cover all API interactions at high level

    In-depth
      a. think about data model 
      b. Queue to avoid synchronous processing and support fault tolerant, durability 


  
    
System design thinking framework
  https://www.youtube.com/watch?v=i7twT3x5yv8 

   

Distributed Systems estimate

    Latency video
      https://www.youtube.com/watch?v=FqR5vESuKe0
      https://bytebytego.com/courses/system-design-interview/back-of-the-envelope-estimation
      https://matthewdbill.medium.com/back-of-envelope-calculations-cheat-sheet-d6758d276b05 

    Estimates in KB

    Send 1 KB sequential from 1 Gbps network = 10 us  
    Send 1 KB sequential from memory  = 0.25 us   (4 GB/s = 0.25 *  10 ^ -6 s) 
    Send 1 KB sequential from SSD   = 1 us    (1 GB/s = 1 * 10 ^ -6 s )
    Send 1 KB sequential from disk    = 30 us     (30 MB/s = 0.03 * 10 ^ -3 s ) 


    L1/L2 cache = 1 ns
    L3 shared cache = 10 ns
    Memory call = 100 ns
    Mutex lock/unlock =	100 ns
    System call = 1 us
    Context switching between linux threads : 10 us
    Ngnix to process http request : 100 us
    SSD write latency : 500 us
    SSD read latency = 16 us
    Round trip within the same datacenter 	 = 500 us 
    Redis cache latency including network : 1 ms (usually 500 us by internal redis)
    Between two availability zones : 5 ms
    Send packet CA (California) ->Netherlands->CA	= 150 ms 
    Disk seek	= 10 ms
    DNS response time ranges : 10ms to 200ms.
    Replication lag guarantee for AWS RDS : < 100 ms 
      https://www.bluematador.com/docs/troubleshooting/rds-replica-lag#:~:text=Replication%20lag%20measures%20how%20far,than%20100ms%20of%20replication%20lag. 
    TLS handshake time : 250 - 500 ms 
        https://zoompf.com/blog/2014/12/optimizing-tls-handshake/#:~:text=This%20handshake%20will%20typically%20take,is%20when%20the%20handshake%20happens. 
        So suggest to use nginx for keep alive and caching session key to avoid SSL/TLS handshake
    Multi-region failover time for AWS RDS : 60-100s  
      https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZSingleStandby.html
    Hard disk random seeks 
      https://bytebytego.com/courses/system-design-interview/s3-like-object-storage
      100 ~ 150 IOPS (per disk seeks ~ 10ms)

    AWS availability ~ 99.9% https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/appendix-a-designed-for-availability-for-select-aws-services.html
    AWS A100 GPU Lambda ~ 1$ per hour https://lambdalabs.com/service/gpu-cloud 
    AWS memory limit    ~ 24 TB RAM
    AWS DB (MySQL) limit ~ 64 TB storage
    AWS disk size ~ unlimited via EBS
    Max socket connections : 65,536 https://www.ibm.com/docs/en/zos/2.1.0?topic=domain-maximum-number-sockets 
    Max connections on the server : 
        https://josephmate.github.io/2022-04-14-max-connections
        Some think the limit 216=65,536 because that’s all the ports available in the TCP spec.
        Then theoretical limit a server can support on a single port is 2^48 which is about 1 quadrillion. This is huge and not possible.
        So basically, your # of connections depends on CPU, Mem, Network and other resources on the server.
        So to estimate max connections, use request size to memory mapping. Example, At 1M concurrent users, assuming each user connection needs 10KB of memory on the server (this is a very rough figure and very dependent on the language choice - 256 KB for java), it only needs about 10GB of memory to hold all the connections on one box.
        Best idea is to load test your services 
          AWS distributed load tests - https://aws.amazon.com/solutions/implementations/distributed-load-testing-on-aws/ 
            uses containers
            runs on fargate
            supports jmeter scripts


    Modern fiber optic cable vs ethernet (copper) cable
      - 10 Gbps vs 1 Gbps
      - https://www.cablewholesale.com/blog/index.php/2020/09/16/fiber-optic-vs-copper-ethernet-cables-the-difference/#:~:text=A%20more%20modern%20take%20on,much%20faster%20than%20copper%20cables.

  Types of Memory
  https://www.youtube.com/watch?v=lX4CrbXMsNQ
  RAM
  ROM 
  Hard drive
  SSD
  NVMe - high performance interface for SSDs thats connects to CPU via PCIe

  Types of Cache
  https://www.youtube.com/watch?v=dGAgxozNWFE

    HW cache : 
      L1 > L2 > L3
      Virtual memory - 
        maps program address to RAM + disk address  https://www.youtube.com/watch?v=qlH4-oHnBb8
        is equal to RAM + Disk size
      TLB cache for mapping above -  virtual to physical memory address
      OS level 
        Page cache  - recently used disks blocks in memory
        File cache  - 
        iNode cache
          - For context, INode is a file block pointer where filename is stored and looked up. Then using iNode, When we access a local file, we first fetch the metadata in the inode. We then read the file data by following the file block pointers to the actual disk locations.https://bytebytego.com/courses/system-design-interview/s3-like-object-storage   
          - iNode cache speed up file system operations by reducing disk access because if both iNode and file are in disk, then operation is super slow. (https://www.bluematador.com/blog/what-is-an-inode-and-what-are-they-used-for )

    Front-end :
      Browser 
        cache http responses with cache expiration policy
    Back-end : 
      CDN
        improve delivery of static content by caching it from the origin server
        https://cloud.google.com/cdn/pricing 
        remember CDN charges for egress (0.02$ per GiB - 1.024 GB), cache fill (same charge - 0.01$ per GiB) and 0.0075 per 10K req
      Load balancer cache
        to reduce the load on back-end servers and also improve response time.
      Messaging cache such as Kafka
        caches on disk using sequential IO (pretty fast)
        allows consumer to consume at their own pace
      Distributed caches
        Redis : in-memory db
        RocksDB : LSM tree to leverage disk 
      Full text search
        Elastic search - does reverse indexing which indexes texts to doc id
      Database cache
        WAL/binlog : data is first written to WAL before being indexed in B tree. Retention is 30 days for MySQL
        Buffer pool : cache query results
        Materialized views : precompute query results
        replication log : tracks replication state (MySQL uses binlog as its replication log)
          What is replication log
          https://help.serena.com/doc_center/cm/ver14_2_0_1/admin_console/adminhelp/ddevg_replication_logs/what_is_a_replication_log.htm



=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================


Distributed best practices
  Top 7 distributed design patterns 
    https://www.youtube.com/watch?v=nH4qjmP2KEE&t=12s&ab_channel=ByteByteGo
    Ambassador
      - proxy to offload work like handle logging, monitoring, handling retries
      - example, kubernetes uses envoy proxy to simply communication between services
    Circuit Breaker 
    CQRS
    Event Sourcing 
    Leader Election 
    Pub/Sub
    Strangler
      - helps in migration esp. monolithic to microservice. It has Strangler Facade which takes the decision whethere this API call should be made to monolithic or microservice. 
    Sharding

  Smallest short length required ( say for tiny url )
    The hashValue consists of characters from [0-9, a-z, A-Z], containing 10 + 26 + 26 = 62 possible characters. To figure out the length of hashValue, find the smallest n such that 62^n ≥ 365 billion. 

  Choices with protocol
    - TCP vs UDP
        https://www.spiceworks.com/tech/networking/articles/tcp-vs-udp/
        1. TCP is connection-oriented so you can get an ack (like success 200 OK in HTTP over TCP) while UDP is connectionless
        So UDP protocol is NOT suitable for sending electronic mail, viewing a web page, or downloading a file. However, it is preferred mainly for real-time applications like broadcasting or multitasking network traffic.

        Then why TCP is a still good choice on reciever of live streaming protocols like HLS, DASH ? 
          - because TCP has flow control mechanism to calibrate the pace of data transmission. Depending on the recipient host, TCP can adjust the speed at which data packets travel and avoid overwhelming the recipient. 

        Then what are the scenarios where UDP is recommended ? 
          Video calling (app like Zoom): UDP can support video 30 frames per second refresh rates (frame is single image in the video - https://darvideo.tv/dictionary/frame/ and min is 24 fps and max is 60 fps even better). The data transmission is so fast that a few dropped packets do not affect the user experience. 
          Online gaming: TCP’s many checklists and balances will significantly impact gaming experiences. Without perfect network conditions, frames will frequently freeze, and connections will restart if using TCP. That is why UDP is recommended. 

  Remove inconsistency caused by writes through multiple servers - versioning using vector clocks
    https://bytebytego.com/courses/system-design-interview/design-a-key-value-store
    This is very applicable for leaderless replication like Cassandra
    Vector clocks
      https://www.waitingforcode.com/big-data-algorithms/conflict-resolution-distributed-applications-vector-clocks/read 

        Local vector	                            Sent vector	                                New vector
        [(N1, 1), (N2, 1), (N3, 3)]	      [(N1, 1), (N2, 0), (N3, 3)]	                    [(N1, 1), (N2, 1), (N3, 3)]
        [(N1, 1), (N2, 0), (N3, 3)]	      [(N1, 1), (N2, 1), (N3, 3)]	                    [(N1, 1), (N2, 1), (N3, 3)]
        As you can see, during the merge of vectors, the largest values are taken. The whole process is illustrated more clearly in the last section vector clock example.

        Until now we saw that vector clocks can easily detect conflicts. But what happens after detecting one ? The first solution delegates the conflict resolution to the client. The client application receives all conflicted values and it decides how to deal with them (merge, discard, etc.). Another approach is server-based. One of its implementations can be last-write-wins strategy where the most recent value is returned to the client. It's only a short introduction to available solutions that will probably be detailed more in further posts.

        Cassandra use LWW 
        Dynamo uses Read-Repair (resolve conflicts during read) and anit-entropy

  Failure detection algo 
    - gossip prototocol to detect availability failures in distributed systems
    - Raft (description below) using used for master-replica model does both detect failures and resolve conflicts
      

  Master election concensus algo
      Ensure all replicas are always in-sync and master->slave is elected correctly when master goes down. We could use consensus algorithms such as Paxos [21] and Raft [22], or use consensus-based distributed databases such as YugabyteDB [23] or CockroachDB [24].
          https://bytebytego.com/courses/system-design-interview/stock-exchange 
          The leader sends heartbeat messages (AppendEnties with no content as shown in Figure 21) to its followers. If a follower has not received heartbeat messages for a period of time, it triggers an election timeout that initiates a new election. The first follower that reaches election timeout becomes a candidate, and it asks the rest of the followers to vote (RequestVote). If the first follower receives a majority of votes, it becomes the new leader.
      Nice visual - https://raft.github.io/
      Raft is one of the best concensus algo since it also helps in ensuring strong consistency for distributed transaction in cockroachDB https://github.com/cockroachdb/cockroach/blob/master/docs/design.md
      Even when new node is lagging behind or came live, leader in raft sends replication log along with the heartbeat to keep it consistent http://thesecretlivesofdata.com/raft/#replication 
        What is replication log
          https://help.serena.com/doc_center/cm/ver14_2_0_1/admin_console/adminhelp/ddevg_replication_logs/what_is_a_replication_log.htm
      Raft is used in Kafka, etcd, CockroachDB


  Failure handling algo
  - for temporary failures
      1. Sloppy quorum and hinted handoff (for Leaderless replication)
          https://github.com/keyvanakbary/learning-notes/blob/master/books/designing-data-intensive-applications.md#sloppy-quorums-and-hinted-handoff
          Leaderless replication may be appealing for use cases that require high availability and low latency, and that can tolerate occasional stale reads.

          It's likely that the client won't be able to connect to some database nodes during a network interruption.

            1. Is it better to return errors to all requests for which we cannot reach quorum of w or r nodes?
            2. Or should we accept writes anyway, and write them to some nodes that are reachable but aren't among the n nodes on which the value usually lives?

          The latter is known as sloppy quorum: writes and reads still require w and r successful responses, but those may include nodes that are not among the designated n "home" nodes for a value.

          Once the network interruption is fixed, any writes are sent to the appropriate "home" nodes (hinted handoff).

          Sloppy quorums are useful for increasing write availability: as long as any w nodes are available, the database can accept writes. This also means that you cannot be sure to read the latest value for a key, because it may have been temporarily written to some nodes outside of n


        
  - detecting inconsistency          
      1. Merklee tree (can detect and handle failures by syncing remaining replicas)
          What if a replica is permanently unavailable? To handle such a situation, we implement an anti-entropy protocol to keep replicas in sync. Anti-entropy involves comparing each piece of data on replicas and updating each replica to the newest version. A Merkle tree is used for inconsistency detection and minimizing the amount of data transferred.
          Quoted from Wikipedia [7]: “A hash tree or Merkle tree is a tree in which every non-leaf node is labeled with the hash of the labels or values (in case of leaves) of its child nodes. Hash trees allow efficient and secure verification of the contents of large data structures”.
                Step 1: Divide key space into buckets (4 in our example) as shown in Figure 13. A bucket is used as the root level node to maintain a limited depth of the tree.
                Step 2: Once the buckets are created, hash each key in a bucket using a uniform hashing method (Figure 14).
                Step 3: Create a single hash node per bucket (Figure 15).
                Step 4: Build the tree upwards till root by calculating hashes of children (Figure 16).
          https://bytebytego.com/courses/system-design-interview/design-a-key-value-store

  Merging concurrently written values
    https://github.com/keyvanakbary/learning-notes/blob/master/books/designing-data-intensive-applications.md#sloppy-quorums-and-hinted-handoff
    No data is silently dropped. It requires clients do some extra work, they have to clean up afterward by merging the concurrently written values. Riak calls these concurrent values siblings.

    Merging sibling values is the same problem as conflict resolution in multi-leader replication. A simple approach is to just pick one of the values on a version number or timestamp (last write wins). You may need to do something more intelligent in application code to avoid losing data.

    If you want to allow people to remove things, union of siblings may not yield the right result. An item cannot simply be deleted from the database when it is removed, the system must leave a marker with an appropriate version number to indicate that the item has been removed when merging siblings (tombstone).

    Merging siblings in application code is complex and error-prone, there are efforts to design data structures that can perform this merging automatically (CRDTs).

  Directed acyclic graph (DAG) model
        To support different video processing pipelines and maintain high parallelism, it is important to add some level of abstraction and let client programmers define what tasks to execute. For example, Facebook’s streaming video engine uses a directed acyclic graph (DAG) programming model, which defines tasks in stages so they can be executed sequentially or parallelly [8]. In our design, we adopt a similar DAG model to achieve flexibility and parallelism. Figure 8 represents a DAG for video transcoding.


  Symmetric vs Asymmetric encryption
    https://blog.mailfence.com/symmetric-vs-asymmetric-encryption/ 
    RSA, DSA, Diffie-Hellman - key exchange is asymmetric
    AES, DES - is symmetric
    Hash
    SHA-1 is hash function and not encryption one
    SHA-256 is more secure https://www.keycdn.com/support/sha1-vs-sha256 
    BCrypt / Scrypt (slow hashes than SHA-256 but more secure)

  Eliminate disk access
    Disk accesses can be eliminated using mmap. `mmap(2)` provides a mechanism for high-performance sharing of memory between processes. 
    Modern exchanges take advantage of this to eliminate as much disk access from the critical path as possible. `mmap(2)` is used in the server to implement a message bus over which the components on the critical path communicate. The communication pathway has no network or disk access, and sending a message on this mmap message bus takes sub-microsecond. By leveraging mmap to build an event store, coupled with the event sourcing design paradigm which we will discuss next, modern exchanges can build low-latency microservices inside a server.

    It basically just tells the OS to put your requested file in main memory shared by multiple process - https://db.cs.cmu.edu/mmap-cidr2022/#:~:text=Memory%2Dmapped%20(MMAP)%20file,file%20resided%20entirely%20in%20memory.  

  Change Data Capture
  CDC is a mechanism that reads data changes from the database and applies the changes to another data system. 
    -  One common solution is Debezium [9]. It uses a source connector to read changes from a database and applies them to cache solutions such as Redis [10].
    - Another is Kafka Connect (pg 457)
  Pg 455 - DDIA 
    Rather than database triggers, Parsing replication log can be a more robust approach. Example Debezium uses MySQL binlog (same as replication log for MYSQL).
    Along with this you also need "Initial snapshot" since replication log cannot carry entire DB changes.

  In-sync replicas
    https://bytebytego.com/courses/system-design-interview/distributed-message-queue
    We mentioned that messages are persisted in multiple partitions to avoid single node failure, and each partition has multiple replicas. Messages are only written to the leader, and followers synchronize data from the leader. One problem we need to solve is keeping them in sync.
    In-sync replicas (ISR) refer to replicas that are “in-sync” with the leader. The definition of “in-sync” depends on the topic configuration. For example, if the value of replica.lag.max.messages is 4, it means that as long as the follower is behind the leader by no more than 3 messages, it will not be removed from ISR [10]. The leader is an ISR by default.

  Data mirroring with replicas
    https://bytebytego.com/courses/system-design-interview/distributed-message-queue 
    If all the replicas of a partition crash, the data for that partition is lost forever. When choosing the number of replicas and replica locations, there’s a trade-off between data safety, resource cost, and latency. It is safer to distribute replicas across data centers, but this will incur much more latency and cost, to synchronize data between replicas. As a workaround, data mirroring can help to copy data across data centers, but this is out of scope. The reference material [14] covers this topic.
    Mirroring refers to keeping a backup database server for a master database server. It is not meant for distributed systems like (master(write)->replica(read)) but it morese serves as a DR result. So this is reason why there is always mirroring on replicas
    https://www.tutorialspoint.com/difference-between-mirroring-and-replication 

  HTTP Encrpytion, Authentication, Authorization and Security 
        Asymmetric encryption could be used here for payment gateway
          https://www.youtube.com/watch?v=AQDCe585Lnc
          also used in SSH, Bitcoin, Emails using PGP(Pretty Good Privacy) protocol

        How HTTPS works ? How it uses SSL and TLS
          https://www.youtube.com/watch?v=hExRDVZHhig
          HTTPS uses public key encryption to secure data using SSL (Secure socket layer) protocol. Basically server gives SSL certicate to client and acknowledgement is established between the two. Then info can be transferred securely using encryption.
          TLS is latest and successor of SSL
          Today most websites supports https because of google standards

          ByteByteGo : https://www.youtube.com/watch?v=j9QmMEWmcfo
            Moderm HTTPS uses TLS - https://excalidraw.com/#room=6ee4f5d6f525a19d546c,Duz5YdLv0qhwGv9pNSZo_Q 
            1. first establishes TCP handshake at transport (helps in getting the sequence number)
            2. then client sends hello and gets the certificate from server which has public key of server (Asymmetric) 
            3. then client encryptes his/her session key with server's public key and then on server, it gets clients sesssion key by decrypting with server's private key. This is Asymmetric encryption
            4. Now both has session key and uses session key as cipher to encrypt and decrypt at both sides. This is Symmetric encryption.

            SSL uses public key encryption (Asymmetric encryption) only. However, this website claims it supports both Asymmetric and symmetric - https://www.trentonsystems.com/blog/symmetric-vs-asymmetric-encryption
          

        TLS handshake latency 
          TLS handshake time : 250 - 500 ms https://zoompf.com/blog/2014/12/optimizing-tls-handshake/#:~:text=This%20handshake%20will%20typically%20take,is%20when%20the%20handshake%20happens.  
          Suggestion is to use nginx which uses keep-alive and stores encrypted session key 

        How SSO work ? 
            https://www.youtube.com/watch?v=O1cRJWYF-g4
            
            Two ways 
              using SAML
                - uses XML 
                - uses public key encryption 
              Open id connect
                - uses json web token (JWT) 
                - which uses token
        
        What is SAML ? 
          https://www.youtube.com/watch?v=7RrFhC-e8QA&pp=ygUEU0FNTA%3D%3D&ab_channel=JumpCloud

        Authentication and Authorization
          https://blog.bytebytego.com/p/password-session-cookie-token-jwt 
          modern websites uses session id
          
          Oauth 2.0 work ? 
            https://www.youtube.com/watch?v=CPbvxxslDTU
            JWT is data format for user information in the OpenID Connect standard, which is the standard identity layer on top of the OAuth 2.0 protocol. Nginx plus supports this - https://docs.nginx.com/nginx/admin-guide/security-controls/configuring-jwt-authentication

            Related to above 
            How payment gateway work ?
              https://www.youtube.com/watch?v=GUurzvS3DlY

        Session id based authenitication vs JWT
          https://stytch.com/blog/jwts-vs-sessions-which-is-right-for-you/ 
          - session id needs db look up after authentication although nginx can cache it
          - jwt does not need db look up since all necessary info is contained in the client request. Invalidating JWT token is complex so token is valid even after log out.
      
      How to send passwords from client to server
        https://github.dev/ppatel03/System-design/blob/main/back-of-envelope 
        before you send the password from browser, you should know the server is already https which uses TLS encryption. So no need to encrypt on client side code.  
        - this is why you observer that client passwords are sent in plaintext from developer tools at application layer (browser) which is layer 7
        - Encryption takes place in between application and transport which could be presentation layer or session layer in OSI or Transport layer in TCP/IP
            (https://community.fs.com/blog/tcpip-vs-osi-whats-the-difference-between-the-two-models.html)
            (https://security.stackexchange.com/questions/19681/where-does-ssl-encryption-take-place)

      Difference between Oauth 2.0 vs SAML and OpenID Connect
        https://www.okta.com/identity-101/whats-the-difference-between-oauth-openid-connect-and-saml/
          OAuth 2.0 is a framework that controls authorization to a protected resource such as an application or a set of files, while OpenID Connect and SAML are both industry standards for federated authentication.

          OpenID Connect is built on the OAuth 2.0 protocol and uses an additional JSON Web Token (JWT), called an ID token, to standardize areas that OAuth 2.0 leaves up to choice, such as scopes and endpoint discovery. It is specifically focused on user authentication and is widely used to enable user logins on consumer websites and mobile apps.
          SAML is independent of OAuth, relying on an exchange of messages to authenticate in XML SAML format, as opposed to JWT. It is more commonly used to help enterprise users sign in to multiple applications using a single login.

          Okta uses SAML 

      What Is IAM (Identity & Access Management)?
        https://spectralops.io/blog/top-11-identity-access-management-tools/
          Identity and Access Management (IAM) tools are designed to manage identities (users) and access (authentication and authorization). The goal of IAM tools is to streamline the management of user accounts and privileges from all aspects.
          In most cases, an IAM solution will let you define a policy. This policy will, in turn, determine the roles of users. Each role defined will have permissions set. These permissions allow access to specific resources. 
          Example, Okta, Auth0 and other cloud service providers (AWS IAM, Microsoft Azure)
            Small disadvantage with Okta
              Currently, Okta falls short on passwordless solutions, prompting users to change their passwords often. In addition, users also report some technical issues with logins.
            While Auth0 could be integrated with Google / Twitter / FB sign-in (which uses open id connect) - https://www.youtube.com/watch?v=yufqeJLP1rI&ab_channel=Fireship 

      Not important : 
      So what tools usually company use for authorization internally ? 
        https://www.reddit.com/r/sysadmin/comments/k09crr/what_tools_does_your_company_use_for/ 
          SAML and LDAPS would cover most use cases which are standardized authentication protocols
          SAML can be offered via Okta 
          SAML vs LDAP
            https://www.youtube.com/watch?v=_NCcLJin30E&ab_channel=JumpCloud
            Both are used to access IT resources
            LDAP -                                                                            SAML 
            1990s (still used today)                                2000s has SSO solutions which federates identities through web applications
            mostly used in back-end of user auth such user info     extends user creds to the cloud  
            source of truth for IDP                                 its not SOT but protocol for exchanging authenication between directories and web apps. 
            used in Linux applications like OpenVPN, docker         used by IAM providers like Okta 

  Network/s / Networking
    OSI model
      https://www.youtube.com/watch?v=0y6FtKsg6J4

    TCP/IP model
      https://www.techtarget.com/searchnetworking/definition/TCP-IP 
      Modern day uses TCP/IP over OSI model

    HTTP over TCP/IP 
      https://www.goanywhere.com/blog/http-vs-tcp-whats-the-difference
      In the case of HTTP which applies at layer 7, before a client and server can exchange an HTTP request/response, they must establish a TCP connection first. Therefore, HTTP relies on the TCP standard in order to successfully do its job.

      HTTP 1.0, 1.1 and 2.0
        https://www.youtube.com/watch?v=a-sBfyiXysI
        1.0 -> 
          tcp handshake all the time. Very low performat
        1.1 -> 
          keep alive, persistent conenection avoids tcp handshake. But suffers from head of line blocking - if a request is blocked for any reason them all subsequent requests are blocked. So applications like browsers send muliple requests by opening many TCP connections in parallel.
        2.0 -> 
          brought concept of multiple independent streams on same tcp connection which resolves head of line blocking at application layer. But the issue still in transport layer in TCP.
          it brought push capabiliy (SSE) to send update to the client from  the server
        3.0 -> Quic which uses UDP
          recent and used Quic protocol which uses UDP instead of TCP
          multiple streams share the same Quic connection. Due to use of UDP, it resolves head of line blocking even at transport layer
          Application : 
            designed for mobile heavy internet usage where user keeps connecting between different networks.
            How ? uses the concept of connection id between client and server which always remains same even if different network is selected.
        https://www.linkedin.com/posts/gkcs_http3-systemdesign-networkprotocols-activity-7041095963335139329-uV44/?utm_source=share&utm_medium=member_desktop 

    Video streaming protocol
      https://linuxhit.com/rtmp-vs-hls-vs-dash-streaming-protocols/#0-rtmp-vs-hls-vs-dash-streaming-protocols 
      RTMP
        RTMP stands for Real Time Messaging Protocol. Sounds more generic than streaming doesn’t it? RTMP is a creation of Macromedia and through Adobe’s acquisition of Macromedia it now belongs to Adobe. Remember all those flash videos and the flash player?
        RTMP is a TCP based protocol designed for low latency.
        Low latency vs HLS and DASH.
        Used heavily in production or broadcasting of video streams from streamer's device
      HLS
        HLS is Apple’s streaming protocol. HLS stands for HTTP Live Streaming. Leveraging HTTP has many benefits. Internet infrastructure like firewalls and content delivery networks already handle HTTP.
        - key benefit of HLS is adaptive streaming. Adaptive streaming enables changing the quality of the video mid-stream. For example, if your internet connection starts slowing down, the bit rate can adapt to ensure the stream stays viewable. Despite the degradation in bandwidth.
        - HLS only supports the H264 codec for video and AAC for audio.
      DASH
        DASH stands for Dynamic Adaptive Streaming is an open standard and similar to HLS. It uses HTTP, small chunks and also supports adaptive streaming. A key difference between HLS and DASH is that DASH is codec agnostic. Hence you can use any codec you want.

    DNS
      https://www.youtube.com/watch?v=27r4Bzuj5NQ
      queries from browser go to DNS resolver (ISP, Cloudfare, Google) which fetches authoritative name server in hierarchial fashion.
      DNS lookups are cached at client machine, resolver

    How Wifi works
      https://computer.howstuffworks.com/wireless-network.htm
      A wireless network uses radio waves, just like cell phones, televisions and radios do.
        1. A computer's wireless adapter translates data into a radio signal and transmits it using an antenna.
        2. A wireless router receives the signal and decodes it. The router sends the information to the internet using a physical, wired ethernet connection.
      The radios used for WiFi communication are very similar to the radios used for walkie-talkies, cell phones and other devices. They can transmit and receive radio waves, and they can convert 1s and 0s into radio waves and convert the radio waves back into 1s and 0s.
      2.4 GHz connections are now considered somewhat obsolete because they carry lower data speeds than 5 GHz.
      Remember 2.4 GHz vs 5 GHz this is different from 4G vs modern 5G (upto 1Gbps) which is based on bandwidth to tranfer data

    Anycast 
      https://www.youtube.com/watch?v=vOYjcOs1dUU
      multiple servers with same IP address so that nearest one can serve. Example, CDN or multiple load balancers

  API design for websocket connection
    https://bytebytego.com/courses/system-design-interview/nearby-friends
    - do not add paths else it will look like REST which is HTTP
    format
      1. Initiated by Client : Request and Response
      2. Initiated by Server : Data sent 

  Physical cores, Virtual cores and Logical cores
    https://www.youtube.com/watch?v=O2g6381An_k 
    Example, i7 has 4 physical cores and Each core can have ability to run 2 threads simultaneously. So 4 virtual cores. So 8 logical cores

  Processes vs Thread
    https://www.youtube.com/watch?v=4rLW7zg21gI 
      Process is responsible for executing set of instructions from an application
      Process will contain multiple threads


  Logging (errors), Metrics Monitoring (system level, business) and automated Deployment
    Logging (errors)
      - datadog
      - logging errors to trigger alerts, sev
    Metrics Monitoring (system level, business)
      - datadog, 
    Alerting
      - datadog
    Continuous integration
        develop -> commit -> build -> test ->  deploy
      https://blog.inedo.com/continuous-integration-performance-testing-best-practices

  Failure/Error handling to make fault tolerant and recover it making it resilient
    - Common techniques are retry the processing for queue consumer or offload to another server 
    - try maintaining the commit log (checkpointing)
    - explore event sourcing technique since events are immutable and you can generate the desired state (CQRS)
    - Exponential backoff [14] might be a good retry strategy (https://bytebytego.com/courses/system-design-interview/distributed-email-service)
    - Using Kafka Queue which can persist messages even when downstream services are down
     


  Event sourcing
      https://www.confluent.io/blog/event-sourcing-cqrs-stream-processing-apache-kafka-whats-connection/
      https://bytebytego.com/courses/system-design-interview/digital-wallet
    One design philosophy that systematically answers those questions is event sourcing, which is a technique developed in Domain-Driven Design (DDD) [9].
      Advantages :- https://www.youtube.com/watch?v=i2eVTk2Fb40&t=137s
          - complete rebuild to get state of system
          - temporal queries (what happened at day x)
          - event replay for debugging
          Example, nice examples of event sourcing design 
          - https://www.youtube.com/watch?v=lg6aF5PP4Tc
              Event sourcing is an alternative means of storing data as log of events rather than table that is updated,read and deleted. In event sourcing, you create a table and append events to it as they occur building up an event log. So reading data has an additional step because you need to pull up all the events and reduce it to derive into table format. Doing this process in real-time for large number of events is complicated and thus needs pre-processing of table format data from event log. This technique is an example of CQRS where you write in event log and store it immediately in read-only table or materialized views for reading.  
              Confluence using Kafka as event log for writes and generates materialized views for read 

  Verify integrity through blockchain
    https://www.youtube.com/watch?v=SSo_EIwHSd4 

  Using LSMTree + SSTable are efficienct for heavy writes and heavy reads on noSQL databases. B tree is not
      Great video : https://www.youtube.com/watch?v=I6jB0nM9SKU 
      Good read : https://rahulpradeep.medium.com/sstables-and-lsm-trees-5ba6c5529325
      This technique is used by Cassandra, RocksDB, Google BigTable, Apache HBase
      MongoDB can be tuned to use LSM treee
    
  Algo if element is the member of the set
    Use Bloom filter
      https://www.youtube.com/watch?v=V3pzxngeLqw
      A bloom filter is a space-efficient probabilistic technique to test if an element is a member of a set. Refer to the reference material [2] for more details.
      Applications
        key value store to check in SSTable
        crawlers to check if url in malicious 
        CDN if page is in cache
  
  Clock synchronization amongst different servers in distributed systems
    https://www.youtube.com/watch?v=f1hlCZB0GDA 
    - Use network time protocol (NTP)
    - How does NTP gets the time from GPS 
        https://www.galsys.co.uk/time-reference/development-of-atomic-clocks-and-timescales/atomic-clock-ntp-servers.html 
        Atomic clocks are also the basis of GPS (Global Positioning System) as each satellite contains an atomic clock as accurate time is integral for positioning (a position anywhere is made up of a direction, a velocity and time).
        GPS signals can also be used to capture a time signal. This is now the most common way computer networks retain accurate time which is also essential in many communications and applications.Most computer networks use a NTP server (Network Time Protocol) to synchonise their devices to an atomic time signal received via the GPS network.
    - NTP has some challenges but its still the reliable way to synchronize clocks over the network with the error delay of 35 ms - Pg 288 DDIA. So applicable for scheduling systems with 1s - 1 min granularity
    However, some trading firms enforce accuracy of no more than 100 us microseconds and worst case 100 ms for network congestion. Such accuracy can be achieved with GPS (the precision time protocol PTP) with careful deployment and monitoring but requires effort and expertise.
    For systems which highly relies on time like "Scheduler", any server(node) which drifts too far at NTP sync (say 10s or even 1 min) should be removed.
    - LWW (last write wins) is common issue with multi-leader replication and leaderless databases like Cassandra - Pg 292 DDIA
    Google achieving accuracy with time synchronization
    - Google Spanner (the next CockroachDB) provides time intervals using Google's Truetime API(deploys GPS receiver or atomic clock in each datacenter) to support visibility into accuracy of its time. - Pg 294 DDIA
    - Using above, Google Spanner implements snapshot isolation across datacenters based on timestamp with intervals reported above
    - To improve accuracy, Google deploys GPS or atomic clock in each datacenter with guarantee SLA of 10 ms

  Dynamic rendering to allow search engines to parse content
    https://developers.google.com/search/docs/crawling-indexing/javascript/dynamic-rendering 
    Dynamic rendering is a workaround and not a recommended solution, because it creates additional complexities and resource requirements.

  Scale through Kafka
    Kafka is known for high throughput, low latency streaming, message persistence, high availability, fault tolerant system
    https://bytebytego.com/courses/system-design-interview/metrics-monitoring-and-alerting-system
    There are a couple of ways that we can leverage Kafka’s built-in partition mechanism to scale our system.
      Configure the number of partitions based on throughput requirements.
      Partition metrics data by metric names, so consumers can aggregate data by metrics names.
      Further partition metrics data with tags/labels.
      Categorize and prioritize metrics so that important metrics can be processed first - possible with kafa

  Scale map reduce operations 
    https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation 
    If you are interested in the details, please refer to reference material [20]. Aggregation service is horizontally scalable by adding or removing nodes.
    Question is how you increase the throughput ? 
        1. assign each thread in the server
        2. deploy aggregation service nodes on resource providers like Apache Hadoop YARN [21]
      Option 1 is easier to implement and doesn’t depend on resource providers. In reality, however, option 2 is more widely used because we can scale the system by adding more computing resources.
  Using star schema as pre-filtered form 
    https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation
    To support data filtering like “show me the aggregated click count for ad001 within the USA only”, we can pre-define filtering criteria and aggregate based on them. This technique is called the star schema [11], which is widely used in data warehouses. The filtering fields are called dimensions. 
    A limitation with this approach is that it creates many more buckets and records, especially when we have a lot of filtering criteria.


  Combine batch processing and streaming with one service 
    https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation
    For a system that contains two processing paths (batch and streaming) simultaneously, this architecture is called lambda [14]. A disadvantage of lambda architecture is that you have two processing paths, meaning there are two codebases to maintain. Kappa architecture [15], which combines the batch and streaming in one processing path, solves the problem. The key idea is to handle both real-time data processing and continuous data reprocessing using a single stream processing engine. 
    Why Kappa - no need to maintain two codebases for stream processing and batch processing
      https://www.oreilly.com/radar/questioning-the-lambda-architecture/

  Process delayed events 
    https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation
    One way to mitigate this problem is to use “watermark” (the extended rectangles in Figure 14), which is regarded as an extension of an aggregation window. This improves the accuracy of the aggregation result. By extending an extra 15-second (adjustable) aggregation window, window 1 is able to include event 2, and window 3 is able to include event 5.
    Remember, this is timestamp based - so it avoids overlap between two windows. Example, window 1 is to accept data which has timestamp from 0 to 10s and window 2 is for 10s to 20s. But packets in window 1 coould be delayed, so wait for extra 2 seconds.

  Aggregation window
    https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation
    According to the “Designing data-intensive applications” book by Martin Kleppmann [16], there are four types of window functions: tumbling (also called fixed) window, hopping window, sliding window, and session window. We will discuss the tumbling window and sliding window as they are most relevant to our system.
      - In the tumbling window (highlighted in Figure 15), time is partitioned into same-length, non-overlapping chunks. The tumbling window is a good fit for aggregating ad click events every minute (use case 1).
      - In the sliding window (highlighted in Figure 16), events are grouped within a window that slides across the data stream, according to a specified interval. A sliding window can be an overlapping one. This is a good strategy to satisfy our second use case; to get the top N most clicked ads during the last M minutes.

  Fault tolerance systems
    - retry logic by storing last online state in S3 / KV store (checkpointing)
    - save computational work in snapshots storage for low latency (https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation)

  Monitoring metrics
    total Latency and also at each stage
    Queue size if used
    system resources : CPU, disk, JVM, etc.

  Reconcilliation for data consistency
    https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation
    Reconciliation means comparing different sets of data in order to ensure data integrity. Unlike reconciliation in the banking industry, where you can compare your records with the bank’s records, the result of ad click aggregation has no third-party result to reconcile with.

  Alternative designs for aggregation
    https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation 
    In a generalist system design interview, you are not expected to know the internals of different pieces of specialized software used in a big data pipeline. Explaining your thought process and discussing trade-offs is very important, which is why we propose a generic solution. Another option is to store ad click data in Hive, with an ElasticSearch layer built for faster queries. Aggregation is usually done in OLAP databases such as ClickHouse [24] or Druid [25]. Figure 29 shows the architecture.


  Line protocol
    https://bytebytego.com/courses/system-design-interview/metrics-monitoring-and-alerting-system
      CPU.load host=webserver01,region=us-west 1613707265 76
      CPU.load host=webserver01,region=us-west 1613707265 83

      The average CPU load could be computed by averaging the values at the end of each line. The format of the lines in the above example is called the line protocol. It is a common input format for many monitoring software in the market. Prometheus [6] and OpenTSDB [7] are two examples.

          A metric name	String
          A set of tags/labels	List of <key:value> pairs
          An array of values and their timestamps	An array of <value, timestamp> pairs

  Push vs Pull model
      https://bytebytego.com/courses/system-design-interview/metrics-monitoring-and-alerting-system 
      Metrics monitoring : 
        Examples of pull architectures include Prometheus.
        Examples of push architectures include Amazon CloudWatch [16] and Graphite [17].

      Easy debugging: pull
      Health check : pull
      Short lived jobs : push
          Some of the batch jobs might be short-lived and don’t last long enough to be pulled. Push wins. But This can be fixed by introducing push gateways for the pull model [22].
      Firewall or complicated network setups : push
      Performance : Push
        since push uses UDP, it wins vs pull which relies on TCP
      Data authenticity	: Pull
        zookeeper can provide valid servers to pull from
      Fault tolerant and reliability : Pull
        since failed messages can be queried again

  gRPC
    For production systems, inter-service communication often employs a modern and high-performance remote procedure call (RPC) framework like gPRC. There are many benefits to using such frameworks. To learn more about gPRC in particular, check out [3].
    NOT recommended for browser -> server communication

  Avoid Risk of double booking in transaction systems like hotel rooms, payment systems
    There are two common approaches to solve this problem:

      Client-side implementation. A client can gray out, hide or disable the “submit” button once a request is sent. This should prevent the double-clicking issue most of the time. However, this approach is not very reliable. For example, users can disable JavaScript, thereby bypassing the client check.

      Idempotent APIs. Add an idempotency key in the reservation API request. An API call is idempotent if it produces the same result no matter how many times it is called. Figure 8 shows how to use the idempotency key (reservation_id) to avoid the double-reservation issue. The detailed steps are explained below.

  Different types to avoid race conditions during concurrency
  applications : hotel booking for last room, uber drivers all responding to request
    1. Pessimistic locking 
        - as name suggests, it locks the record while read + update
        - good when data contention is high
        - slows down performance
    2. Optimistic locking
        - has validation check based on version number. So no locking is needed. 
        - good when data contention is low and improves performances
        - bad when data contention is high 
    3. database constraint
        - add constraint at table level
        - same as Optimistic and its easier to implement
    another example when you need locking for optimistic or database constraint : when all uber drivers confirm the ride

  CI/CD practices
    https://www.youtube.com/watch?v=42UP1fxi2SY
    - CI stands for continuous integration and is more common. Its the practice of using automation to enable teams to merge code changes in shared repository. Each commit triggers an automated workflow on the CI server that builds and runs series of tests to make sure commit is safe to merge into main branch.
      - CI process : code -> build -> test -> release
      - tools for CI - github
      - tools to manage CI process: code (github), build (gradle, webpack, github actions), test (jest), release (jenkins, buildkite)

    - CD stands for continuous deployment but it is hard and many companies only apply to stateless systems. Needs good production monitoring for CD like datadog. Blue green deployments is common. Canary deployment is also possible for products with 100 million users.
    - CD is hard for database backend clusters or websocket cluster. So deployment process is manual here and needs dedicated platform team
      - CD process : code -> build -> test -> acceptance -> deploy in production
      - CD tools : github actions, jenkins, argoCD for kubernetes 

    How  companies perform modern CI/CD today ? 
          https://levelup.gitconnected.com/gitops-ci-cd-using-github-actions-and-argocd-on-kubernetes-909d85d37746 
      CI : build -> test -> commit -> push image to ECR and update image reference(version) in Helm chart
          - with help of github actions, it can build and run tests on the PR. Once commit is merged, it pushes the image in the respository

      CD : argoCD polls Helm chart and deploys to Kubernetes (AWS EKS)
          - argoCD detects thid image via polling and deploys on kubernetes using helm charts (yaml file for kubernetes deployment - https://www.youtube.com/watch?v=w51lDVuRWuk&ab_channel=DevOpsJourney) 
  

  How to provison resources in the could
    https://www.youtube.com/watch?v=tomUWcQ0P3k
    Use terraform as Infrastructure as code

  
  How to store password in the database
    https://www.youtube.com/watch?v=zt8Cocdy15c
    use hash(password + salt(random string attached))
    salting helps in eliminating rainbow attacks - https://www.beyondidentity.com/glossary/rainbow-table-attack
    Even if you salt in same DB as password, attacker will never able to guess the actually password since it does not know how salt text is combined with password before hashing.

  Bare metal, VMs and containers
    https://www.youtube.com/watch?v=Jz8Gs4UHTO8
    VMs have hypervisor to run the guest OSes
    Containers have container agent (eg., docker) instead of hypervisor to run the application in its own environment but uses instance/host OS to operate and have faster operations and more flexibility but suffers from security sharing common instance/host OS. To achieve security, you can have a container inside the VM but it looses flexibility now. 
    A bare metal server can host more containers than VMs

  Cloud Native practices
    https://www.youtube.com/watch?v=p-88GN1WVs8
    - microservice architecture
    - containers
    - Dev ops (ci/cd)
    - Cloud native open standards (distributed tracing, service mesh)

  Kubernetes
    https://www.youtube.com/watch?v=TlHvYWVUZyc
    - container orchestration system
    - VERY IMPORTANT :  use it when you need 
      - automated rollbacks (deployment) : kubernetes smartly roll back the deploy if the pod with new container image is throwing errors (health checks, exceptions)
      - self-healing (downtime)
      - horizontal scaling 
    - due to its complexity, do not use it if your org is small enough
    https://www.youtube.com/watch?v=VnvRFRk_51k&t=78s
      use master-worker architecture. 
        Master - 
          - contains API server which can accept configuration about your application through YAML file
          - controller managers to keep track of what's happening in the cluster
          - scheduler ensures pod placement on workers
          - etcd which has info about workers and also a backing store 
          - has another master as secondary to avoid SPOF 
        Workers 
          - deploys your application into pods which is a wrapper for containers. So one pod can have many containers.
          - usually its one pod per application. You only have multiple applications in one pod when its helper to another - example, datadog agent.
          - If pod dies, new ones gets easily created.
          - Since each pod gets IP address, then won't it creates disruption if a pod dies. Example MySQL pod dies ? 
            - So pod is supported by "service" (may be kube-proxy) sitting in each pod. This concept which is similar to load balancer and IP address is assigned to a service instead of a pod. It also serves as a load balancer
          Similar video : https://www.youtube.com/watch?v=2vMEQ5zs1ko 

  WebSocket - use for bi-directional communication
    Websocket with load balancer or not - debatable 
      websocket can be supported with load balancing through websocket tunnel but suggest not to do it https://www.haproxy.com/documentation/hapee/latest/load-balancing/protocols/websocket/ 
      comparison of websocket vs long polling during scaling up / down  
        - https://dev.to/kevburnsjr/websockets-vs-long-polling-3a0o 
      Some systems does support websocket and load balancer https://bytebytego.com/courses/system-design-interview/nearby-friends 
    Types of socket connections
      https://www.ibm.com/docs/en/zos/2.1.0?topic=sockets-socket-types
      Stream - data transmitted in continuous stream with length so that the receiver can read that length and process the data. Its reliable means data is sent without error or duplication
      Datagram -  socket interface defines a connectionless service. Datagrams are sent as independent packets. The service provides no guarantees; data can be lost or duplicated, and datagrams can arrive out of order
      raw socket -  interface allows direct access to lower layer protocols, such as IP and Internet Control Message Protocol (ICMP). This interface is often used for testing new protocol implementations.
    Websocket API
      https://www.geeksforgeeks.org/difference-between-rest-api-and-web-socket-api/ 
      - Unlike REST, Websocket API does not require a new connection to be set up for each message to be sent between clients and servers. Once the connection is set up the messages can be sent and received continuously without any interruption.
      - WebSocket APIs are suitable for IoT Applications with low latency or high throughput requirements. 
      - Websockets could be scaled on single server ~ 65K ports
      - Challenges with websocket are security, browser compactibility are complex to handle
    Difference between socket and a port
      https://www.geeksforgeeks.org/difference-between-socket-and-port/?ref=rp
      Both Socket and Port are the terms used in Transport Layer. To be more precise, Socket is in session layer in OSI model. A port is a logical construct assigned to network processes so that they can be identified within the system. A socket is a combination of port and IP address. An incoming packet has a port number which is used to identify the process that needs to consume the packet.
    How to scale Websocket clusters
      https://bytebytego.com/courses/system-design-interview/nearby-friends
        For the WebSocket cluster, it is not difficult to auto-scale based on usage. However, the WebSocket servers are stateful, so care must be taken when removing existing nodes. Before a node can be removed, all existing connections should be allowed to drain. To achieve that, we can mark a node as “draining” at the load balancer so that no new WebSocket connections will be routed to the draining server. Once all the existing connections are closed (or after a reasonably long wait), the server is then removed.
         
        Releasing a new version of the application software on a WebSocket server requires the same level of care.
        It is worth noting that effective auto-scaling of stateful servers is the job of a good load balancer. Most cloud load balancers handle this job very well.
        So Suggest to use least-connection load balancer so that it can route new request to Web socket servers with least connections

    SSE (Server side events) - used for server -> client connection 
      https://stackoverflow.com/questions/5195452/websockets-vs-server-sent-events-eventsource 
      - single direction from server -> client 
      Ideal use cases of SSE:
        Stock ticker streaming
        twitter feed updating
        Notifications to browser
      SSE gotchas:
        No binary support
        Maximum open connections limit
      Advantages of Websockets over SSE:
          Real time, two directional communication.
          Native support in more browsers

    IOT devices
      https://www.youtube.com/watch?v=6mBO2vqLv38
      - all devices are connected to IOT gateway through MQTT or HTTP
      IOT device architecture
        https://www.youtube.com/watch?v=KeaeuUcw02Q&t=630s
        - IoT devices (sensors)
        - IoT Gateway (collect data from sensors)
        - Cloud (Processing engine or event processing layer)
        - Application layer or API management layer 
        entire layers above is secured by device manager and IAM
      IoT Reference architecture
        https://www.youtube.com/watch?v=KeaeuUcw02Q&t=630s
        - Device layer
          like sensors which are interconnected 
          eg., bluetooth (via mobile phone -> Wifi gateway), raspberry pi (via Wifi -> direct connect to ethernet), zigbee (via zigbee gateway)
        - Communication layer or Gateway layer
          Rest protocol - HTTP or MQTT
        - Bus layer or Aggregation layer
          acts as a message broker supports http server or MQTT broker or Gateway
        - Even processing and Analysics layer (cloud)
      How IoT devices connect to the internet
        https://theiotpad.com/different-ways-to-connect-iot-device-over-internet/
        Communication devices include Wi-Fi, Bluetooth, and Ethernet cables.
        Although Wi-Fis use more power than your average Bluetooth system, they are more reliable and scalable. You can receive signals to your mobile devices and relay signals that penetrate barriers like walls and objects.

      Practical 
        https://www.youtube.com/watch?v=QyIeFy2-MvU
        light bulb having an IP address connected via a computer application using node.js



 
  Use Nginx 
    - Has triats of load balancing + reverse proxy + API gateway
         - supports load balancing
         - supports request forwarding based on different paths like api gateway
         - supports reverse proxy 
         - supports rate limiting
         - supports caching
         - supports request grouping
         - supports secure auth SSL. Also does optimizations with keep alive connection and cache to avoid SSL handshake - https://docs.nginx.com/nginx/admin-guide/security-controls/terminating-ssl-http/ 
         Also a good read https://www.nginx.com/blog/http-keepalives-and-web-performance/
    - Nginx supports L7 (application level load balancer) and also L4 ? 
    - For modern websites, Nginx in form of reverse proxy is set up at multiple layers 
        - first layer : edge server 
        - second layer : API gateway / load balancer
        However, many of them combine both as one ingress service
    Nginx vs HaProxy 
      https://blog.gitnux.com/comparison/haproxy-vs-nginx/#:~:text=Haproxy%20and%20Nginx%20are%20both,web%20server%20depending%20on%20configuration.
      - The main difference between the two is that Haproxy is a proxy server specifically designed to handle high levels of traffic while Nginx can be used as either a proxy or web server depending on configuration. 
      https://cloudinfrastructureservices.co.uk/haproxy-vs-nginx-whats-the-difference/ 
      - At the time of HAProxy’s implementation, the processes do not share any memory that affects the configuration parameters and session persistence is not possible.
      - At the time of NGINX implementation, the processes do share memory.
      - Nginx is used by Netflix, Atlassian, Dropbox, etc.
    Cons with Nginx
      - NGINX does not support exportable metrics (paid edition only)
      - NGINX supports gRPC, WebSocket, and but only support protocols like HTTP, HTTPS, and Email.


  Use gRPC as interservice communication between microservices in your cloud
  https://www.youtube.com/watch?v=gnchfOojMk4
    - its high performant because 
          -it uses protocal buffers as efficiency binary encoding format as compared to json and 
          - http 2.0
    - supports many programming languages
    So when to use REST vs gRPC ? 
      Here are use cases for a REST API:
        Web-based architectures
        Public-facing APIs for ease of understanding by external users
        Simple data communications
      A gRPC API is better for these use cases:
        High-performance systems
        High data loads
        Real-time or streaming applications
        gRPC is NOT recommended for browser -> server communication due to limited support
      

  Multipart upload when upload file or video
    https://bytebytego.com/courses/system-design-interview/s3-like-object-storage 
    It is possible to upload such a large object file directly, but it could take a long time. If the network connection fails in the middle of the upload, we have to start over. A better solution is to slice a large object into smaller parts and upload them independently. After all the parts are uploaded, the object store re-assembles the object from the parts. This process is called multipart upload.

    - S3 client supports multipart upload (https://www.baeldung.com/aws-s3-multipart-upload)
    - S3 also supports faster multipart upload through edge locations called "transfer acceleration" https://aws.amazon.com/blogs/compute/uploading-large-objects-to-amazon-s3-using-multipart-upload-and-transfer-acceleration/  
    - HTML support "multipart/form-data" encoding type which is used for forms that include binary data, such as an image or audio files. When a user submits a form with “multipart/form-data” encoding, the data is split into multiple parts and sent to the server in a way that preserves the binary data. The data is then reassembled by the server and processed accordingly.
    - you can parallelize video upload with GOP alignment (https://bytebytego.com/courses/system-design-interview/design-youtube)
    - For email attachment
      An email attachment is sent along with an email message, commonly with Base64 encoding [6]. There is usually a size limit for an email attachment. For example, Outlook and Gmail limit the size of attachments to 20MB and 25MB respectively as of June 2021. This number is highly configurable and varies from individual to corporate accounts. 
      Multipurpose Internet Mail Extension (MIME) [7] is a specification that allows the attachment to be sent over the internet. https://bytebytego.com/courses/system-design-interview/distributed-email-service

  Use GPU for video transcoding 
    Meta uses GPU as ASICs for AI Inference or video transcoding

  Use Snapshot Isolation to handle dirty read(even with read commited ), phantom read (non repeatable) and is high performant as compared to serializable (pessimistic locking)
    How snapshot actually works : https://www.youtube.com/watch?v=Tgpa9TrxsfU 
      it just stores all versions of modified rows in WAL and reads from a committed transaction in WAL. So you do not have to worry about someone modify/add the data when you are read (dirty or phantom read) because at transaction x, all rows were in consistent state.
    https://www.geeksforgeeks.org/what-is-snapshot-isolation/
    Dirty read, phantom read
    https://jennyttt.medium.com/dirty-read-non-repeatable-read-and-phantom-read-bd75dd69d03a
    With nice demo - https://techcommunity.microsoft.com/t5/sql-server-blog/serializable-vs-snapshot-isolation-level/ba-p/383281  
    Cockraoch DB uses serializable snapshot isolation by default - https://github.com/cockroachdb/cockroach/blob/master/docs/design.md 
    
    Issue with Snapshot Isolation : 
     "write skew anamoly" with this approach https://www.youtube.com/watch?v=eym48yrObhY 
      happens when write in one row affects the contraints of other rows 
      - example, unique username, atleast one doctor on call, booking meeting rooms based on time 
      


  Use CQRS pattern to improve performance
    https://medium.com/design-microservices-architecture-with-patterns/cqrs-design-pattern-in-microservices-architectures-5d41e359768c# 
    CQRS stands for Command and Query Responsibility Segregation, a pattern that separates read and update operations for a data store. Implementing CQRS in your application can maximize its performance, scalability, and security.

    CQRS could be combined with event sourcing since it will generate sync events from write DBs to read DBs. But most DBs put WAL only for 30 days by defaut. So its limited time event sourcing.

    Example CQRS application is digital wallet system design where you need to separate our read and write part - https://bytebytego.com/courses/system-design-interview/digital-wallet

    Example, nice examples of event sourcing design - Kafka 
      - https://www.youtube.com/watch?v=lg6aF5PP4Tc
          Event sourcing is an alternative means of storing data as log of events rather than table that is updated,read and deleted. In event sourcing, you create a table and append events to it as they occur building up an event log. So reading data has an additional step because you need to pull up all the events and reduce it to derive into table format. Doing this process in real-time for large number of events is complicated and thus needs pre-processing of table format data from event log. This technique is an example of CQRS where you write in event log and store it immediately in read-only table or materialized views for reading.  
          Confluence using Kafka as event log for writes and generates materialized views for read 

  Read after write consistency 
    https://avikdas.com/2020/04/13/scalability-concepts-read-after-write-consistency.html 
      Read-after-write consistency is the ability to view changes (read data) right after making those changes (write data). For example, if you have a user profile and you change your bio on the profile, you should see the updated bio if you refresh the page. There should be no delay during which the old bio shows up.

      If there is a delay, this is known as a read-after-write inconsistency.

        It’s important to note the consistency only applies to the one performing the write. For example, you may update your profile, but someone else may not see the update for another minute. For most large-scale systems, some amount of delay like this is inevitable, but it’s really important the original writer see their update immediately.
      
      So then what are some solutions ? Pg 163 DDIA
       - always make read from master if its user's own info 
       - but what if there are many editable fields - maintain the mapping of the info most recently modified and based on that read from leader
       - have client store timestamp of most recent write

  Monotonic read 
      Pg 164 DDIA 
    means user is read the data moving backwards in time. Its possible if one read goes to updated replica and another to outdated replica.
    This is the reason why always have consistent hashing selection of server based on user id 

  Use Parquet column over row format or csv file 
    https://www.upsolver.com/blog/apache-parquet-why-use
    https://towardsdatascience.com/demystifying-the-parquet-file-format-13adb0206705
      Have you ever used pd.read_csv() in pandas? Well, that command could have run ~50x faster if you had used parquet  instead of CSV.
      Traditionally there are three main layouts that convert our 2 dimensional table down to 1:
        Row-based: sequentially store rows (CSV).
        Column-based: sequentially store columns (ORC).
        Hybrid-base: sequentially store chunks of columns (Parquet).
      hybrid layouts are really effective for OLAP workflows because they support both projection(select) and predicates(where).
      Before moving on, it’s important to note that parquet is often described as a columnar format. However, due to the fact that it stores chunks of columns, as shown in the bottom of figure 2, a hybrid storage layout is a more precise description.
      Parquet leverages metadata to skip parts of our data that can be excluded according to our predicate. 
      Parquet intelligently solves this by storing max and min values for each row group, allowing us to skip entire row groups.
      Additionally it supports compression techniques like RLE, Dictionary and Bit encoding

  

  Use Circuit breaker pattern to support Fault tolerant/resilient services (like services doing retry for failed operations)
    https://learn.microsoft.com/en-us/azure/architecture/patterns/circuit-breaker
    The Circuit Breaker pattern, popularized by Michael Nygard in his book, Release It!, can prevent an application from repeatedly trying to execute an operation that's likely to fail. Allowing it to continue without waiting for the fault to be fixed or wasting CPU cycles while it determines that the fault is long lasting.
      The purpose of the Circuit Breaker pattern is different than the Retry pattern. The Retry pattern enables an application to retry an operation in the expectation that it'll succeed. The Circuit Breaker pattern prevents an application from performing an operation that is likely to fail. An application can combine these two patterns by using the Retry pattern to invoke an operation through a circuit breaker. However, the retry logic should be sensitive to any exceptions returned by the circuit breaker and abandon retry attempts if the circuit breaker indicates that a fault is not transient.

     

  Choosing the right Batch size - https://bytebytego.com/courses/system-design-interview/distributed-message-queue 
    The choice of the batch size is a classic tradeoff between throughput and latency (Figure 13). With a large batch size, the throughput increases but latency is higher, due to a longer wait time to accumulate the batch. With a small batch size, requests are sent sooner so the latency is lower, but throughput suffers. Producers can tune the batch size based on use cases.

  Choosing pull over push model for Queue. - https://bytebytego.com/courses/system-design-interview/distributed-message-queue 
    Pros:

      Consumers control the consumption rate. We can have one set of consumers process messages in real-time and another set of consumers process messages in batch mode.

      If the rate of consumption falls below the rate of production, we can scale out the consumers, or simply catch up when it can.

      The pull model is more suitable for batch processing. In the push model, the broker has no knowledge of whether consumers will be able to process messages immediately. If the broker sends one message at a time to the consumer and the consumer is backed up, new messages will end up waiting in the buffer. A pull model pulls all available messages after the consumer’s current position in the log (or up to the configurable max size). It is suitable for aggressive batching of data.

    Cons:
        When there is no message in the broker, a consumer might still keep pulling data, wasting resources. To overcome this issue, many message queues support long polling mode, which allows pulls to wait a specified amount of time for new messages [6].

  Transactional database like SQL
    Pros :
    1. guarantees ACID (ddia pg. 224)
        Atomicity : (overloaded and different from atomic operations in multithreaded programming), here it just means abortability if anything fails
        Consistency : (overloaded due to replica consistency, consistent hashing and in CAP theorem), here it just means invariants in your database must hold true. Example, credits and debits must be balanced. Your application should control this but at times even database can help by integrity constraints (db constraints in hotel reservation system - https://bytebytego.com/courses/system-design-interview/hotel-reservation-system). The letter C does not belong in ACID since its upto the application to handle C
        Isolation : means that concurrently executing transaction are isolated from each other. In textbooks, isolation is formalized as "serializability" which ensures that when all transactions have committed, the result is same as if they all ran serially(one after another) even though they may have run concurrently causing race condition. Oracle 11g offers weak isolation level called Snapshot Isolation
        Durability : committed transactions are never lost despite of failures like disk, network, outage,etc. Solved using WAL. 

        Isolation level pg 234 DDIA
        1. Read committed : can solve the problem of dirty reads and dirty writes. It can be implemented with row level locking.
        locking is fine for writes but slows down read. So to avoid locking on read operations, we need to allow reading the last committed read.It ensures two things - 1. No dirty reads and 2. No dirty writes. This is also a default isolation level for MySQL and PostgreSQL.
        2. Snapshot isolation / Reapeatable Read: 
          With last committed read, there is still an issue of databasse concurrency issue. If user A has 500$ in account 1 and 2 and transfer 100$ from account 1 to account 2, balance is first updated and committed for account 1 and then on account 2. If before account 2 committed, values are read, then from it may appear that the user have only 900$ (account 1 is now 400$ but account 2 is still 500$).
          This isolation solves this problem by taking the snapshot at particular version (MVCC say using timestamp) and allows reading only from that version. This is used in Oracle 11g and Google Cloud spanner.


          Issue with snapshot isolation and also read committed isolation : The read committed and snapshot isolation levels we’ve discussed so far have been primarily about the guarantees of what a read-only transaction can see in the pres‐ ence of concurrent writes. We have mostly ignored the issue of two transactions writing concurrently. The best known of these is the lost update problem, illustrated in Figure 7-1 with the example of two concurrent counter increments.

            Concurrent write can cause lost update problem - how to solve it ? 
              - using atomic writes (locks on DB)
              - using application locks
              - using compare-and-set method 

            But what if its a write skew or phantom (where a write in one transaction changes the result of a search query in another transaction, is called a phantom)
              - use "materialize conflicts" technique by creating rows. Example, creating meeting room time slots records to ensure no two rooms are in one time slot. But this will now work in cases like unique username, you cannot create records of all possible usernames.
              - Use serializable isolation
              - To improve performance, you can use Snapshot Serializable solution

            https://efficientcodeblog.wordpress.com/2017/12/25/leaderless-replication-dynamo-style-quorum-consensus-eventual-consistency-high-availability-and-low-latency/ 
            But what if its distributed -  multileader or leaderless replication ? 
              - need versioning to resolve conflicts  at DB level or application 
              - some ways to resolve conflicts at DB level
                - use read-repair process (resolve conflicts when you read) - DynamoDB
                - use anti-entropy process (resolve conflicts in background you read) - DynamoDB
                - use LWW - Cassandra
            But how to ensure you are consistent values from different DB nodes ? 
              - Quorum consistency - achieving concensus with R + W > N. Usually N is picked as odd number in DynamoDB
                    w = r = (n + 1) / 2
                - Limitations of Quorum consistency 
                    Higher Latency – Lower Availability
                    Quorum Consistency can sometimes lead to higher latency and lower availability if proper caution is not taken and there is a network interruption and many replicas become unreachable. This is because, if the number of reachable replicas falls below w or r, the database would become unavailable for writing or reading.
              - Eventual Consistency
                From the discussion so far we see that, although quorums appear to guarantee that a read returns the latest written value, in practice it is not so simple. Dynamo-style databases are generally optimized for eventual consistency. The parameters w and r allow us to adjust the probability of stale values being read, but it’s wise to not take them as absolute guarantees.
                Application - Leaderless replication which can tolerate stale reads and has high availability and low latency
              - Sloppy quorums and hinted handoff 
                  https://github.com/keyvanakbary/learning-notes/blob/master/books/designing-data-intensive-applications.md#sloppy-quorums-and-hinted-handoff
                    Sloppy quorums are useful for increasing write availability: as long as any w nodes are available, the database can accept writes. This also means that you cannot be sure to read the latest value for a key, because it may have been temporarily written to some nodes outside of n.
              - Multi-datacenter operation
                Each write from a client is sent to all replicas, regardless of datacenter, but the client usually only waits for acknowledgement from a quorum of nodes within its local datacenter so that it is unaffected by delays and interruptions on cross-datacenter link. 

            Lets take a look in details : 

            Preventing Lost updates
                We have mostly ignored the issue of two transactions writing concurrently
                1. Atomic writes
                    For example, the following instruction is concurrency-safe in most relational databases:
                      UPDATE counters SET value = value + 1 WHERE key = 'foo';
                    Atomic operations are usually implemented by taking an exclusive lock on the object when it is read so that no other transaction can read it until the update has been applied. This technique is sometimes known as cursor stability.
                2. Explicit locking
                    Another option for preventing lost updates, if the database’s built-in atomic opera‐tions don’t provide the necessary functionality, is for the application to explicitly lock objects that are going to be updated.
                      BEGIN TRANSACTION;
                        SELECT * FROM figures
                        WHERE name = 'robot' AND game_id = 222
                        FOR UPDATE;
                        -- Application logic code starts here
                          -- Check whether move is valid, then update the position
                          -- of the piece that was returned by the previous SELECT.
                        -- Application logic code ends here
                        UPDATE figures SET position = 'c4' WHERE id = 1234;
                      COMMIT;
                      The FOR UPDATE clause indicates that the database should take a lock on all rows returned by this query.
                        This works, but to get it right, you need to carefully think about your application logic. It’s easy to forget to add a necessary lock somewhere in the code, and thus introduce a race condition.
            Automatically detecting lost updates
              1. Compare-and-set
                  If the content has changed and no longer matches 'old content', this update will have no effect, so you need to check whether the update took effect and retry if necessary
                    UPDATE wiki_pages SET content = 'new content'
                    WHERE id = 1234 AND content = 'old content';
              2. Conflict resolution and replication
                  Locks and compare-and-set operations assume that there is a single up-to-date copy of the data. However, databases with multi-leader or leaderless replication usually allow several writes to happen concurrently and replicate them asynchronously, so they cannot guarantee that there is a single up-to-date copy of the data. 
                  Instead, as discussed in “Detecting Concurrent Writes” on page 184, a common approach in such replicated databases is to allow concurrent writes to create several conflicting versions of a value (also known as siblings), and to use application code or special data structures to resolve and merge these versions after the fact. On the other hand, the last write wins (LWW) conflict resolution method is prone to lost updates, as discussed in “Last write wins (discarding concurrent writes)” on page 186. Unfortunately, LWW is the default in many replicated databases. 
            Write Skew and Phantoms
              In the previous sections we saw dirty writes and lost updates, two kinds of race conditions that can occur when different transactions concurrently try to write to the same objects. 
              But what if the objects are different ? The explicit locking query won't work in following cases because the returned rows are different
                1. there are at least two doctors on call,
                2. there are no existing bookings for that room at that time, 
                3. the position on the board doesn’t already have another figure on it, 
                4. the username isn’t already taken, there is still money in the account
              This effect, where a write in one transaction changes the result of a search query in another transaction, is called a phantom [3]. Snapshot isolation avoids phantoms in read-only queries, but in read-write transactions like the examples we discussed, phantoms can lead to particularly tricky cases of write skew. 
              Materializing conflicts : please consider this as last resort though due to complexities
                For example, in the meeting room booking case you could imagine creating a table of time slots and rooms. Each row in this table corresponds to a particular room for a particular time period (say, 15 minutes). You create rows for all possible combinations of rooms and time periods ahead of time, e.g. for the next six months.
                Now a transaction that wants to create a booking can lock (SELECT FOR UPDATE) the rows in the table that correspond to the desired room and time period. After it has acquired the locks, it can check for overlapping bookings and insert a new booking as before.
                This approach is called materializing conflicts, because it takes a phantom and turns it into a lock conflict on a concrete set of rows that exist in the database 
        3. Serializability  
            Serializable isolation is usually regarded as the strongest isolation level. It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at a time, serially, without any concurrency.
            1. Actual Serial Execution
              The simplest way of avoiding concurrency problems is to remove the concurrency entirely: to execute only one transaction at a time, in serial order, on a single thread.
              Two developments caused this rethink: 
                a. RAM became cheap enough that for many use cases is now feasible to keep the entire active dataset in memory (see “Keeping everything in memory” on page 88). When all data that a transaction needs to access is in memory, transactions can execute much faster than if they have to wait for data to be loaded from disk.
                b. Database designers realized that OLTP transactions are usually short and only make a small number of reads and writes (see “Transaction Processing or Analytics?” on page 90). By contrast, long-running analytic queries are typically readonly, so they can be run on a consistent snapshot (using snapshot isolation) outside of the serial execution loop.
              Encapsulating transactions in stored procedures
                systems with single-threaded serial transaction processing don’t allow interactive multi-statement transactions. Instead, the application must submit the entire transaction code to the database ahead of time, as a stored procedure.
                Stored procedures have existed for some time in relational databases, They have gained a somewhat bad reputation, for various reasons:
                  Each database vendor has its own language for stored procedures. These languages haven’t kept up with developments in general-purpose programming languages, so they look quite ugly.
                  A database is often much more performance-sensitive than an application server, because a single database instance is often shared by many application servers. A badly written stored procedure (e.g., using a lot of memory or CPU time) in a database can cause much more trouble than equivalent badly written code in an application server
                Partitioning
                  Executing all transactions serially makes concurrency control much simpler, but lim‐its the transaction throughput of the database to the speed of a single CPU core on a single machine. Read-only transactions may execute elsewhere, using snapshot isola‐tion, but for applications with high write throughput, the single-threaded transaction processor can become a serious bottleneck. If you can find a way of partitioning your dataset so that each transaction only needs to read and write data within a single partition, then each partition can have its own transaction processing thread running independently from the others. Since cross-partition transactions have additional coordination overhead, they are vastly slower than single-partition transactions. VoltDB reports a throughput of about 1,000 cross-partition writes per second, which is orders of magnitude below its single-partition throughput and cannot be increased by adding more machines 
            2. Two-Phase Locking (2PL)
                    - If transaction A has read an object and transaction B wants to write to that object, B must wait until A commits or aborts before it can continue. (This ensures that B can’t change the object unexpectedly behind A’s back.)
                    - If transaction A has written an object and transaction B wants to read that object, B must wait until A commits or aborts before it can continue. (Reading an old version of the object, like in Figure 7-1, is not acceptable under 2PL.)
                  After a transaction has acquired the lock, it must continue to hold the lock untilthe end of the transaction (commit or abort). This is where the name “twophase” comes from: the first phase (while the transaction is executing) is when the locks are acquired, and the second phase (at the end of the transaction) is when all the locks are released.
            3. Serializable Snapshot Isolation (SSI)  
                an algorithm called serializable snapshot isolation (SSI) is very promis‐ing. It provides full serializability, but has only a small performance penalty com‐pared to snapshot isolation. Today SSI is used both in single-node databases (the serializable isolation level in PostgreSQL since version 9.1 [41]) and distributed databases (FoundationDB, CockroachDB uses a similar algorithm).
                Basically you read using Snapshot Isolation but while writing, the database detects its a stale snapshot(new values are updated) and instructs abort/retry operations
                How does the database know if a query result might have changed? There are two cases to consider:
                  • Detecting reads of a stale MVCC object version (uncommitted write occurred before the read)
                  • Detecting writes that affect prior reads (the write occurs after the read)
              Performance of serializable snapshot isolation
                Compared to two-phase locking, the big advantage of serializable snapshot isolation is that one transaction doesn’t need to block waiting for locks held by another trans‐action. Like under snapshot isolation, writers don’t block readers, and vice versa. 
                Compared to serial execution, serializable snapshot isolation is not limited to the throughput of a single CPU core.
                The rate of aborts significantly affects the overall performance of SSI. For example, a transaction that reads and writes data over a long period of time is likely to run into conflicts and abort, so SSI requires that read-write transactions be fairly short

    Consistency and Concensus with distributed databases  Pg 321

    What is Linearizability vs Serializability
        - Linearizability is the recency guarantee while Serializability is the ordering guarantee
         

  How to design ranking system based on multile criteria (used in multiple designs like google ranking, FB ranking)
    https://towardsdatascience.com/ranking-algorithms-know-your-multi-criteria-decision-solving-techniques-20949198f23e
      Consider one attribute at a time and try to maximize or minimize it (as per the requirement) to generate optimized score.
      Introduce weights to each attributes to get optimized weighted scores.
      Combine the weighted scores (of each attribute) to create a final score for an entity (here car).
      reverse - just use weight * (1 - column %)

  Use Virtual Cluster map for placement service (placement service will help to indentify where to place the job or upload the file)
    https://bytebytego.com/courses/system-design-interview/s3-like-object-storage
    The placement service determines which data nodes (primary and replicas) should be chosen to store an object. It maintains a virtual cluster map, which provides the physical topology of the cluster. The virtual cluster map contains location information for each data node which the placement service uses to make sure the replicas are physically separated. This separation is key to high durability. See the “Durability” section below for details.


  Perform rate limiting whenever possible to absorb spikes (eg., NewsFeed service)
    https://bytebytego.com/courses/system-design-interview/design-a-news-feed-system 

  Saving storage space
    https://bytebytego.com/courses/system-design-interview/design-google-drive 
    Moving infrequently used data to cold storage. Cold data is the data that has not been active for months or years. Cold storage like Amazon S3 glacier [11] is much cheaper than S3.

  Try to keep UI and back-end as separate services
    https://michaelwashburnjr.com/blog/4-reasons-web-app-separated-frontend-backend
      Modularity, Re-usability, Content Delivery, Responsiveness(?) and Versioning

  How VPN works ?

  CORS - Cross origin resource sharing 
    https://www.youtube.com/watch?v=4KHiSt0oLJ0&ab_channel=Fireship
    by default, browser does not allow requests to another domain on your website. To allow this, you need to send "Access-Control-Allow-Origin" header in the response.

=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================


Distributed systems today and Benchmarking on common systems

    Live streaming
      https://www.youtube.com/watch?v=7AMRfNKwuYo
      - streamer device(encoder) --> PoPs --> transcoding(different resolutions and bitrate) --> packaging (hls, dash format) --> PoPs --> receiving device (decoder)
            1. streamer starts his stream wired up to an encoder (software in browser, webcam, mobile ) which packages the video stream and send it in a transport protocol that the live streaming platform can recieve for further processing.
            2. Transcoding : convert video into different resolutions, frame rate and bit rates which is called "adaptive bit rate streaming". 
              a. Segmentation : transcoded stream is divided into smaller video of 1s called segments
              b. Conversion : Then it converts video into different resolution and bit rates
               Both steps requires massive computation and done in parallel which needs lots of CPU. That's why DAG is used here.
            3. Then Transcoded segments are packaged to support most common live streaming format - HLS, DASH (.mp4). 
                Both of these format consist of the manifest file  and the series of video chunks where each chunk contains video of few seconds. 
                This manifest file is like a directory to tell the client video player what all output formats are, where to load the video chunks over HTTP.
                Both manisfest file and video chunks are usually cached in CDN for faster streaming.

                Just right click on any youtube video and view network call, it requests manifest file periodically ( 5 s)

          Open questions 
            - Why not UDP at streamers device
            - Why PoP
            - If CDN is used at receivers device, how updates on manifest file and video is published
              1. use TTL. when cache reaches TTL 
              2. OR use push updates to the cache
            - How to avoid thundering herd ? 
              - USE PROMISE CACHE. 
                  - basically if say 1 M clients requests for same item and there is cache miss. So instead of forwarding all requests to DB, only one request is made to DB to populate the cache. Rest all the requests are given promises.

            Questions are answered below
      - Live Streaming system design in detail 
          https://www.youtube.com/watch?v=S8aSoSQJ4G8&t=1079s&ab_channel=System-DesignByVinayakSangar
            - Avoid UDP at source : UDS is fast but lossy which means it can loose resolution at source itself. So use TCP at source and protocol used is RTMP which is based on TCP.  

      * Video transcoding is also called video encoding
        Many types of encoding formats are available; however, most of them contain two parts:
          Container: This is like a basket that contains the video file, audio, and metadata. You can tell the container format by the file extension, such as .avi, .mov, or .mp4.

          Codecs: These are compression and decompression algorithms aim to reduce the video size while preserving the video quality. The most used video codecs are H.264, VP9, and HEVC.

      Why RTMP for live streamer and HLS/DASH for receiver/consumer
        https://linuxhit.com/rtmp-vs-hls-vs-dash-streaming-protocols/#0-rtmp-vs-hls-vs-dash-streaming-protocols 
        RTMP vs HLS vs DASH – Which one should I use?
          If you are producing live video content then you probably want to publish your stream to a streaming server via RTMP. It is possible to allow consumption of the stream via RTMP. But you probably don’t want to do that. HLS has widespread support on client devices that consume your video. Additionally you can leverage CDNs and not need to worry about firewalls. So you will want to broadcast with HLS.

      - Protocols
        - the most popular transport protocol is RTMP which is TCP based but moving towards SRT which is UDP based
        - remember hls, dash formats are different from streaming protocol - hls, dash.
        - RTMP is a TCP-based protocol that is widely used because it offers persistent connections and low-latency streaming. On the other hand, HLS is an HTTP-based protocol for adaptive bitrate streaming of live and on-demand content. It is often better than RTMP because it has a lower latency. https://castr.com/blog/rtmp-vs-hls/ 

        Why not UDP for live streamer? 
          - you will loose packets on source itself

        


        Resolution vs FPS vs bitrate
          https://www.youtube.com/watch?v=_XzGhc9mPVk&ab_channel=JamesArcher
          Resolution - no of pixels both lengthwise and breadthwise 
            - 4K Ultra HD : 3840 X 2160 px
            - 2K HD : 2800 X 1600 px
            - Full HD : 1920 X 1080 px 
            - HD : 1280 X 720 px

            For each pixel, it has color depth which is usually 8-bit. More the color depth better is the video. 

          Frames rate - no of frames per second 
            - 24 fps : cinematic like Netflix 
            - 60 fps : gaming which gives immersive experience. Avoid more fps in movie though 

          Bit rate - amount of data per second ( Resolution * color depth * frame rate )
            - Compression can help condense the data 
            - So higher the bit rate, better the quality 
            - So 4k is 35-45 Mbps and 2K is 16 Mbps

      

    Video conferencing app like Zoom
      https://medium.com/@himanishaik48/zoom-system-design-most-frequently-asked-question-in-interview-f60f6fe8d198 
 
    Multiplayer Gaming distributed app
      https://theredrad.medium.com/designing-a-distributed-system-for-an-online-multiplayer-game-requirements-part-2-de1d1ae9ae9b
      - use of kubernetes
      - use of TCP tunnels
      How unity company multiplayer game architecture - https://www.youtube.com/watch?v=77vYKsXC4IE 
        - Amazon GameLift has dedicated server for hosting games
        - GCP also has game server built on top of kubernetes 
        - use auth provider like playfab
        - use matching server like playfab matching or flexmatch from amazon gamelift
        - leaderboard, 
        - persistent data like sql
        - use GameAnalytics

    Design Live commenting system 
      https://leetcode.com/discuss/interview-question/583184/FBInstagram-'Live-Comments'-System-design/918003 
      - Same as proxity service in byte byte go
        - client <-> server websocket connection
        - redis pub sub 
        - DB for persistence 
      - My try : https://leetcode.com/discuss/interview-question/583184/FBInstagram-'Live-Comments'-System-design/918003 
      - Facebook's  --> Write locally, Read globally technique

    Design search engine
      https://medium.com/double-pointer/system-design-interview-search-engine-edb66b64fd5e 
        Web -> Crawler -> Indexer -> Retiever <-> Web Browser or Mobile

        Crawler : crawls from robots.txt, uses some seed URLs, uses priority Q, adds links in priority Q to crawl next
        Indexer : pre-process (remove stop words), indexes usually inverted index as word -> doc ids
        Retreiver : 
          Since search query have multiple words, two types : 
            Conjunctive Search which is AND based 
            Disjunctive Search which is OR based

    Types of database indexes
      https://medium.com/must-know-computer-science/system-design-indexes-f6ad3de9925d 


    Layer 4 Load balancer
      https://www.nginx.com/resources/glossary/layer-4-load-balancing/
      When the Layer 4 load balancer receives a request and makes the load balancing decision, it also performs Network Address Translation (NAT) on the request packet, changing the recorded destination IP address from its own to that of the content server it has chosen on the internal network. Similarly, before forwarding server responses to clients, the load balancer changes the source address recorded in the packet header from the server’s IP address to its own. 
      Layer 4 load balancers make their routing decisions based on address information extracted from the first few packets in the TCP stream, and do not inspect packet content.

    Layer 7 Load balancer
      https://www.nginx.com/resources/glossary/layer-4-load-balancing/ 
      Layer 7 load balancers base their routing decisions on various characteristics of the HTTP header and on the actual contents of the message, such as the URL, the type of data (text, video, graphics), or information in a cookie.
      Taking into consideration so many more aspects of the information being transferred can make Layer 7 load balancing more expensive than Layer 4 in terms of time and required computing power, but it can nevertheless lead to greater overall efficiency. For instance, because a Layer 7 load balancer can determine what type of data (video, text, and so on) a client is requesting, you don’t have to duplicate the same data on all of the load-balanced servers.

    
    Map Reduce
      https://www.youtube.com/watch?v=MAJ0aW5g17c (awesome)
        Example of GFS performing map function on worker servers and reduce using worker servers co-ordinated by master
      The MapReduce framework is a good option to aggregate ad click events. The directed acyclic graph (DAG) is a good model for it [9]. The key to the DAG model is to break down the system into small computing units, like the Map/Aggregate/Reduce nodes (https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation). Facebook uses DAG model for its video streaming
      Hotspot issue in map reduce
        supported by resource manager (https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation)

    Zookeeper 
    https://www.youtube.com/watch?v=gZj16chk0Ss&t=413s&ab_channel=ddddd
    - What it does  ?
      - which node is the master ? (leader election)
      - what tasks are assigned to which workers ? (distributed store)
      - which workers are available currently ? (service discovery)

    - How does it work ? 
      - it is push-based so that workers gets notified of any changes

      https://bytebytego.com/courses/system-design-interview/distributed-message-queue
       If you are not familiar with it, Zookeeper is an essential service for distributed systems offering a hierarchical key-value store. It is commonly used to provide a distributed configuration service, synchronization service, and naming registry [2]. 
     - centralized configuration, key value store for state and also co-ordination service like selecting leader/master
     - can perform as a shard manager to find out the shard based on key ( https://kousiknath.medium.com/all-things-sharding-techniques-and-real-life-examples-in-nosql-data-storage-systems-3e8beb98830a#:~:text=You%20can%20use%20some%20configuration,directly%20talk%20to%20the%20shard. )
     - The number of partitions and addresses of all Redis nodes can be stored in a centralized place. We could use Zookeeper [4] as a highly-available configuration storage solution. (https://bytebytego.com/courses/system-design-interview/digital-wallet)
     - Apache Zookeeper [7] is a popular open-source solution for service discovery. It registers all the available chat servers and picks the best chat server for a client based on predefined criteria. (https://bytebytego.com/courses/system-design-interview/design-a-chat-system). You can also use this strategy for game server. 
     - There are many service discovery packages available, with etcd [4] and Zookeeper [5] among the most popular ones. Our need for the service discovery component is very basic. https://bytebytego.com/courses/system-design-interview/nearby-friends 
        - Under the “Key” mentioned in point 1, we store a hash ring of all the active Redis pub/sub servers in the service discovery component (See the consistent hashing chapter in Volume 1 of the System Design Interview book or [6] on details of a hash ring). The hash ring is used by the publishers and subscribers of the Redis pub/sub servers to determine the pub/sub server to talk to for each channel. 
      https://ramcloud.atlassian.net/wiki/spaces/RAM/pages/6848719/ZooKeeper+Performance 
    - Interesting fact about zookeeper 
      https://stackoverflow.com/questions/37293928/zookeeper-vs-in-memory-data-grid-vs-redis 
    - Zookeeper avg req latency ~ 10 ms 
        - 2-core 3 node cluster
        https://ramcloud.atlassian.net/wiki/spaces/RAM/pages/6848719/ZooKeeper+Performance
    

    Kafka
      https://www.youtube.com/watch?v=UNUz1-msbOM
      latency : 1 ms
      Why fast :
      - sequential access from the disk with append-omly log 
      - zero copy principle ( copy path from disk -> os buffer -> application buffer -> socket buffer -> nic buffer TO  disk -> os buffer  -> socket buffer )
      How to guarantee "exactly-once" processing ? 
        1. https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/
            From producer to broker
              Through idempotence of message through sequnce number. So even if the producer sends twice, broker can dedup.
            From broker to consumer
              Through Atomic transaction - This feature also allows you to commit your consumer offsets in the same transaction along with the data you have processed, thereby allowing end-to-end exactly-once semantics. It can be enabled by selected isolation level = read committed.

      How Kafka stores offset by consumer group for each partition - https://bytebytego.com/courses/system-design-interview/distributed-message-queue
        In message queus, The data access patterns for consumer states (last consumed offset) are:
            Frequent read and write operations but the volume is not high.
            Data is updated frequently and is rarely deleted.
            Random read and write operations.
            Data consistency is important.

          Lots of storage solutions can be used for storing the consumer state data. Considering the data consistency and fast read/write requirements, a KV store like Zookeeper is a great choice. Kafka has moved the offset storage from Zookeeper to Kafka brokers. 
            

      Excatly once delivery is hard but Yelp implements it. Check it out https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation 
        To achieve “exactly-once” processing, we need to put operations between step 4 to step 6 in one distributed transaction. A distributed transaction is a transaction that works across several nodes. If any of the operations fails, the whole transaction is rolled back.
      Benchmarking
        https://developer.confluent.io/learn/kafka-performance/
        5 ms
        600 MB/s on 25 Gbps network on AWS 
        1 million messages per second
        i3en.2xlarge instance type (with 8 vCores, 64 GB RAM, 2 x 2,500 GB NVMe SSDs)

      When to use Kafka ? 
        https://www.engati.com/blog/redis-kafka-rabbitmq
        Use it when you need pull based architecture, lightening fast queuing, streaming and need message persistence
          Pull based : allows many consumer to connect on different offset
          Scale: Apache Kafka can scale up to send a million messages every second.
          Persistency: It supports data persistency.
          Consumer Capability: Kafka only supports one-to-many consumers

    RabbitMQ
      https://www.upsolver.com/blog/kafka-versus-rabbitmq-architecture-performance-use-case
      - Slow as compared to Kafa and RabbitMQ pub-sub 50K messages per second (https://www.engati.com/blog/redis-kafka-rabbitmq)
      - supports both point to point and pub-sub model.
      - once messages are consumed, the are removed from queue and acknowledgement is provided but they can also be persisted like Kafka depending on configuration
      - It can be both synchronous and asynchronous. supports acknowledge too
      - smart broker -> dumb consumer since it manages the offset consumed
      - uses push model from queue to consumer
      - has control over consistency 
      - does not have message ordering 
      - does not guarantee atomicity
      Use case - 
        Use it when you need push-based model,  do not need to persist message and play smart broker role (complex routing) - https://www.engati.com/blog/redis-kafka-rabbitmq
        - Applications that need to support legacy protocols, such as STOMP, MQTT, AMQP, 0-9-1.
        - Granular control over consistency/set of guarantees on a per-message basis
        - (Smart broker) - Complex routing to consumers
        - Applications that need a variety of publish/subscribe, point-to-point request/reply messaging capabilities.

    Redis pub sub
      https://bytebytego.com/courses/system-design-interview/nearby-friends 
      https://www.engati.com/blog/redis-kafka-rabbitmq
      Use it when you need lightening fast queueing service without persisting message.
        Push based system : smart broker, dumb consumer
        Scale: Redis can send up to 1 million messages per second like Kafka
        Persistency: It does not support data persistency, it is an in-memory data store. However, you can configure to be persistent ~ 1 min snapshotting https://redis.io/docs/management/persistence/
        Consumer Capability: It can handle both, one-to-one and one-to-many consumers.
      Redis pub sub needs scaling support with external party like Zookeeper. Refer this awesome video - https://www.youtube.com/watch?v=6G22a5Iooqk&ab_channel=Redis
      


    RocketMQ 
      doesn’t support delayed messages with arbitrary time precision, but delayed messages with specific levels are supported. Message delay levels are 1s, 5s, 10s, 30s, 1m, 2m, 3m, 4m, 6m, 8m, 9m, 10m, 20m, 30m, 1h, and 2h. https://bytebytego.com/courses/system-design-interview/distributed-message-queue

    Hierarchical time wheel [17].
      https://bytebytego.com/courses/system-design-interview/distributed-message-queue
      for scheduled messages
 


    Streaming vs Batching systems 
     https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation
     Batch : MapReduce
     Stream : Flink (also mentioned in https://bytebytego.com/courses/system-design-interview/metrics-monitoring-and-alerting-system )


    Block storage vs File Storage vs Object storage
      https://aws.amazon.com/what-is/block-storage/
      Block storage
        Common storage devices like hard disk drives (HDD) and solid-state drives (SSD) that are physically attached to servers are all considered as block storage. Files are split into evenly size blocks of data with each block having its own address. Block storage presents the raw blocks to the server as a volume. This is the most flexible and versatile form of storage. The server can format the raw blocks and use them as a file system, or it can hand control of those blocks to an application.
        - can mount VM to this using iSCSI protocol
        - used in operational systems and databases
        - As per geeksforgeeks, this storage is not accessible via the internet and only for attaching to VMs (https://www.geeksforgeeks.org/difference-between-aws-s3-and-aws-ebs/)
        - Metadata(location of each block in the file, name,etc) is limited in block storage which makes it faster
        Granular control
          Developers gain a high degree of control over storing data on block storage. For example, they can optimize performance by grouping fast-changing data on specific blocks and storing static files on others. This improves system performance as ongoing updates only affect a small number of data blocks instead of an entire file. For example, block storage gives you the flexibility to tier fast-changing data on solid state disk (SSD) for the highest performance, and store warm or cold data on lower cost hard drives (HDD).
          By contrast, file storage has an extra layer consisting of a file system (NFS, SMB) to process before accessing the data.

      File storage
        - built on top of block storage. It provides a higher-level abstraction to make it easier to handle files and directories. Data is stored as files under a hierarchical directory structure. 
        - File storage could be made accessible by a large number of servers using common file-level network protocols like SMB/CIFS [3] and NFS [4].
        - high performant as block storage
        - used in big data applications, which demand significant node throughput, low-latency file access, and read-after-write operations.

      Object storage 
        https://bytebytego.com/courses/system-design-interview/s3-like-object-storage 
        - low cost, vast scalability and supports binary+unstructured data as compared to block and file. It is low performant though.
        - Object storage stores all data as objects in a flat structure. There is no hierarchical directory structure. Data access is normally provided via a RESTful API.
        - Data lake and big data analytics, Backup and restoration, Reliable disaster recovery and Methodical archiving using S# Glacier

      AWS EBS vs EFS vs S3 : https://www.missioncloud.com/blog/resource-amazon-ebs-vs-efs-vs-s3-picking-the-best-aws-storage-option-for-your-business
      EBS vs S3 video - https://www.youtube.com/watch?v=r8bVw0iVvGk 

      Block storage vs object storage
        https://aws.amazon.com/what-is/block-storage/ 
        Both storage solutions are beneficial depending on the use case. Block storage provides low latency and high-performance values in various use cases. Its features are primarily useful for structured database storage, VM file system volumes, and high volumes of read and write loads. Object storage is best used for large amounts of unstructured data, especially when durability, unlimited storage, scalability, and complex metadata management are relevant factors for overall performance.


        https://www.cloudflare.com/learning/cloud/object-storage-vs-block-storage/#:~:text=Block%20storage%20is%20fast%2C%20and,content%2C%20webpages%2C%20and%20emails. 
        Capability	                  Block storage	                                                    Object storage
        Storage capacity	            Limited	                                                            Nearly unlimited
        Storage method	              Data stored in blocks of fixed size, reassembled on demand	        Unstructured data in non-hierarchical data lake
        Metadata	                    Limited	                                                            Unlimited and customizable
        Data retrieval method	        Data lookup table	                                                  Customizable
        Performance	                  Fast, especially for small files	                                  Depends, but works well with large files
        Cost	                        Depends on vendor, usually more expensive	                          Depends on vendor, usually less expensive 
                                                                                                          (aside from egress fees)


      

  Cache benchmarking

    Azure Redis Cache : 
        53 GB, with 99.9% availability (https://github.com/Huachao/azure-content/blob/master/articles/redis-cache/cache-faq.md)
        230 us (https://redis.io/docs/management/optimization/latency/)
        Redis QPS - 50K https://redis.io/docs/management/optimization/benchmarks/
        1 million TPS - https://bytebytego.com/courses/system-design-interview/digital-wallet 
        Redis also privides sorted set data structure. Our leaderboard use case maps perfectly to sorted sets. Internally, a sorted set is implemented by two data structures: a hash table and a skip list [1]. The hash table maps users to scores and the skip list maps scores to users. In sorted sets, users are sorted by scores. (https://bytebytego.com/courses/system-design-interview/real-time-gaming-leaderboard)
        Sorted set is used in Rate limiter sliding window design which fetches based on timestamp range and avoids race conditions. 
        Redis cluster also supports sharding with hash slot ( different from consistent hashing)
        Redis cluster uses master-replica model
        Redis Cluster does not guarantee strong consistency in terms of CAP. But has strict isolation level - seriazable
        Max Size of each redis instance from AWS : 500 GB https://aws.amazon.com/elasticache/pricing/?nc=sn&loc=4 
        Sorted Set - https://redis.io/docs/data-types/sorted-sets/
        How redis works 
          https://www.youtube.com/watch?v=5TRFpFBccQM
        Redis is single threaded and supports serializability isolation level (serial execution), epi pg. 253
        Persistence 
          - you can configure to be persistent ~ 1 min snapshotting https://redis.io/docs/management/persistence/

        Promise Cache to resolve thundering herd - VERY IMPORTANT
          https://redis.com/blog/caches-promises-locks/
          - basically if say 1 M clients requests for same item and there is cache miss. So instead of forwarding all requests to DB, only one request is made to DB to populate the cache. Rest all the requests are given promises. 
          - With redis, you can create the lock:my_value using NX option. This option ensures only one request will be able to set the key. 
          - So if a request is able to get the this lock - lock:my_value, then its your job to fetch the value from SOT and set in DB. Once you set the key, you can notify on the pub/sub channel for lock:my_value
          - All other requests which  are not able to get this lock, they subscribe to notif:foo pub sub channel and gets notified about the requested value.

    CDN
        https://www.youtube.com/watch?v=RI9np1LWzqw
        - brings content closer to the user to increase the performance
        - CDN server locations called point of presence PoPs. Server inside POPs are called edge server
        - DNS based routing and anycast used to send the content to the user
        - reduce bandwidth requirements of origin server
        - can minify js files, transform image from old format to modern format
        - Because of its reverse proxy capabilities, all TLS connection terminate at edge server(CDN) thereby reducing TLS handshake overhead. One of the reason why modern servers cache dynamic content on CDN
        - Security : huge network capacity to avoid DDOS attacks. CDN built on anycast network to diffuse ddos traffic
        - Improves availability by having copies available at many locations 

        Pricing 
          https://cloud.google.com/cdn/pricing 
          - remember CDN charges for egress (0.02$ per GiB), cache fill (same charge - 0.01$ per GiB) and 0.0075 per 10K req


      Memcache Vs Redis
        https://www.baeldung.com/memcached-vs-redis#:~:text=Architecture,can%20perform%20better%20than%20Redis. 

        Memcached is a distributed memory caching system designed for ease of use and simplicity and is well-suited as a cache or a session store.
        Redis is an in-memory data structure store that offers a rich set of features. It is useful as a cache, database, message broker (pub sub), and queue.

        Memcache does not support persistence 
        Redis supports persistence (https://redis.com/redis-enterprise/technology/durable-redis/)

        Memcached stores key-value pairs as a String and has a 1MB size limit per value. 
        However, Redis also supports other data structures like list, set, and hash, and can store values of up to 512MB in size.

        Memcached doesn't support transactions, although its operations are atomic.
        Redis provides out-of-the-box support for transactions to execute commands. We can start the transaction using the MULTI command. Then, we can use the EXEC command for the execution of the following subsequent commands. Finally, Redis provides the WATCH command for the conditional execution of the transaction. 
        One of the reason why Redis is a good choice for applications which need atomic transactions and serializable isolation - Rate limiter

        Memcached doesn't support publish/subscribe messaging out-of-the-box.
        Redis, on the other hand, provides functionality to publish and subscribe to messages using pub/sub message queues.

        Geospatial support is useful for implementing location-based features for our applications. Unlike Memcached, Redis comes with special commands to manage real-time geospatial data. For instance, the GEODIST command calculates the distance between two geospatial entries. Likewise, the GEORADIUS command returns all the entries within the radius provided. Additionally, we can use Spring Data Redis to enable Redis geospatial support in a Java application.

        Memcached implements a multi-threaded architecture by utilizing multiple cores. Therefore, for storing larger datasets, Memcached can perform better than Redis.
        Redis uses a single core and shows better performance than Memcached in storing small datasets when measured in terms of cores.

        We can certainly conclude that Memcached is a solid choice for solving simple caching problems. However, generally speaking, Redis outperforms Memcached by offering richer functionality and various features that are promising for complex use-cases.



  SQL DB and benchmarking - Relational
    1. MySQL benchmarking
          48 cores, single node
          4 x # of cores ~ 200
            200 reads ~ 200 us
            200 writes ~ 1 ms (writes 4x slower)
          load test
            10K reads ~ 10 ms 
            10K writes ~ 50 ms (writes 4x slower)

          - Indexed Read
              a. with primary key reads which is already indexed
                (https://dev.mysql.com/blog-archive/mysql-connection-handling-and-scaling/)
                Recommend 4 * number of cores parallel for ~ 200 us based on simple primary key look ups. For example, 120 (~4 * 32) parellel requests for 32 cores will take 200 us
                1K requests on 32 will take ~ 1ms for simply select query
          
          - Write
             http://minervadb.com/wp-content/uploads/2020/10/MySQL-8-Performance-Benchmarking-on-Amazon-EC2.pdf 
            impact of write performance on indexing
              4 times slower : https://logicalread.com/impact-of-adding-mysql-indexes-mc12/#.Y__7d-zMLuU 

          - General benchmarking
            Today, a relational database running on a typical data center node can support a few thousand transactions per second.


          - Max size set by MySQL
            256 TB (https://dev.mysql.com/doc/mysql-reslimits-excerpt/8.0/en/table-size-limit.html#:~:text=You%20are%20using%20a%20MyISAM,2567%20%E2%88%92%201%20bytes)
            AWS limit : 64 TB storage and 24 TB of RAM

          Some facts
          - MySQL client <-> server is socket based connection. Check why ?
          - Max connection limit for MySQL = 100 K
          - per connection -> one user thread
          - size of user thread depends on THD connection data structure which is per connection ~ 10 MB 
          - max THD ~ 10K 
          - So recommended min size of your MySQL = 10 MB * 10 K = 100 GB ??

          - MySQL sorting (https://www.pankajtanwar.in/blog/what-is-the-sorting-algorithm-behind-order-by-query-in-mysql)
              External merge sort (quick sort + merge sort) if data doesn’t fits into the memory
              Quick sort, if data fits into the memory and we want all of it
              Heap sort, if data fits into the memory but we are using LIMIT to fetch only some results
              Index lookup (not exactly a sorting algorithm, just a pre-calculated binary tree)

          - MySQL query cache (https://docs.oracle.com/cd/E17952_01/mysql-5.1-en/query-cache.html)
              As of MySQL 5.1.63, the query cache is not supported for partitioned tables, and is automatically disabled for queries involving partitioned tables. The query cache cannot be enabled for such queries. 

          - MySQL sharding (https://stackoverflow.com/questions/1610887/how-to-partition-mysql-across-multiple-servers)
              this is different from mysql partitioning (https://vertabelo.com/blog/everything-you-need-to-know-about-mysql-partitions/)
          
          - Moreover, MySQL involves no standard implementation for sharding. (https://kinsta.com/blog/mongodb-vs-mysql/)

          - MySQL indexing 
              https://www.youtube.com/watch?v=YuRO9-rOgv4 using B tree
              B tree performance : https://www.youtube.com/watch?v=FgWbADOG44s 
              Its inefficient as compared to B tree for writes : https://www.youtube.com/watch?v=I6jB0nM9SKU 

          - MySQL replica sync
              https://serverfault.com/questions/30605/how-fast-is-mysql-replication
              MySQL replication happens as close to real-time as possible (AWS has <100 ms SLA), as limited by disk and network I/O. The slaves open a socket to the master, which is kept open. When a transaction occurs on the master, it gets recorded in the binlog (MySQLs replication log), and is simply replayed on the slave(s). If the socket between master and slave is interrupted, the binlog is replayed for the slave upon the next successful connection.


          - Multi-region failover time for AWS RDS : 60-100s 
              https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZSingleStandby.html

          - Replication lag for AWS RDS : < 100 ms 
              https://www.bluematador.com/docs/troubleshooting/rds-replica-lag#:~:text=Replication%20lag%20measures%20how%20far,than%20100ms%20of%20replication%20lag. 

          - we could simply deploy a simple relational database on each data node. SQLite [19] is a good choice here. It is a file-based relational database with a solid reputation. https://bytebytego.com/courses/system-design-interview/s3-like-object-storage 


          - Practical
              how MySQL gurantees ACID : https://www.youtube.com/watch?v=0OYFsJ1-1YA
              Nice theoretical : https://www.youtube.com/watch?v=clPPKgYJC10 
               

    2. CockroachDB
      https://github.com/cockroachdb/cockroach/blob/master/docs/design.md 
        CockroachDB is a distributed SQL database. The primary design goals are scalability, strong consistency and survivability (hence the name). CockroachDB aims to tolerate disk, machine, rack, and even datacenter failures with minimal latency disruption and no manual intervention. CockroachDB nodes are symmetric; a design goal is homogeneous deployment (one binary) with minimal configuration and no required external dependencies. 
        Distributed transaction : https://www.youtube.com/watch?v=OJySfiMKXLs&t=1271s 
       - uses RocksDB for its key value storage
       -  Nice video explaining how concurrency is handle on distributed transactions : https://www.youtube.com/watch?v=iD_Yk5AhNGc 
          1. So atomicity is guaranteed with Raft
            Raft : https://thesecretlivesofdata.com/raft/ 
              - Raft handles atomic writes and consistent reads
              
          How about Isolation 
          2. SSI by default in distributed databases - how?
              - MVCC - multiversion concurrency control -  https://youtu.be/iD_Yk5AhNGc?t=1783
                - uses timestamp as form of versioning but depends on time synchronization which is a challenges in distributed systems. Even with NTP, there is a challenge. 

        Example of shifting to CockroachDB from Cassandra 
          https://www.cockroachlabs.com/blog/cassandra-to-cockroachdb/#more-on-cockroachdb-vs-cassandra
          - needed relational data modelling
          - SQL query flexibility

        Benchmarking : 
        Benchmark
        - Single row reads and writes
          https://www.cockroachlabs.com/docs/v22.2/performance.html
          across availability zones - 3 node cluster, c5d.9xlarge (36v CPUs, 72 G RAM)
            100K-200K QPS
            single-row reads in 50-100 ms and processes 
            single-row writes in 50-100 ms
        - Batch
          https://www.cockroachlabs.com/docs/v22.2/performance-benchmarking-with-tpcc-small   
            - p95 in 250-300 ms
              



    NoSQL value DB and benchmarking
        1. Timeseries : OpenTSDB / InfluxDB

             Why Time-series DB and not other key-values like Cassandra, MongoDB ???? 
                - in built support for time-based queries example using Time-structured merge tree
                - in built support for sampling through retention config

                Proof : Thus performance is 100x faster
                  https://www.influxdata.com/blog/influxdb-vs-cassandra-time-series/ 
                     InfluxDB 1.8.10 and Cassandra v4.0.5, 
                  - InfluxDB outperformed Cassandra in all three tests with 5x greater write throughput, while using 2.4x less disk space, and delivering up to 100x faster response times for tested queries. Thus, can handle spiky reads

            Different time-series DB in the market and its comparison
                https://www.youtube.com/watch?v=W-ouPw944CM
                Influx DB
                  - private and non open source
                  - uses push based model
                  
                Prometheus 
                  - uses pull based model and thus samples heavily. May not be ideal for monitoring based on revenue but good for system stats.

                Timescale DB based on Postgres
                  - built on top of postgres so that you can run MySQL
                  - not suitable for scale and does not have AWS RDS support 


            Influx DB benchmarking from bytebytego : 
              8 cores and 32GB RAM can handle over 250,000 writes per second, greater than 1 M series
              (https://bytebytego.com/courses/system-design-interview/metrics-monitoring-and-alerting-system)

              Performance optimization
                https://bytebytego.com/courses/system-design-interview/metrics-monitoring-and-alerting-system
                  According to a research paper published by Facebook [24], at least 85% of all queries to the operational data store were for data collected in the past 26 hours. If we use a time-series database that harnesses this property, it could have a significant impact on overall system performance. If you are interested in the design of the storage engine, please refer to the design document of the Influx DB storage engine [26].
                  - Storage : Store only delta for timestamps
                  - Storage and querying : Downsampling
                  - use cold storage for old data

            

        2. Object storage / cold storage benchmarking
            S3 Standard is designed for 99.99% data availability and durability of 99.999999999% of objects across multiple Availability Zones in a given year. AWS S3 provides a great performance. It automatically scales to high request rates, with a very low latency of 100–200 milliseconds.Your application can achieve at least 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket (https://aws.plainenglish.io/optimize-your-aws-s3-performance-27b057f231a3)
            Another option is to store the data in Amazon S3 using one of the columnar data formats like ORC [5], Parquet [6], or AVRO [7]. We could put a cap on the size of each file (say, 10GB) and the stream processor responsible for writing the raw data could handle the file rotation when the size cap is reached. (https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation)


        3. Cassandra

              100s video : https://www.youtube.com/watch?v=ziq7FUKpCS8
              Slower version of 100s : https://www.youtube.com/watch?v=YjYWsN1vek8 
              
             
              How Cassandra data is  stored in disk :  
                https://www.baeldung.com/cassandra-storage-engine (LSM Tree + SS Table + Sparse Index)
               
                
              Cassandra as Columnar databse :
                https://stackoverflow.com/questions/13010225/why-many-refer-to-cassandra-as-a-column-oriented-database
                http://dbmsmusings.blogspot.com/2010/03/distinguishing-two-major-types-of_29.html
                https://www.baeldung.com/cassandra-column-family-data-model 
                  - Its not a direct column based store but its based on column family which stores data row wise
                
                Cassandra column family : 
                  https://www.scylladb.com/glossary/cassandra-column-family/ 
                  Basically you can create multiple column families with different columns and each will have its own LSM tree + SS Table
                  Practical : 
                  Create multiple column families in Cassandra within a key space
                  https://www.dbrnd.com/2016/05/nosql-create-your-first-cassandra-column-family-table/  

              Great read : Use of parition key + clustering key storages in Cassandra
                http://distributeddatastore.blogspot.com/2020/03/cassandra-new-sstable-storage-format.html
                https://cassandra.apache.org/doc/latest/cassandra/architecture/storage_engine.html
                - parition key to assign the nodes and also the key component of SS Table. So there inside every node, there is an index.db in form of SSTable storing partition key and its position of data in data.db
                - At the position of parition key in data.db, clustering key is used as sorted data within that partition key
                - Remember a partition key could be in more than 1 SSTable due to subsequent insert/update

              How updates are handled in Cassandra 
              https://docs.datastax.com/en/cassandra-oss/3.x/cassandra/dml/dmlWriteUpdate.html 
              Periodically, the rows stored in memory are streamed to disk into structures called SSTables. At certain intervals, Cassandra compacts smaller SSTables into larger SSTables. If Cassandra encounters two or more versions of the same row during this process, Cassandra only writes the most recent version to the new SSTable. After compaction, Cassandra drops the original SSTables, deleting the outdated rows.
              
              Cassandra indexing
                Paritioning : https://www.youtube.com/watch?v=Np5RJpCiCzM
                Clustering used as sort key : https://www.youtube.com/watch?v=OCakxrzwuU4 
                  Remember data in cassandra on disk supports SSTable which is sorted string table.
                  So searching could be done in logN times like relational or mongoDB

              Practical 
                    Cassandra indexing (partition + clustering which is sort key) : https://www.youtube.com/watch?v=S9rmf4X7E_E

              Garbage collection
                4.0 uses Z GC instead of G1 GC (java8 to java 16)

              Cassandra and map-reduce
                https://subscription.packtpub.com/book/data/9781787127296/1/ch01lvl1sec5/mapreduce-and-spark
                Unlike MongoDB, Cassandra does not offer built-in MapReduce capabilities. But it can be integrated with Hadoop in order to perform MapReduce operations across Cassandra data sets, or Spark for real-time data analysis. 
                Datastax is Lightning-fast cluster computing with Apache Spark™ and Apache Cassandra (https://github.com/datastax/spark-cassandra-connector)

            

              Cassandra performance on writes + reads on indexes 
                - LSM tree + SS tables
                Great video : https://www.youtube.com/watch?v=I6jB0nM9SKU 
                Good read : https://rahulpradeep.medium.com/sstables-and-lsm-trees-5ba6c5529325 
                B Tree vs LSM tree technique - https://www.youtube.com/watch?v=4z7-SrDiBoU

              Consistent hashing
                If we add a new node to the cluster, it automatically rebalances the virtual nodes among all nodes. No manual resharding is required. See Cassandra’s official documentation for more details [22]. - https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation 

              Consencus
                Paxos - https://cassandra.apache.org/_/blog/Cassandra-4.1-is-here.html 
                  Some examples of use cases are selling seats to a concert and where an INSERT or UPDATE operation must be unique, such as for a customer ID. Historically, Lightweight Transactions (LWTs) have suffered from poor performance, particularly in a WAN setting and under contention. As LWTs used a read-before-write approach, LWTs had more round trips which hurt performance.
                  Recognizing these shortcomings, Paxos was optimized, improving LWT performance by 50%, improving latency, and halving the number of required round-trip to achieve consensus. Linearizability is now guaranteed across range movements in-line with what you would expect from a database with strong consistency.

              Challenges with Cassandra
                1. Data loss or durability challenge : LWW (last write wins) is common issue with multi-leader replication and leaderless databases like Cassandra - Pg 292 DDIA. With Paxos, you can achieve Linearizability but face issue with LWW if clocks are not synchronized.
                2. Cassandra uses consistent hashing and 
                
              Benchmarking
                - Expensive hardware https://www.scylladb.com/2021/08/19/cassandra-4-0-vs-cassandra-3-11-comparing-performance/
                  i3.4xlarge (16 vCPUs, 122 GB), 3 node cluster
                   Read : 100 K ops/s
                   Write : 100 K ops/s
                   Read Latency : ~10 ms
                   Write Latency : ~10 ms

        4. MongoDB
              - How Mongo DB search works : https://www.youtube.com/watch?v=tSgPhxZdhLk&t=96s 
                  using index in B tree
              - MongoDB supports range-based or hash-based sharding - https://kinsta.com/blog/mongodb-vs-mysql/
              - For aggregations, Instead of map-reduce, you should use an aggregation pipeline. Aggregation pipelines provide better performance and usability than map-reduce. https://www.mongodb.com/docs/manual/core/map-reduce/
              - How Mongo DB sharding works : https://kinsta.com/blog/mongodb-vs-mysql/ 
              - Practical
                  set up mongo based sharding https://www.youtube.com/watch?v=mjSNKjTzeao 
                  mongoDB gridFS https://www.youtube.com/watch?v=mZE3aBdr010 

        6. Dynamo DB
            Nice intro in https://bytebytego.com/courses/system-design-interview/real-time-gaming-leaderboard

            DynamoDB is a fully managed NoSQL database that offers reliable performance and great scalability. To allow efficient access to data with attributes other than the primary key, we can leverage global secondary indexes [16] in DynamoDB. A global secondary index contains a selection of attributes from the parent table, but they are organized using a different primary key. 
            So Every global secondary index must have a partition key, and can have an optional sort key. The index key schema can be different from the base table schema. 

            DynamoDB splits data across multiple nodes using consistent hashing.

            uses quorum consistency 
            uses read-repair and anti-entropy
            supports serializable transactions


        7. S3

        8. Neo4j
            https://www.youtube.com/watch?v=GM9bB4ytGao&ab_channel=Neo4j 
            Data is already in the form of relationship 
            benchmark
             -> depth of 2 ~ 10 ms
             -> depth of 3 ~ 150ms
             -> depth of 5 ~ 2s vs 1 hour in MySQL

        9. RocksDB
           https://rocksdb.org/
          embeddable persistent key-value store for fast storage
          - high performance due to LSM implementations
          - RocksDB is optimized for fast, low latency storage such as flash drives and high-speed disk drives. RocksDB exploits the full potential of high read/write rates offered by flash or RAM.
          RocksDB                                 vs                    Redis 
          optimized for SSDs(flash), disks                              in-memory db
          more storage larger than RAM                                  storage size restricted due to RAM (in-memory)
          performance close to its SSD/Disk (~1-10ms)                   best performance (200 us with 50K transaction on 53 G)

        10. Apache Flink - Stream processing system
          https://www.youtube.com/watch?v=ZU1r7uEAO7o 
           - Vs Batch processing, it processes the events immediately locally storing the state
           https://www.youtube.com/watch?v=_G-hQfT02BA 
           - Achive fault tolerant with periodic snapshots to S3/HDFS storage which serve as a checkpoint. 
           - Also guarantees exactly-once processing

          https://www.openlogic.com/blog/apache-flink-vs-kafka-streams# 
           It was the first open source framework that could deliver on throughput at scale (up to tens of millions of events per second), sub-second latency as low as 10s of milliseconds, and accurate results.

            How this is different from Kafka ?
            https://www.openlogic.com/blog/apache-flink-vs-kafka-streams#
              - The main difference between Flink vs. Kafka Streams is that Flink is a data processing framework that uses a cluster model, whereas the Kafka Streams API is an embeddable library that eliminates the need for building clusters.

            Flink vs Spark
              https://www.youtube.com/watch?v=VAwtpa9EHf0 
              - Spark can be configured to be near real time processing but has slow performance
              - Flink is made for real-time processing with 2x performance as compared to Spark
              
          11. Google Cloud Spanner
            https://research.google/pubs/pub39966/
            Spanner is Google's scalable, multi-version, globally-distributed, and synchronously-replicated database. It is the first system to distribute data at global scale and support externally-consistent distributed transactions.
            https://en.wikipedia.org/wiki/Spanner_(database)
            Spanner is a distributed SQL database management and storage service developed by Google.[1] It provides features such as global transactions, strongly consistent reads, and automatic multi-site replication and failover. Spanner is used in Google F1, the database for its advertising business Google Ads.[2]

  Bigdata systems today
    Understand Hadoop ecosystem
      https://www.youtube.com/watch?v=YzAAhbmSt_k
      MapReduce - allows parallel processing and merging
      HDFS - Highly distributed file system based on Google file system (GFS)
      YARN - Job scheduling and resource management
      Common utilitieis
    https://ahana.io/learn/comparisons/hive-vs-presto-vs-spark/ 

    HBase
      It is nothing but a hadoop database and its no SQL. Great layered diagram in https://www.educba.com/hadoop-vs-hbase/ 

    Hive
      https://www.youtube.com/watch?v=taTfW2kXSoE 
      - Hive creates SQL query layer which in turn uses map reduce to query big tables stored in HDFS. This helps SQL experts query big data in HDFS directly in SQL and does not need complicated logic of map reduce.
      - Hive is not a database

      Hive architecture - https://www.youtube.com/watch?v=rr17cbPGWGA&t=270s
        Hive is optimized for query throughput and thus supports large data aggregations 

    Presto
      https://www.youtube.com/watch?v=hEFsHQ_kJR8 
       - similar to Hive, but has amazing capability of query multiple databases - noSQL + SQL ones including Hive. It is optimized for latency and thus used for interactive queries or quick data exploration. Facebook uses Presto.
      Nice clarification about Presto
        https://ahana.io/learn/presto/
        - Presto does not use Map reduce 
        - Presto is not a database


      How twitter uses presto
        https://www.youtube.com/watch?v=WaooVcS75xA

    Spark
      https://www.ibm.com/cloud/blog/hadoop-vs-spark
        Spark is a Hadoop enhancement to MapReduce. The primary difference between Spark and MapReduce is that Spark processes and retains data in memory for subsequent steps, whereas MapReduce processes data on disk. As a result, for smaller workloads, Spark’s data processing speeds are up to 100x faster than MapReduce.

        Furthermore, as opposed to the two-stage execution process in MapReduce, Spark creates a Directed Acyclic Graph (DAG) to schedule tasks and the orchestration of nodes across the Hadoop cluster. This task-tracking process enables fault tolerance, which reapplies recorded operations to data from a previous state.

        Working of DAG Optimizer in Spark
        https://data-flair.training/blogs/dag-in-apache-spark/
          We optimize the DAG in Apache Spark by rearranging and combining operators wherever possible. For, example if we submit a spark job which has a map() operation followed by a filter operation. The DAG Optimizer will rearrange the order of these operators since filtering will reduce the number of records to undergo map operation.

        Then why DAG is better than MapReduce ? 
          BEST - explanation  
          https://www.quora.com/What-are-the-advantages-of-DAG-directed-acyclic-graph-execution-of-big-data-algorithms-over-MapReduce-I-know-that-Apache-Spark-Storm-and-Tez-use-the-DAG-execution-model-over-MapReduce-Why-Are-there-any-disadvantages 

        Spark architecture
          - components include spark SQL, streaming, machine learning, 
          - follows master-slave architecture ( Driver which is master -> cluster manager -> worker node with cache)
          - Driver uses its Sparkcontext and connect with cluster manager to get resource info and generates RDD datastructures. Then it convert core transformations and actions into DAG.
          - Driver schedules the task on workers and resources are allocated to tasks by cluster manager.
            Spark architecture in 3 mins : https://www.youtube.com/watch?v=rJFg2i_auAg&ab_channel=BigDataElearning 

          - Some Cluster manager used by Spark
              https://data-flair.training/blogs/apache-spark-cluster-managers-tutorial/
              - Apache Spark Standalone Cluster Manager 
                  - simple allocates resources based on the core. By default, an application will grab all the cores in the cluster.
              - Apache Mesos (popular)
                  - Mesos handles the workload in distributed environment by dynamic resource sharing and isolation. It is healthful for deployment and management of applications in large-scale cluster environments. Apache Mesos clubs together the existing resource of the machines/nodes in a cluster. From this, a variety of workloads may use. This is node abstraction, thus it decreases an overhead of allocating a specific machine for different workloads. It is resource management platform for Hadoop and Big Data cluster.
                  In some way, Apache Mesos is the reverse of virtualization. This is because in virtualization one physical resource divides into many virtual resources. While in Mesos many physical resources are club into a single virtual resource. 
              - Hadoop YARN

              What to use
              Hence, in this Apache Spark Cluster Managers tutorial, we can say Standalone mode is easy to set up among all. It will provide almost all the same features as the other cluster managers.Moreover, to use richer resource scheduling capabilities (e.g. queues), both YARN and Mesos provide these features



        Other DAG resource manager example - Apache Airflow
            https://www.youtube.com/watch?v=mtJHMdoi_Gg 

        Parquet
          https://spark.apache.org/docs/latest/sql-data-sources-parquet.html
          Parquet is a columnar format that is supported by many other data processing systems. Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. 

        

  Attempt to standardized DB selection criteria 
      1. MySQL vs noSQL 

            When MySQL
            if ACID, 
              then MYSQL (high confidence)
            if reasonable size (< 1 TB) not expected to scale rapidly,
              if heavy reads (reading from read replicas is faster) 
                if low writes 
                  then MySQL (high confidence)
                if heavy writes (else have to keep synchronizing and index making)
                  then MySQL (low confidence)
            If db needs rapid scale (size keeps growing)
              if manual sharding is needed ( say by user id )
                if sharding has all related rows (try to denormalize)
                  if there are less aggregations 
                    then MySQL (medium confidence)
                  if there are more aggregations 
                    then MySQL (low confidence since aggregations with sharding could be better achieved with noSQL DBs like MongoDB, Cassandra)
            If db needs better search performance (not full text search)  
              if index-ing on fixed columns,
                then MySQL (high confidence)
              if db experience heavy writes
                then MySQL (low confidence since indexes increases write latency) 

      2.  MySQL vs MongoDB

      3.  MongoDB vs Cassandra
            https://www.youtube.com/watch?v=3z2EzILA3Rk 

                Cassandra                          vs               MongoDB 
            column data model                                   json document data model
            no config server(ring structure)                    mongo master, slave and mongo config servers for replication
            high availability due to multiple masters           master -> slave takes time
            accept writes in parallel                           write capacity is limited since they just go to master
            similar to SQL                                        based on json formatting

            When mongo DB
              query also based on secondary indexing and flexible querying
              built in data aggregation framework like aggregation pipeline
              read-heavy workload

            When Cassandra 
                query based on primary key indexing 
                100% uptime guarantee due to replication and inconsistency resolution using Paxos(similar to raft)
                high write speed
                language support for SQL
                write-heavy, read-heavy workload
                eventual consistency (as per bytebytego - https://bytebytego.com/courses/system-design-interview/design-a-key-value-store )

     

      Open questions
            1. Why is MySQL better for read heavy low write workload ? answered above
            2. MySQL is better for ACID systems and structured ? How ? answered above
            2. How MySQL syncs with its replicas ? answered above
            2. Why is Cassandra (noSQL) better for read heavy write heavy workload ? answered above
            3. Aggregation on sharded MySQL needs Application level handling vs Cassandra or HBase(Column) can handle this since they are built on top of distributed systems . Fact and derived above
            4. How MySQL indexing works ? answered above
            5. Cassandra benchmarking ? answered above
            7. Time taken from master to slave - MySQL, MongoDB ? MySQL ( 100 ms aws)
            9. Refer NTP ? answered above
            8. MongoDB benchmarking ?
            6. Study about graphdbs ? 

 Operation system
    User level threads vs Kernel level threads
      https://www.youtube.com/watch?v=_5q8ZK6hwzM

 

 
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================
=========================================================================================================================


References

Dump : https://coursehunters.online/t/educative-io-design-gurus-grokking-the-system-design-interview-part-5/584
Quick read : https://github.com/Jeevan-kumar-Raj/Grokking-System-Design

    
- Load balancing
    Resources
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/load-balancing.md
      https://www.cloudflare.com/learning/performance/types-of-load-balancing-algorithms/
      https://igotanoffer.com/blogs/tech/load-balancing-system-design-interview
    Static load balancing - round robin, weight round robin, IP hash
    Dynamic load balanacing - least connection, least resource, least response time, least bandwidth
    When to use Static ?
      If the server pool and the requests are both homogeneous, and there is only one balancer. Stateless servers handling single api can use static load balancer
      
    When to use Dynamic ?
      if the requests are heterogenous, Least Load is helpful to prevent servers from being intermittently overloaded.
      
    What are Hardware load balancers ?
      They have high performant hardware resources like L4 and L7. 
      Read about L4 and L7 here https://levelup.gitconnected.com/l4-vs-l7-load-balancing-d2012e271f56
      
    What are Software load balancers ?
      They run on standard servers and are less hardware optimized, but cheaper to set up and run. Example, Nginx or HAProxy. For Nginx 100s video, 
      refer https://www.youtube.com/watch?v=JKxlsvZXG7c. Nginx can serve routing to different servers, rate limiting, handle spikes,
      reverse proxy, security, cache, etc. 
      
    When to use Software or Hardware ?
      Tip : Since software load balancing can run on ordinary hardware and supports, always prefer software load balancer. Example Nginx
       
      
    What is proxy server, reverse proxy and API Gateway ? How is different from load balancer
      proxy vs reverse proxy vs load balancer : https://www.youtube.com/watch?v=MiqrArNSxSM
      Proxy : just protects the client side
      Reverse Proxy : protects the server by serving as proxy for all backend services - routing, rate limitng, load balancing
      API gateway : rate limiting, routing, handle spikes
      Load balancer : only performs load balancing
      
      So Reverse proxy can perform both API Gateway and Load Balancing. Example nginx.
      A load balancer can never be API gateway or reverse proxy.
      
     Always choose Nginx ? Why ?
      It is powerful due to non-blocking worker threads
        - Nginx does not create one thread/process per request/connection and wait for response from server or next request from client. Instead it has fixed worker threads which can listen to multiple clients or server responses at a time whichever is ready https://www.nginx.com/blog/inside-nginx-how-we-designed-for-performance-scale/ 

       Has triats of load balancing + reverse proxy + API gateway
         - supports load balancing
         - supports request forwarding based on different paths like api gateway
         - supports reverse proxy 
         - supports rate limiting
         - supports caching
         - supports request grouping
         - supports secure auth SSL. Also does optimizations with keep alive connection and cache to avoid SSL handshake - https://docs.nginx.com/nginx/admin-guide/security-controls/terminating-ssl-http/ 
         Also a good read https://www.nginx.com/blog/http-keepalives-and-web-performance/
       Nginx supports L7

       Why Load balancer + API gateway should be consolidated with  Nginx 
         https://www.nginx.com/blog/consolidating-your-api-gateway-and-load-balancer-with-nginx/
        1 million QPS, 65K SSL TPS, 70 Gbps throughput 
        Netflix, Hulu, Airbnb, Pinterest


    
================================================================

      
  - Caching
    Resources
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/caching.md
      Distributed cache on Application server VS Global cache like Redis, (mostly preferred)
      CDN is another form of cache for static content but its often expensive
      
          
       What are different caching strategies ? 
          https://codeahoy.com/2017/08/11/caching-strategies-and-how-to-choose-the-right-one/
          Cache-Aside (application server has cache aside like Redis, Memcache and DB separately)
            - Pros : logic controlled by application server, resilent to cache failures, data model in cache vs db could be different
            - Cons : stale data in cache for db writes
          Read-Through Cache (application server reads from cache only)
            - Pros : logic controlled by cache, read-heavy, good consistency when combined with write through 
            - Cons : needs data model to be consistent, first time data always results in cache miss
            Application : autocomplete suggestion to read from trie cache - https://bytebytego.com/courses/system-design-interview/design-a-search-autocomplete-system 

          Belpw are Cache writes invalidation policy 

          Write through (data is written to cache and synced with storage)
            - Pros : fast retreival and data consistent systems
            - Cons : slow writes though
          Write around (data is written to storage, not cache)
            - Pros : Saves write operation on the cache
            - Cons : recently written data creates a cache miss and higher latency.
          Write back (data is written to cache only, then synced later to storage)
            - Pros : fast retreival, low latency read / writes
            - Cons : Risk of data loss
            
        Real applications of 
          1. read-through, write-through for high consistency
                DynamoDB Accelerator (DAX) for dynamoDB
          2. read-through, write-around for high performance for situations where data is written once and read less frequently or never.
                real-time logs, chatroom messages
          3. cache-aside, write back cache to absorb spikes during peak load
                custom applications using redis
          4. write back cache
                InnoDB which is relational database storage engine. Queries are first written to memory and eventually flushed to the disk.
                
                
================================================================
    - Sharding / Partitioning 
        Resources
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/sharding.md
          https://coursehunters.online/t/educative-io-design-gurus-grokking-the-system-design-interview-part-5/584
          Horizontal, Vertical and Directory based paritioning
            Directory based paritioning has work around to solve some challenges with Horizontal and Vertical?
          What's the best Partitioning criteria ?
            Consitent hashing which is combination of hash and list based partitioning
          Commom problems with Sharding / Partitioning
            Joins -> Denormalization
            Referential Integrity (primary key -> foreign key. look at image in https://en.wikipedia.org/wiki/Referential_integrity)
            Hot shard problem needs rebalancing
              How to rebalance ? 
              It would take downtime, check directory based partitioning as per our educative.io resource.
              But consistent hashing can solve this problem better - https://medium.com/nerd-for-tech/consistent-hashing-6524e48ac648
              Great guide above - check consistent hashing with gossip protocol so that each partition knows where to fetch data from db
            Heterogeneity: the number of virtual nodes for a server is proportional to the server capacity. For example, servers with higher capacity are assigned with more virtual nodes.
          Applications of consistent hashing ?
          Apache Cassandra, Dynamo DB
 
 ================================================================

    - Indexing
        Resources
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/indexes.md
          What is a database index ? 
            https://www.codecademy.com/article/sql-indexes
          Pros
            helps in speeding up the search 
          Cons
            lowers write/update/delete performance by 4 times
          When to use indexes ?
            In the case of data sets that are many terabytes in size but with very small payloads (e.g., 1 KB), 
            indexes are a necessity for optimizing data access. Finding a small payload in such a large dataset can be a real challenge since
            we can’t possibly iterate over that much data in any reasonable time. Furthermore, it is very likely that such a large data set is 
            spread over several physical devices—this means we need some way to find the correct physical location of the desired data. 
            Indexes are the best way to do this.
            
   ================================================================

     - Proxy 
        Resources
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/proxies.md
          
          Usage : 
          filter requests, log requests, cache, encryption and most important batch request. Further, it can get smarter to o collapse requests for data that is spatially close together in the storage (consecutively on disk). This strategy will result in decreasing request latency. 
          For example, let’s say a bunch of servers request parts of file: part1, part2, part3, etc. We can set up our proxy in such a way 
          that it can recognize the spatial locality of the individual requests, thus collapsing them into a single request and reading complete file, 
          which will greatly minimize the reads from the data origin.
          
          Proxy are of two types 
          Client proxy to protect the clients
          Server proxy to protect the servers. Also called reverse proxy
          
          Should we use proxy then?
          Only use proxy to protect the client.
          For server side, Always Nginx since its open source and supports both reverse proxy + load balancing + API gateway.
          
          
   
   ==============================================================================

          Queues
          
            Resource :
            - https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/queues.md
            
            When to use queue :
            Queues are implemented on the asynchronous communication protocol, meaning when a client submits a task to a queue they 
            are no longer required to wait for the results; instead, they need only acknowledgment that the request was properly received.
            
            Queues are also used for fault tolerance as they can provide some protection from service outages and failures. For example, 
            we can create a highly robust queue that can retry service requests that have failed due to transient system failures
            
            When not to use queue :
            When client expects respone in real-time
            
            Example of queues : 
             RabbitMQ and Kafka (which is open source)
             
  ====================================================================================
  
        Redundancy and Replication
        
          Resource : 
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/redundancy.md
          
          When to use Redundant system :
          Always. its a key concept of distributed system.
          Always prefer shared nothing architecture so that each node can operate independently and can scale.
          
          
    ====================================================================================
        SQL vs NoSQL
        
          Resource : 
          https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/sql-vs-nosql.md
          
          SQL 
          Relational databases store data in rows and columns. Example, MySQL, Oracle, MS SQL Server, SQLite, Postgres
          
          NoSQL 
          Key-value :  Example, DynamoDB, Redis, Memcache
          Document DB : Example, MongoDB, 
          Wide-Column Database : Example, HBase, Cassandra - https://hevodata.com/learn/columnar-databases/
          Graph Databases : Example, Neo4j
          
          Differences between both 
          Storage : SQL is tables but NoSQL has different storage models
          Schema : changing schema with SQL is possible but requires whole database modification
          Querying : using SQL for SQL dbs. 
          Scalability : SQL is vertically scalable and possible to scale it horizontally but has limitations. NoSQL is horizontally scalable
          Reliability or ACID : Definitely SQL ensures ACID but NoSQL solutions sacrifice ACID compliance for performance and scalability.
          
          When to use SQL
            ACID compliance for e-commerce and finanicial transactions
            Your data is structured and unchanging.
            Often Read heavy and low-write

            
          When to use NoSQL
            - Storing large volumes of data that often have little to no structure. 
            - Making the most of cloud computing and storage. Cloud-based storage is an excellent cost-saving solution 
            but requires data to be easily spread across multiple servers to scale up. 
            - Rapid development. NoSQL is extremely useful for rapid development as it doesn’t need to be prepped ahead of time. 
            
          What about write performance ? 
          noSQL has better write performance due to SSTable + LSM tree implementation. Example, Cassandra
          

            
            
  ====================================================================================
  
    CAP Theorem
    Resource : 
    https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/cap-theorem.md
    
    CAP theorem states that it is impossible for a distributed software system to simultaneously provide more than two out of three of 
    the following guarantees (CAP): Consistency, Availability and Partition tolerance.
    
    We can only build a system that has any two of these three properties. Because, to be consistent, all nodes should see the same set of updates 
    in the same order. But if the network suffers a partition, updates in one partition might not make it to the other partitions before a client 
    reads from the out-of-date partition after having read from the up-to-date one. The only thing that can be done to cope with this possibility is to stop serving requests from the out-of-date partition, but then the service is no longer 100% available.

    When to choose consistency over availability ? 
      Banking, Payment systems
    
    When to choose availability over consistency   ? 
      Most distributed system use cases like Google Maps, Twitter, etc.
      
    Since reliability consists of both consistency and availability, ASK YOUR interviewer what it means for system to be 99.99% reliable
    
    
   ====================================================================================
 
    Consistent Hashing
    
    Resource 
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/consistent-hashing.md
      Recommended ones 
        https://medium.com/nerd-for-tech/consistent-hashing-6524e48ac648 - Explains well about rebalancing
        https://bytebytego.com/courses/system-design-interview/design-consistent-hashing
        
      How does it work ? 
        Hashing is done in the range where it maps the key to the integer in that range. Example 0 -> 255 are the integers placed in ring form such that values are wrapped around.
        
        1. the servers are mapped to the integers in that range
        2. to map key to a server, simply hash the key to the integer in that range and move clockwise till you find the server (could be binary search)
        
        Now how this is better :
        1. adding a server : say S1 is added at position 20 and is near S2 at position 25. Then keys from before 20 wuld map to S1.
        2. removing a server : say S1 is removed at position 20 and was near S2 at position 25. Then  all keys even before 20 would map to S2. 
        3. uniform distribution : add “virtual replicas”. Instead of mapping each server to a single point on the ring, 
            we map it to multiple points on the ring, i.e. replicas. 
        4. easy to rebalance : when server is added or removed, only its next clockwise neighbouring server is affected and requires re-mapping of keys.
        
        
    ====================================================================================
    
    Client-Server Communication
    
      Resource : 
        https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/client-server-communication.md
        
      Types : 
      1. Standard HTTP Web Request : client opens the connection, gets response from the server and terminate the connection 
      2. Ajax Polling : client performs HTTP Web Request above periodically (say every 1 s). May create unecessary empty responses if server is not ready.
      3. Long Polling : client performs HTTP Web Request and waits for long time till response is received (say 1 min). Then sends request again.
      4. Web Socket: persistent connection between a client and a server that both parties can use to start sending data at any time.
      5. SSE :  persistent onnection between a client and a server where server send data to the client but not the reverse
      
      When to use Long Polling vs Web Socket ? 
        https://ably.com/blog/websockets-vs-long-polling
        https://dev.to/kevburnsjr/websockets-vs-long-polling-3a0o
        
        Clearly Websockets have many advantages over long polling and thus are appropriate for many applications which require consistent low latency 
        full duplex high frequency communication such as chat applications and real time applications.
        
        Scaling up 
        Websockets have problems with load distribution due to persistent connection.
        Long polling will have equal load distribution after its long timeout
        
     When to use Web Socker vs Server Sent Events ?
        Websocket for bi-directional communication while SSE for server to client communication
        https://blog.bitsrc.io/websockets-vs-server-sent-events-968659ab0870
        
        WebSockets are widely used and valued in technological solutions such as real-time polling, chat, media players, multiplayer games, etc.
        Server-Sent Events: There are many applications where sending data from the client isn’t necessary. 
        SSEs are especially useful in status updates, social-media news feeds, push notifications, newsletters, etc.

      Keep alive : 
        https://www.imperva.com/learn/performance/http-keep-alive/
        - improves latency to avoid 3-way handskae and SSL/TLS connections
        - less consumption of network resources to use single connection. This can drop network conjestion




===============================================================

- Key characteristics
    Resources
      https://github.com/Jeevan-kumar-Raj/Grokking-System-Design/blob/master/basics/key-characteristics.md 
    Scalability, Availability, Reliability, Efficiency and Servicability/Manageability
    -> Remember an Avaiability system does not means Reliable but Reliable system is always available. 

  How to handle burst / spiky reads ? 
    Use Nignx to do request collapsing for similar type of content
    Use queue if content does not need to be delivered real-time
    Other strategies : Data Cache, CDN (https://www.onecloudsystems.com/2016/10/25/how-to-ensure-site-can-handle-traffic-spikes/)

  Pull vs push models
    In Pull approach, the metrics collector needs to know the complete list of service endpoints to pull data from. The good news is that we have a reliable, scalable, and maintainable solution available through Service Discovery, provided by etcd [14], Zookeeper [15], etc., wherein services register their availability and the metrics collector can be notified by the Service Discovery component whenever the list of service endpoints changes. 
    If a server goes down, then you can re-try in the next pull. But here you need dedupe logic or  store the offset in S3 to know what messages to retry.

    In a push model, a collection agent is commonly installed on every server being monitored. Aggregation is an effective way to reduce the volume of data sent to the metrics collector. If the push traffic is high and the metrics collector rejects the push with an error, the agent could keep a small buffer of data locally (possibly by storing them locally on disk), and resend them later.
    If a server goes down or cannot handle burst traffic, then you loose the message.
 

    
   
================================================================================================================================
- Key learnings from system design

 
  
   
         
    

      
    
   
  

  

  
  
  

  
  

  


  




























              






=================================================================================

    
