Ad Click Event Aggregation
      https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation

      Raw data vs Aggregated Data
        Should we store raw data or aggregated data? Our recommendation is to store both. Let’s take a look at why.
          1. It’s a good idea to keep the raw data. If something goes wrong, we could use the raw data for debugging. If the aggregated data is corrupted due to a bad bug, we can recalculate the aggregated data from the raw data, after the bug is fixed.
          2. Aggregated data should be stored as well. The data size of the raw data is huge. The large size makes querying raw data directly very inefficient. To mitigate this problem, we run read queries on aggregated data.
          3. Raw data serves as backup data. We usually don’t need to query raw data unless recalculation is needed. Old raw data could be moved to cold storage to reduce costs.
          4. Aggregated data serves as active data. It is tuned for query performance.


      Choose the right database
        When it comes to choosing the right database, we need to evaluate the following:
        What does the data look like? Is the data relational? Is it a document or a blob?
        Is the workflow read-heavy, write-heavy, or both?
        Is transaction support needed?
        Do the queries rely on many online analytical processing (OLAP) functions [3] like SUM, COUNT?

        As shown in the back of the envelope estimation, the average write QPS is 10,000, and the peak QPS can be 50,000, so the system is write-heavy. On the read side, raw data is used as backup and a source for recalculation, so in theory, the read volume is low.
        Relational databases can do the job, but scaling the write can be challenging. NoSQL databases like Cassandra and InfluxDB are more suitable because they are optimized for write and time-range queries.
      
      Asynchronous processing
        The design we currently have is synchronous. This is not good because the capacity of producers and consumers is not always equal. Consider the following case; if there is a sudden increase in traffic and the number of events produced is far beyond what consumers can handle, consumers might get out-of-memory errors or experience an unexpected shutdown. If one component in the synchronous link is down, the whole system stops working.

        A common solution is to adopt a message queue (Kafka) to decouple producers and consumers. This makes the whole process asynchronous and producers/consumers can be scaled independently.
        You might be wondering why we don’t write the aggregated results to the database directly. The short answer is that we need the second message queue like Kafka to achieve end-to-end exactly-once semantics (atomic commit). "Exactly-once" feature in distributed queues.

      Aggregation service
        The MapReduce framework is a good option to aggregate ad click events. The directed acyclic graph (DAG) is a good model for it [9]. The key to the DAG model is to break down the system into small computing units, like the Map/Aggregate/Reduce nodes
          Map node
            You might be wondering why we need the Map node. An alternative option is to set up Kafka partitions or tags and let the aggregate nodes subscribe to Kafka directly. This works, but the input data may need to be cleaned or normalized, and these operations can be done by the Map node. Another reason is that we may not have control over how data is produced and therefore events with the same ad_id might land in different Kafka partitions.
          Aggregate node
            An Aggregate node counts ad click events by ad_id in memory every minute. In the MapReduce paradigm, the Aggregate node is part of the Reduce. So the map-aggregate-reduce process really means map-reduce-reduce.
          Reduce node
            A Reduce node reduces aggregated results from all “Aggregate” nodes to the final result. 
        Main use cases
          Use case 1: aggregate the number of clicks
            input events are partitioned by ad_id (ad_id % 3) in Map nodes and are then aggregated by Aggregation nodes.
          Use case 2: return top N most clicked ads
            Input events are mapped using ad_id and each Aggregate node maintains a heap data structure to get the top 3 ads within the node efficiently. In the last step, the Reduce node reduces N ads 
          Use case 3: data filtering
            To support data filtering like “show me the aggregated click count for ad001 within the USA only”, we can pre-define filtering criteria and aggregate based on them.

      Streaming vs batching
        In our design, both stream processing and batch processing are used. We utilized stream processing to process data as it arrives and generates aggregated results in a near real-time fashion. We utilized batch processing for historical data backup.
        For a system that contains two processing paths (batch and streaming) simultaneously, this architecture is called lambda [14]. A disadvantage of lambda architecture is that you have two processing paths, meaning there are two codebases to maintain. Kappa architecture [15], which combines the batch and streaming in one processing path, solves the problem.

        Data recalculation
          Sometimes we have to recalculate the aggregated data, also called historical data replay. For example, if we discover a major bug in the aggregation service, we would need to recalculate the aggregated data from raw data starting at the point where the bug was introduced.
      
      Time
        We need a timestamp to perform aggregation. The timestamp can be generated in two different places:
          Event time: when an ad click happens.
          Processing time: refers to the system time of the aggregation server that processes the click event.
          
          Due to network delays and asynchronous environments (data go through a message queue), the gap between event time and processing time can be large.

      Aggregation window
          According to the “Designing data-intensive applications” book by Martin Kleppmann [16], there are four types of window functions: tumbling (also called fixed) window, hopping window, sliding window, and session window. We will discuss the tumbling window and sliding window
          In the tumbling window (highlighted in Figure 15), time is partitioned into same-length, non-overlapping chunks.
          In the sliding window (highlighted in Figure 16), events are grouped within a window that slides across the data stream, according to a specified interval.
      
      Delivery guarantees
          Since the aggregation result is utilized for billing, data accuracy and completeness are very important. The system needs to be able to answer questions such as:

          How to avoid processing duplicate events?
          How to ensure all events are processed?
        Solution : use queue with "exactly once" policy

      Data deduplication
        Client-side. For example, a client might resend the same event multiple times. Duplicated events sent with malicious intent are best handled by ad fraud/risk control components. If this is of interest, please refer to the reference material [18].
        Server outage. If an aggregation service node goes down in the middle of aggregation and the upstream service hasn’t yet received an acknowledgment, the same events might be sent and aggregated again. Let’s take a closer look
        
        The most straightforward solution (Figure 18) is to use external file storage, such as HDFS or S3, to record the offset.
        
        To achieve “exactly-once” processing, we need to put operations between step 4 to step 6 in one distributed transaction. A distributed transaction is a transaction that works across several nodes. If any of the operations fails, the whole transaction is rolled back.

      Scale the system
        Scale the message queue
          We have already discussed how to scale the message queue extensively in the “Distributed Message Queue” chapter, so we’ll only briefly touch on a few points.

          Producers. We don’t limit the number of producer instances, so the scalability of producers can be easily achieved.

          Consumers. Inside a consumer group, the rebalancing mechanism helps to scale the consumers by adding or removing nodes.When there are hundreds of Kafka consumers in the system, consumer rebalance can be quite slow and could take a few minutes or even more. Therefore, if more consumers need to be added, try to do it during off-peak hours to minimize the impact.
        
        Brokers (Remember each broker has multiple topics and and each topic has multiple partitions for parallelization. Each consumer group can be attached to many topics but no two consumers in same consumer group is attached to one partitions. Each consumer in the group maintains its own offset to the partition)

          Hashing key

            Using ad_id as hashing key for Kafka partition to store events from the same ad_id in the same Kafka partition. In this case, an aggregation service can subscribe to all events of the same ad_id from one single partition.

          The number of partitions

            If the number of partitions changes, events of the same ad_id might be mapped to a different partition. Therefore, it’s recommended to pre-allocate enough partitions in advance, to avoid dynamically increasing the number of partitions in production.

          Topic physical sharding

            One single topic is usually not enough. We can split the data by geography (topic_north_america, topic_europe, topic_asia, etc.,) or by business type (topic_web_ads, topic_mobile_ads, etc).

        Scale the aggregation service
            In the high-level design, we talked about the aggregation service being a map/reduce operation.

              Option 1: In same host, Allocate events with different ad_ids to different threads,
              Option 2: Deploy aggregation service nodes on resource providers like Apache Hadoop

        Scale the database
          Cassandra natively supports horizontal scaling, in a way similar to consistent hashing.

      
      Hotspot issue
        A shard or service that receives much more data than the others is called a hotspot. This occurs because major companies have advertising budgets in the millions of dollars and their ads are clicked more often. Since events are partitioned by ad_id, some aggregation service nodes might receive many more ad click events than others, potentially causing server overload.
        This problem can be mitigated by allocating more aggregation nodes to process popular ads. Map-Reduce architecure uses master slave model to handle this resource management.

      Fault tolerance
        Replaying data from the beginning of Kafka is slow. A good practice is to save the “system status” like upstream offset to a snapshot and recover from the last saved status. In our design, the “system status” is more than just the upstream offset because we need to store data like top N most clicked ads in the past M minutes.

      Reconciliation
        Reconciliation means comparing different sets of data in order to ensure data integrity. 
        What we can do is to sort the ad click events by event time in every partition at the end of the day, by using a batch job and reconciling between raw data and  aggregation result.

      Alternative design
        In a generalist system design interview, you are not expected to know the internals of different pieces of specialized software used in a big data pipeline. Explaining your thought process and discussing trade-offs is very important, which is why we propose a generic solution. Another option is to store ad click data in Hive, with an ElasticSearch layer built for faster queries.


      TODO:
      Study about different types of database 
      s3 
      Streaming vs Batching table : https://bytebytego.com/courses/system-design-interview/ad-click-event-aggregation


