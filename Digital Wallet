Digital Wallet
      https://bytebytego.com/courses/system-design-interview/digital-wallet

      Step 2 - Propose High-Level Design and Get Buy-In

        API Design

          API	Detail
          POST /v1/wallet/balance_transfer

        In-memory sharding solution
          For in-memory stores, one popular choice is Redis. One Redis node is not enough to handle 1 million TPS. We need to set up a cluster of Redis nodes and evenly distribute user accounts among them. This process is called partitioning or sharding.

          The number of partitions and addresses of all Redis nodes can be stored in a centralized place. We could use Zookeeper [4] as a highly-available configuration storage solution.

          The final component of this solution is a service that handles the transfer commands. We call it the wallet service and it has several key responsibilities.
              Receives the transfer command
              Validates the transfer command
              If the command is valid, it updates the account balances for the two users involved in the transfer. In a cluster, the account balances are likely to be in different Redis nodes

          In this example, we have 3 Redis nodes. There are three clients, A, B, and C. Their account balances are evenly spread across these three Redis nodes. There are two wallet service nodes in this example that handle the balance transfer requests. 
          This design works, but it does not meet our correctness requirement. The wallet service updates two Redis nodes for each transfer. There is no guarantee that both updates would succeed.

        Distributed transactions

          Database sharding

            The first step is to replace each Redis node with a transactional relational database node. 

            Using transactional databases only solves part of the problem. As mentioned in the last section, it is very likely that one transfer command will need to update two accounts in two different databases. There is no guarantee that two update operations will be handled at exactly the same time. If the wallet service restarted right after it updated the first account balance, how can we make sure the second account will be updated as well?

          Distributed transaction: two-phase commit

            There are two ways to implement a distributed transaction: a low-level solution and a high-level solution. We will examine each of them.

            The low-level solution relies on the database itself. The most commonly used algorithm is called two-phase commit (2PC).
              - The coordinator, which in our case is the wallet service, performs read and write operations on multiple databases as normal. As shown in Figure 5, both databases A and C are locked.
              - When the application is about to commit the transaction, the coordinator asks all databases to prepare the transaction.

              - In the second phase, the coordinator collects replies from all databases and performs the following:

                    If all databases reply with a “yes”, the coordinator asks all databases to commit the transaction they have received.

                    If any database replies with a “no”, the coordinator asks all databases to abort the transaction.

              The biggest problem with 2PC is that it’s not performant, as locks can be held for a very long time while waiting for a message from the other nodes. Another issue with 2PC is that the coordinator can be a single point of failure, as shown in Figure 6.

          Distributed transaction: Try-Confirm/Cancel (TC/C)

            TC/C is a type of compensating transaction [7] that has two steps:
              In the first phase, the coordinator asks all databases to reserve resources for the transaction.
              In the second phase, the coordinator collects replies from all databases:
                If all databases reply with “yes”, the coordinator asks all databases to confirm the operation, which is the Try-Confirm process.
                If any database replies with “no”, the coordinator asks all databases to cancel the operation, which is the Try-Cancel process.
            It’s important to note that the two phases in 2PC are wrapped in the same transaction, but in TC/C each phase is a separate transaction.
                First phase: Try
                  For the database that contains account A, the coordinator starts a local transaction that reduces the balance of A by $1.
                  For the database that contains account C, the coordinator gives it a NOP (no operation.) To make the example adaptable for other scenarios, let’s assume the coordinator sends to this database a NOP command. The database does nothing for NOP commands and always replies to the coordinator with a success message.
                Second phase: Confirm
                  If both databases reply “yes”, the wallet service starts the next Confirm phase.
                Second phase: Cancel
                  What if the first Try phase fails? In the example above we have assumed the NOP operation on account C always succeeds, although in practice it may fail. For example, account C might be an illegal account, and the regulator has mandated that no money can flow into or out of this account. In this case, the distributed transaction must be canceled and we have to clean up.
                  Because the balance of account A has already been updated in the transaction in the Try phase, it is impossible for the wallet service to cancel a completed transaction. What it can do is to start another transaction that reverts the effect of the transaction in the Try phase, which is to add $1 back to account A.

                TC/C is also called a distributed transaction by compensation. It is a high-level solution because the compensation, also called the “undo,” is implemented in the business logic. The advantage of this approach is that it is database-agnostic. As long as a database supports transactions, TC/C will work. The disadvantage is that we have to manage the details and handle the complexity of the distributed transactions in the business logic at the application layer.

              Phase status table
                  We still have not yet answered the question asked earlier; what if the wallet service restarts in the middle of TC/C? When it restarts, all previous operation history might be lost, and the system may not know how to recover.
                  The solution is simple. We can store the progress of a TC/C as phase status in a transactional database.

              Unbalanced state
                  Have you noticed that by the end of the Try phase, $1 is missing (Figure 11)?
                  It violates a fundamental rule of accounting that the sum should remain the same after a transaction.
                  The good news is that the transactional guarantee is still maintained by TC/C. TC/C comprises several independent local transactions. Because TC/C is driven by application, the application itself is able to see the intermediate result between these local transactions. On the other hand, the database transaction or 2PC version of the distributed transaction was maintained by databases that are invisible to high-level applications.

              Out-of-order execution
                  Let’s assume that the database that handles account C has some network issues and it receives the Cancel instruction before the Try instruction. In this case, there is nothing to cancel.
                    The out-of-order Cancel operation leaves a flag in the database indicating that it has seen a Cancel operation, but it has not seen a Try operation yet.
                    The Try operation is enhanced so it always checks whether there is an out-of-order flag, and it returns a failure if there is.
                  This is why we added an out-of-order flag to the phase status table in the “Phase Status Table” section.

          Distributed transaction: Saga
              Linear order execution
                1. All operations are ordered in a sequence. Each operation is an independent transaction on its own database.

                2. Operations are executed from the first to the last. When one operation has finished, the next operation is triggered.

                3. When an operation has failed, the entire process starts to roll back from the current operation to the first operation in reverse order, using compensating transactions. So if a distributed transaction has n operations, we need to prepare 2n operations: n for the normal case and another n for the compensating transaction during rollback.

                How do we coordinate the operations? There are two ways to do it:

                  Choreography (De-centralized). In a microservice architecture, all the services involved in the Saga distributed transaction do their jobs by subscribing to other services’ events. So it is fully decentralized coordination.

                  Orchestration (Centralized). A single coordinator instructs all services to do their jobs in the correct order.

                  The choice of which coordination model to use is determined by the business needs and goals. The challenge of the choreography solution is that services communicate in a fully asynchronous way, so each service has to maintain an internal state machine in order to understand what to do when other services emit an event. It can become hard to manage when there are many services. The orchestration solution handles complexity well, so it is usually the preferred solution in a digital wallet system.

          Event sourcing
              Background
                In real life, a digital wallet provider may be audited. These external auditors might ask some challenging questions, for example:
                Do we know the account balance at any given time?
                How do we know the historical and current account balances are correct?
                How do we prove that the system logic is correct after a code change?

                One design philosophy that systematically answers those questions is event sourcing, which is a technique developed in Domain-Driven Design (DDD) [9].

          Wallet service example
              For the wallet service, the commands are balance transfer requests. These commands are put into a FIFO queue. One popular choice for the command queue is Kafka [10]
              Let us assume the state (the account balance) is stored in a relational database. The state machine examines each command one by one in FIFO order. For each command, it checks whether the account has a sufficient balance. If yes, the state machine generates an event for each account.
                Read commands from the command queue.
                Read balance state from the database.
                Validate the command. If it is valid, generate two events for each of the accounts.
                Read the next Event.
                Apply the Event by updating the balance in the database.

          Reproducibility
              The most important advantage that event sourcing has over other architectures is reproducibility.
              In the distributed transaction solutions mentioned earlier, a wallet service saves the updated account balance (the state) into the database. It is difficult to know why the account balance was changed. Meanwhile, historical balance information is lost during the update operation. In the event sourcing design, all changes are saved first as immutable history. The database is only used as an updated view of what balance looks like at any given point in time.

            Because of the audit capability, event sourcing is often chosen as the de facto solution for the wallet service.

          Command-query responsibility segregation (CQRS)
            So far, we have designed the wallet service to move money from one account to another efficiently. However, the client still does not know what the account balance is. 
            Rather than publishing the state (balance information), event sourcing publishes all the events. The external world could rebuild any customized state itself. This design philosophy is called CQRS [11].
            In CQRS, there is one state machine responsible for the write part of the state, but there can be many read-only state machines. For example, clients may want to know their balances and a read-only state machine could save state in a database to serve the balance query. Another state machine could build state for a specific time period to help investigate issues like possible double charges. The state information is an audit trail that could help to reconcile the financial records.

      Design Deep Dive
      
          File-based state
            In the previous design, state (balance information) is stored in a relational database. In a production environment, a database usually runs in a stand-alone server that can only be accessed through networks. Similar to the optimizations we did for command and event, state information can be saved to the local disk, as well.

            More specifically, we can use SQLite [14], which is a file-based local relational database or use RocksDB [15], which is a local file-based key-value store.

            RocksDB is chosen because it uses a log-structured merge-tree (LSM), which is optimized for write operations. To improve read performance, the most recent data is cached.


      TODO
        Study mmap

              
